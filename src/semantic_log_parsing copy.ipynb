{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from config import config\n",
    "from gpt_model import get_completion_from_gpt\n",
    "from claude import get_completion_from_claude\n",
    "from ollama import get_completion_from_ollama\n",
    "from format_output import Format_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import seaborn as sns\n",
    "from math import pi\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "from rouge import Rouge\n",
    "from nltk.metrics import edit_distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR to your repository root.\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "# Set the DATA_DIR to the directory where your data resides.\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/loghub_2k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"qwen2.5-coder:7b\" # change this everytime with new model\n",
    "# Model options - deepseek-coder-v2, deepseek-v2:16b, codegemma:7b, qwen2.5-coder:7b, codellama:7b\n",
    "\n",
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "now_time = datetime.datetime.now()\n",
    "date_string = \"Semantic_\" + llm_model_name + now_time.strftime('_%Y-%m-%d-%H-%M')\n",
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "\n",
    "save_dir_path_now = os.path.join(save_dir_path, date_string)\n",
    "\n",
    "raw_save_dir_path = os.path.join(save_dir_path_now, \"raw_results/\")\n",
    "Path(raw_save_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_templates_output_file_name = 'log_template_output.txt'\n",
    "variables_output_file_name = 'variables_output.csv'\n",
    "\n",
    "\n",
    "semantic_prompts_file_path = os.path.join(raw_save_dir_path, \"semantic_prompts.txt\")\n",
    "log_templates_output_file_path = raw_save_dir_path + log_templates_output_file_name\n",
    "variables_output_file_path = raw_save_dir_path + variables_output_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ground_truth_file_path = os.path.join(DATA_DIR, \"ground_truth_template.csv\")\n",
    "raw_logs_file_path = os.path.join(DATA_DIR, \"combined_raw_logs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw log messages\n",
    "with open(raw_logs_file_path, 'r') as raw_file:\n",
    "    log_samples = [line.strip() for line in raw_file.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <TPL>The Apache JServ Protocol (JK2) failed to find a child process with PID 5622 in its scoreboard.</TPL>\n",
      "10: <TPL>Node 129 started an action with ID 1142537875, booting Genvmunix using command 4185.</TPL>\n",
      "20: <TPL>A ServerFileSystem domain panic has occurred on storage442.</TPL>\n",
      "30: <TPL>The partition is closing due to reaching its maximum capacity.</TPL>\n",
      "40: <TPL>The temperature of gige4 interface is 1146058156 and it is a warning.</TPL>\n",
      "50: <TPL>Node node-73 has detected an available network connection on network 0.0.0.0 via interface alt0.</TPL>\n",
      "60: <TPL>User root has opened a session.</TPL>\n",
      "70: <TPL>CUPS daemon successfully shut down.</TPL>\n",
      "80: <TPL>The syslogd service on combo has been restarted.</TPL>\n",
      "90: <TPL>klogd service started successfully.</TPL>\n",
      "100: <TPL>The system is booting with the root filesystem labeled as \"/\" and using graphical boot messages with minimal verbosity.</TPL>\n",
      "110: <TPL>The system console is set to use color VGA with a resolution of 80x25.</TPL>\n",
      "120: <TPL>The rpc.idmapd service has successfully started.</TPL>\n",
      "130: <TPL>The system is enabling support for unmasked SIMD FPU exceptions.</TPL>\n",
      "140: <TPL>The Linux kernel is starting up and initializing support for Plug and Play devices.</TPL>\n",
      "150: <TPL>The system is using BIOS version 1.2 with driver version 1.16ac.</TPL>\n",
      "160: <TPL>The loopback interface was successfully brought up.</TPL>\n",
      "170: <TPL>MobaXterm.exe connected to 183.62.156.108:22 via proxy socks.cse.cuhk.edu.hk:5070 using SOCKS5 protocol.</TPL>\n",
      "180: <TPL>The server with identifier 2 is dropping its connection to the server with identifier 1 because it has a smaller server identifier.</TPL>\n",
      "190: <TPL>The system encountered an error while trying to create a node at \"/home/curi/.zookeeper\" because the node already exists.</TPL>\n",
      "200: <TPL>The request processor has completed its shutdown.</TPL>\n",
      "210: <TPL>The SendWorker is being interrupted.</TPL>\n",
      "220: <TPL>The system is attempting to retrieve a snapshot from the leader.</TPL>\n",
      "230: <TPL>The system encountered an unexpected message code that is neither 51 nor 4294967295.</TPL>\n",
      "240: <TPL>A fatal error occurred due to an instruction cache block touch.</TPL>\n",
      "250: <TPL>Floating point instruction enabled on RAS KERNEL.</TPL>\n",
      "260: <TPL>Error creating node map due to lack of child processes.</TPL>\n",
      "270: <TPL>Error creating node map from file due to bad file descriptor.</TPL>\n",
      "280: <TPL>Node card status indicates active alerts.</TPL>\n",
      "<TPL>Clock mode is set to Low.</TPL>\n",
      "<TPL>Clock select is configured for Midplane.</TPL>\n",
      "<TPL>Phy JTAG Reset is asserted.</TPL>\n",
      "<TPL>ASIC JTAG Reset is not asserted.</TPL>\n",
      "<TPL>Temperature Mask is inactive.</TPL>\n",
      "<TPL>No temperature error detected.</TPL>\n",
      "<TPL>Temperature Limit Error Latch is clear.</TPL>\n",
      "<TPL>PGOOD is asserted, indicating power good status.</TPL>\n",
      "<TPL>PGOOD error latch is clear.</TPL>\n",
      "<TPL>MPGOOD is OK, indicating multi-processor power good status.</TPL>\n",
      "<TPL>MPGOOD error latch is clear.</TPL>\n",
      "<TPL>The 2.5 volt rail is functioning normally.</TPL>\n",
      "<TPL>The 1.5 volt rail is functioning normally.</TPL>\n",
      "290: <TPL>Torus receiver y+ input pipe error(s) detected and corrected.</TPL>\n",
      "300: <TPL>The system has completed its shutdown process.</TPL>\n",
      "310: <TPL>DDR errors detected and corrected on rank 0, symbol 8, bit 7.</TPL>\n",
      "320: <TPL>The system received an interrupt signal 15.</TPL>\n",
      "330: <TPL>The node card is not fully functional.</TPL>\n",
      "340: <TPL>EDRAM error(s) detected and corrected over 282 seconds.</TPL>\n",
      "350: <TPL>The YARN Application Master (AM) is requesting an RM token for the specified application attempt.</TPL>\n",
      "360: <TPL>The system is adding a job token for a specific job to the jobTokenSecretManager.</TPL>\n",
      "370: <TPL>The system added a global filter named 'safety' to enhance security.</TPL>\n",
      "380: <TPL>The system is using a LinkedBlockingQueue for managing call queues.</TPL>\n",
      "390: <TPL>The system is logging information about a container allocation in the default queue.</TPL>\n",
      "400: <TPL>The job-jar file for the specified job is located at hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job.jar.</TPL>\n",
      "410: <TPL>The task attempt transitioned from ASSIGNED to RUNNING.</TPL>\n",
      "420: <TPL>Acknowledgment received for task attempt 1445144423722_0020_m_000003_0.</TPL>\n",
      "430: <TPL>The system launched 1 speculation and will sleep for 15000 milliseconds.</TPL>\n",
      "440: <TPL>Error occurred while writing History Event for TaskAttemptUnsuccessfulCompletionEvent.</TPL>\n",
      "450: <TPL>The system is processing an event of type TASK_ABORT.</TPL>\n",
      "460: <TPL>Error occurred while contacting ResourceManager.</TPL>\n",
      "470: <TPL>The GoogleSoftwareUpdateAgent is checking with the local engine for updates.</TPL>\n",
      "480: <TPL>BTLE scanning has been stopped.</TPL>\n",
      "490: <TPL>The WebKit Networking service exited with an abnormal code, indicating an error.</TPL>\n",
      "500: <TPL>The mDNSResponder service on interface en0 at IP address 10.105.162.32 is experiencing frequent transitions.</TPL>\n",
      "510: <TPL>The system is exiting due to Diskarb.</TPL>\n",
      "520: <TPL>A session was created with ID 101921.</TPL>\n",
      "530: <TPL>BTLE scanning started.</TPL>\n",
      "540: <TPL>The wireless scan failed because the central device is not powered on.</TPL>\n",
      "550: <TPL>The CCProfileMonitor has completed freeing its resources.</TPL>\n",
      "560: <TPL>Manual intervention required for network interface en0.</TPL>\n",
      "570: <TPL>The Wi-Fi interface en0 has its country code set to 'X3'.</TPL>\n",
      "580: <TPL>The system encountered an assertion failure related to the com.apple.telemetry process.</TPL>\n",
      "590: <TPL>The system took 603 milliseconds to set all pages in the hibernate list.</TPL>\n",
      "600: <TPL>The application attempted to retrieve principal information from a CardDAV server but failed due to an offline internet connection.</TPL>\n",
      "610: <TPL>The ChromeExistion process with PID 36852 accessed www.baidu.com.</TPL>\n",
      "620: <TPL>The host controller has been successfully published.</TPL>\n",
      "630: <TPL>The wireless network interface wl0 updated TCP keep-alive sequence with original sequence number 1092633597, acknowledgment number 2572586285, and window size of 4096.</TPL>\n",
      "640: <TPL>The Preview application logged that the page bounds are set to {{0, 0}, {400, 400}}.</TPL>\n",
      "650: <TPL>The GoogleSoftwareUpdateAgent initiated an update check at 01:47:40 on July 7, 2017.</TPL>\n",
      "660: <TPL>Error occurred during CoreDragRemoveTrackingHandler operation with error code -1856.</TPL>\n",
      "670: <TPL>The network configuration on en0 interface has changed, including new IPv4 address 10.105.160.205 and IPv6 address 2607:f140:6000:8:c6b3:1ff:fecd:467f. DNS, Proxy, and SMB services are affected.</TPL>\n",
      "680: <TPL>AirPort interface awdl0 has lost its connection.</TPL>\n",
      "690: <TPL>Unexpected preroll-complete notification received.</TPL>\n",
      "700: <TPL>The SOAPParser encountered an unknown type 'ExchangePersonIdGuid' while trying to parse data for EWSItemType.</TPL>\n",
      "710: <TPL>The GPUToolsAgent encountered an error while trying to invalidate a transport connection.</TPL>\n",
      "720: <TPL>The application failed to establish a connection between NSApplication and NSColorPickerGridView due to a missing setter or instance variable.</TPL>\n",
      "730: <TPL>The NetworkAnalyticsEngine encountered an unexpected switch value of 2.</TPL>\n",
      "740: <TPL>The system received a capture notice with ID 1499506366.010075 due to an authentication failure.</TPL>\n",
      "750: <TPL>The primary key hashing failed, causing the journal record to be dropped.</TPL>\n",
      "760: <TPL>User clicked on a button with the code 0x8002bdf.</TPL>\n",
      "770: <TPL>The system is setting the hostname to \"calvisitor-10-105-162-124.calvisitor.1918.berkeley.edu\".</TPL>\n",
      "780: <TPL>The ContactsAccountsService is unable to access an account due to a missing connection.</TPL>\n",
      "790: <TPL>The AWDL operation mode has been changed from AUTO to SUSPENDED.</TPL>\n",
      "800: <TPL>The system has received a wake call from AppleCamIn with a specific message type.</TPL>\n",
      "810: <TPL>The system is saving statistical data with type 40004, time 1513958400000, statClient 2, and who is 1.</TPL>\n",
      "820: <TPL>New date set to 20171223, type changed to 40006,7140.0, old value was 6900.0.</TPL>\n",
      "830: <TPL>The system has reached a step count or calorie sum threshold, triggering the checkInsertStatus process.</TPL>\n",
      "840: <TPL>The day changed at timestamp 1514044800216.</TPL>\n",
      "850: <TPL>The system retrieved today's basic standard steps, which are 3786.</TPL>\n",
      "860: <TPL>The system has extended with an ID of 30002312 at timestamp 1514046505000.</TPL>\n",
      "870: <TPL>The auto-sync feature is enabled.</TPL>\n",
      "880: <TPL>More authentication failures for user root from IP 106.5.5.195 via SSH.</TPL>\n",
      "890: <TPL>User 'admin' attempted too many authentication failures and was disconnected.</TPL>\n",
      "900: <TPL>Failed login attempt for root user from IP 183.62.140.253 using SSH.</TPL>\n",
      "910: <TPL>Spark executor actor system started and is listening on port 55904.</TPL>\n",
      "920: <TPL>A BlockManager has been registered.</TPL>\n",
      "930: <TPL>HadoopRDD processing input split for file 2kSOSP.log at offset 284388 with length 7303.</TPL>\n",
      "940: <TPL>DataNode is starting a thread to transfer a block to another node.</TPL>\n",
      "950: <TPL>The NameNode updated its block map, adding a new block to the system with an ID of blk_1441315972355360459 and a size of 67108864 bytes.</TPL>\n",
      "960: <TPL>The instance was successfully destroyed on the hypervisor after 1.02 seconds.</TPL>\n",
      "970: <TPL>The nova-compute service reported that for instance faf974ea-cba5-4e1b-93f4-3a3bc606006f, the total disk space is 15 GB and currently no disk space is used.</TPL>\n",
      "980: <TPL>The compute service record for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us has been updated.</TPL>\n",
      "990: <TPL>The image with ID 0673dd71-34c5-4fbb-86c4-40623fbe45b4 is being checked at /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742.</TPL>\n",
      "1000: <TPL>A DHCP client with MAC address 00:11:43:e3:ba:c3 has sent a DHCPDISCOVER request via the eth1 interface.</TPL>\n",
      "1010: <TPL>The cron job for getting temperatures was executed successfully.</TPL>\n",
      "1020: <TPL>The system has detected an ACPI LAPIC with ACPI ID 0x04 and LAPIC ID 0x07, which is enabled.</TPL>\n",
      "1030: <TPL>The system has detected that processor CPU3 supports C1 state.</TPL>\n",
      "1040: <TPL>The filesystem was successfully mounted with ordered data mode.</TPL>\n",
      "1050: <TPL>The system is using MMCONFIG at e0000000 for PCI.</TPL>\n",
      "1060: <TPL>The system has logged information about a processor's ANSI SCSI revision.</TPL>\n",
      "1070: <TPL>The USB HID core driver version 2.0 has been loaded on Nov 9, 2005.</TPL>\n",
      "1080: <TPL>The autorun process has been completed.</TPL>\n",
      "1090: <TPL>Portmap service started successfully.</TPL>\n",
      "1100: <TPL>i8042 AUX port configured at 0x60,0x64 with IRQ 12.</TPL>\n",
      "1110: <TPL>The USB core registered a new driver named usbfs.</TPL>\n",
      "1120: <TPL>User #29# authenticated successfully from #30#.</TPL>\n",
      "1130: <TPL>xinetd service started with 1 available service.</TPL>\n",
      "1140: <TPL>The temperature of device /dev/sda changed from -2 Celsius to 26 Celsius since the last report.</TPL>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Reformulate log messages with semantic understanding\n",
    "counter=0\n",
    "semantic_prompts = []\n",
    "for log in log_samples:\n",
    "    prompt=f\"\"\"You are provided with a log message. Your task is to understand and extract the meaning behind the semi-structured log message.\n",
    "                      \n",
    "                    Log message: {log}. \n",
    "\n",
    "                    A log message usually contains a header that is automatically produced by the logging framework, including information such as timestamp, class, and logging level (INFO, DEBUG, WARN etc.).\n",
    "                    Ignore all these details and just understand meaning behind the natural languagae text which is in the log content.\n",
    "\n",
    "                    The log content typically consists of many parts: \n",
    "                    1. Template - message body, that contains constant strings (or keywords) describing the system events; \n",
    "                    2. Parameters/Variables - dynamic variables, which reflect specific runtime status;\n",
    "\n",
    "                    Please capture the essential context and meaning from the log message to understand the reasoning behind each raw log.\n",
    "                    Provide only the meaning in just one sentence from each log message surrounded by <TPL> and </TPL> in a single line.\n",
    "                    Never provide any text other than just the understanding from the log message\n",
    "                \"\"\"\n",
    "    \n",
    "    # semantic_prompt = get_completion_from_gpt(prompt)\n",
    "    semantic_prompt = get_completion_from_ollama(prompt, model=llm_model_name)\n",
    "    semantic_prompts.append(semantic_prompt)\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter}: {semantic_prompt}')\n",
    "    counter+=1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1146\n"
     ]
    }
   ],
   "source": [
    "print(len(semantic_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed output saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_qwen2.5-coder:7b_2024-12-11-16-02/raw_results/semantic_prompts.txt\n",
      "Semantics from logs saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_qwen2.5-coder:7b_2024-12-11-16-02/raw_results/semantic_prompts.txt\n"
     ]
    }
   ],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(semantic_prompts_file_path, semantic_prompts)\n",
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(semantic_prompts_file_path, semantic_prompts_file_path)\n",
    "\n",
    "# Save all semantic log templates to a file\n",
    "print(f\"Semantics from logs saved to: {semantic_prompts_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150: <TPL>The worker environment initialization was successful using the configuration file /etc/httpd/conf/workers2.properties.</TPL>\n",
      "1160: <TPL>The system is initiating a boot command targeting specific nodes.</TPL>\n",
      "1170: <TPL>The Interconnect-0N02 switch_module encountered a power/control problem.</TPL>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m\n\u001b[1;32m      5\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou will be given a log message enclosed by <MSG> and </MSG> tags, along with its semantic understanding: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msemantic_prompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m            The log message consists of:\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124m            Using the same process as above, output only the single-line log_template enclosed in <TPL> and </TPL>, and nothing else.\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# response = get_completion_from_gpt(prompt)\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_completion_from_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m semantic_based_log_templates\u001b[38;5;241m.\u001b[39mappend(response)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/tenacity/__init__.py:336\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    334\u001b[0m copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    335\u001b[0m wrapped_f\u001b[38;5;241m.\u001b[39mstatistics \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mstatistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/tenacity/__init__.py:475\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m retry_state \u001b[38;5;241m=\u001b[39m RetryCallState(retry_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, fn\u001b[38;5;241m=\u001b[39mfn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m     do \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/tenacity/__init__.py:376\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    374\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mactions:\n\u001b[0;32m--> 376\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/tenacity/__init__.py:398\u001b[0m, in \u001b[0;36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetryCallState\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mis_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_state\u001b[38;5;241m.\u001b[39mretry_run_result):\n\u001b[0;32m--> 398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutcome\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:438\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 390\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/tenacity/__init__.py:478\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    480\u001b[0m         retry_state\u001b[38;5;241m.\u001b[39mset_exception(sys\u001b[38;5;241m.\u001b[39mexc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/src/ollama.py:23\u001b[0m, in \u001b[0;36mget_completion_from_ollama\u001b[0;34m(prompt, model)\u001b[0m\n\u001b[1;32m     18\u001b[0m messages \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     19\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a helpful assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     20\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt}\n\u001b[1;32m     21\u001b[0m ]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 23\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust for the randomness of the output\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     content \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n\u001b[1;32m     29\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(content)\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/openai/_base_client.py:1278\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1266\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1275\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1276\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1277\u001b[0m     )\n\u001b[0;32m-> 1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/openai/_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/openai/_base_client.py:991\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    988\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 991\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    997\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Step 2: Generate log template using 4-shot learning\n",
    "counter_1 = 0\n",
    "semantic_based_log_templates = []\n",
    "for log, semantic_prompt in zip(log_samples, semantic_prompts):\n",
    "    prompt = f\"\"\"You will be given a log message enclosed by <MSG> and </MSG> tags, along with its semantic understanding: {semantic_prompt}.\n",
    "\n",
    "                The log message consists of:\n",
    "                1. A log_template (message body, that contains constant strings (or keywords) describing the system events.)\n",
    "                2. Dynamic variables (dynamic variables, which reflect specific runtime status.)\n",
    "\n",
    "                Your task is to:\n",
    "                - Identify all dynamic variables in the log message.\n",
    "                - Replace each dynamic variable with a placeholder surrounded by angle brackets, like <*>, to produce a log_template.\n",
    "                - Output only the transformed log_template, enclosed in <TPL> and </TPL> tags.\n",
    "                - The final output must be a single line with no leading/trailing spaces, no extra lines, and no explanations.\n",
    "\n",
    "                Here are examples:\n",
    "\n",
    "                Q: <MSG>[081109 204453 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.85:50010 is added to blk_2377150260128098806 size 67108864]</MSG>  \n",
    "                A: <TPL>[BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>:<*> is added to <*> size <*>]</TPL>\n",
    "\n",
    "                Q: <MSG>- 1129734520 2005.10.19 R17-M0-N0-I:J18-U01 2005-10-19-08.08.40.058960 R17-M0-N0-I:J18-U01 RAS KERNEL INFO shutdown complete</MSG>  \n",
    "                A: <TPL>shutdown complete</TPL>\n",
    "\n",
    "                Q: <MSG>20231114T101914E ERROR 14 while processing line 123: cannot find input '42'</MSG>  \n",
    "                A: <TPL>ERROR <*> while processing line <*>: cannot find input <*></TPL>\n",
    "\n",
    "                Q: <MSG>2023-01-14 23:05:14 INFO: Reading data from /user/input/file.txt</MSG>  \n",
    "                A: <TPL>Reading data from <*></TPL>\n",
    "\n",
    "                Now, consider the following log message: <MSG>{log}</MSG>  \n",
    "                Using the same process as above, output only the single-line log_template enclosed in <TPL> and </TPL>, and nothing else.\n",
    "            \"\"\"\n",
    "    # response = get_completion_from_gpt(prompt)\n",
    "    response = get_completion_from_ollama(prompt, model=llm_model_name)\n",
    "    semantic_based_log_templates.append(response)\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter}: {semantic_prompt}')\n",
    "    counter+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(semantic_based_log_templates))\n",
    "\n",
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(log_templates_output_file_path, semantic_based_log_templates)\n",
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(log_templates_output_file_path, log_templates_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "processed_log_templates_file_path = log_templates_output_file_path\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "ground_truth_log_templates = ground_truth_df['EventTemplate'].tolist()\n",
    "ground_truth_systems = ground_truth_df['System'].tolist()\n",
    "\n",
    "output_directory = os.path.dirname(processed_log_templates_file_path)\n",
    "\n",
    "\n",
    "# Load processed output data\n",
    "with open(processed_log_templates_file_path, 'r') as processed_file:\n",
    "    processed_log_templates = [line.strip() for line in processed_file.readlines()]\n",
    " \n",
    "\n",
    "# Ensure the lists are of the same length for comparison\n",
    "min_length = min(len(ground_truth_log_templates), len(processed_log_templates))\n",
    "ground_truth_log_templates = ground_truth_log_templates[:min_length]\n",
    "processed_log_templates = processed_log_templates[:min_length]\n",
    "ground_truth_systems = ground_truth_systems[:min_length]\n",
    "\n",
    "# Calculate evaluation metrics for processed_log_templates\n",
    "precision = precision_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "recall = recall_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "f1 = f1_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "\n",
    "# Print evaluation metrics for processed_log_templates\n",
    "print(f\"Log Templates Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Log Templates Recall: {recall * 100:.2f}%\")\n",
    "print(f\"Log Templates F1 Score: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly parsed log templates for each system\n",
    "correct_parsed_counts = {}\n",
    "for system, gt_template, processed_template in zip(ground_truth_systems, ground_truth_log_templates, processed_log_templates):\n",
    "    if gt_template == processed_template:\n",
    "        if system not in correct_parsed_counts:\n",
    "            correct_parsed_counts[system] = 0\n",
    "        correct_parsed_counts[system] += 1\n",
    "\n",
    "# Print correctly parsed log templates for each system\n",
    "print(\"\\nCorrectly Parsed Log Templates per System:\")\n",
    "total=0\n",
    "for system, count in correct_parsed_counts.items():\n",
    "    total +=count\n",
    "    print(f\"{system}: {count}\")\n",
    "\n",
    "print(f\"Total correctly parsed log templates: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that NLTK and other required libraries are installed\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Description of Metrics\n",
    "print(\"Description of Metrics:\\n\")\n",
    "print(\"Edit Distance: Measures the number of character-level edit operations (insertion, deletion, substitution) needed to transform one string into another. A normalized score between 0 and 1 is calculated by dividing the edit distance by the total character length of the longer string, where 0 indicates identical strings and 1 indicates completely different strings.\\n\")\n",
    "print(\"ROUGE-L: Measures the similarity between two texts based on the longest common subsequence. The score ranges from 0 to 1, with 1 indicating high similarity. ROUGE-L focuses on recall and is useful for comparing the structure of sentences.\\n\")\n",
    "print(\"Cosine Similarity: Measures the cosine of the angle between two vectors, which represent the texts in a multi-dimensional space. The score ranges from 0 to 1, with 1 indicating that the vectors are identical, meaning high similarity.\\n\")\n",
    "print(\"BLEU (Bilingual Evaluation Understudy): Measures the similarity between a predicted sentence and one or more reference sentences by calculating the overlap of n-grams. The score ranges from 0 to 1, with 1 indicating high similarity. BLEU is commonly used for machine translation evaluation.\\n\")\n",
    "\n",
    "# 1. Edit Distance for Each System\n",
    "def calculate_edit_distance_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    system_distances = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        distance = edit_distance(gt, processed)\n",
    "        max_length = max(len(gt), len(processed))\n",
    "        normalized_distance = distance / max_length if max_length > 0 else 0\n",
    "        if system not in system_distances:\n",
    "            system_distances[system] = []\n",
    "        system_distances[system].append(normalized_distance)\n",
    "    \n",
    "    avg_distances = {}\n",
    "    for system, distances in system_distances.items():\n",
    "        avg_distance = sum(distances) / len(distances)\n",
    "        avg_distances[system] = avg_distance\n",
    "        print(f\"System '{system}': Normalized Average Edit Distance = {avg_distance:.2f} (Range: 0 to 1, where lower is better)\")\n",
    "    \n",
    "    return avg_distances\n",
    "\n",
    "# 2. ROUGE-L Score for Each System\n",
    "def calculate_rouge_l_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    rouge = Rouge()\n",
    "    system_rouge_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        if not gt.strip() or not processed.strip():\n",
    "            continue  # Skip empty ground truth or processed templates\n",
    "        scores = rouge.get_scores(processed, gt, avg=True)\n",
    "        rouge_l_score = scores['rouge-l']['f']\n",
    "        if system not in system_rouge_scores:\n",
    "            system_rouge_scores[system] = []\n",
    "        system_rouge_scores[system].append(rouge_l_score)\n",
    "    \n",
    "    avg_rouge_scores = {}\n",
    "    print(\"\\nAverage ROUGE-L Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, rouge_scores in system_rouge_scores.items():\n",
    "        avg_rouge_score = sum(rouge_scores) / len(rouge_scores)\n",
    "        avg_rouge_scores[system] = avg_rouge_score\n",
    "        print(f\"System '{system}':              Average ROUGE-L Score = {avg_rouge_score:.2f}\")\n",
    "    \n",
    "    return avg_rouge_scores\n",
    "\n",
    "# 3. Cosine Similarity for Each System\n",
    "def calculate_cosine_similarity_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    system_cosine_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        if not gt.strip() or not processed.strip():\n",
    "            continue  # Skip empty ground truth or processed templates\n",
    "        vectors = vectorizer.fit_transform([gt, processed])\n",
    "        cosine_sim = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "        if system not in system_cosine_scores:\n",
    "            system_cosine_scores[system] = []\n",
    "        system_cosine_scores[system].append(cosine_sim)\n",
    "    \n",
    "    avg_cosine_scores = {}\n",
    "    print(\"\\nAverage Cosine Similarity Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, cosine_scores in system_cosine_scores.items():\n",
    "        avg_cosine_score = sum(cosine_scores) / len(cosine_scores)\n",
    "        avg_cosine_scores[system] = avg_cosine_score\n",
    "        print(f\"System '{system}':              Average Cosine Similarity Score = {avg_cosine_score:.2f}\")\n",
    "    \n",
    "    return avg_cosine_scores\n",
    "\n",
    "# 4. BLEU Score for Each System\n",
    "def calculate_bleu_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    system_bleu_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        reference_tokens = nltk.word_tokenize(gt)\n",
    "        hypothesis_tokens = nltk.word_tokenize(processed)\n",
    "        bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "        if system not in system_bleu_scores:\n",
    "            system_bleu_scores[system] = []\n",
    "        system_bleu_scores[system].append(bleu_score)\n",
    "    \n",
    "    avg_bleu_scores = {}\n",
    "    print(\"\\nAverage BLEU Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, bleu_scores in system_bleu_scores.items():\n",
    "        avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "        avg_bleu_scores[system] = avg_bleu_score\n",
    "        print(f\"System '{system}':              Average BLEU Score = {avg_bleu_score:.2f}\")\n",
    "    \n",
    "    return avg_bleu_scores\n",
    "\n",
    "# Calculating metrics for the given log templates\n",
    "print(\"Comparing the processed log templates to the ground truth log templates:\\n\")\n",
    "edit_distances = calculate_edit_distance_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "rouge_l_scores = calculate_rouge_l_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "cosine_similarity_scores = calculate_cosine_similarity_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "bleu_scores = calculate_bleu_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrices_results = 'combined_evaluation-log_templates_results.csv'\n",
    "evaluation_metrices_results_file_path = os.path.join(save_dir_path, evaluation_metrices_results)\n",
    "\n",
    "# Storing results into a CSV file\n",
    "all_results = []\n",
    "\n",
    "# Add Syntactic results\n",
    "for system in ground_truth_systems:\n",
    "    all_results.append({\n",
    "        \"Parsing Technique\": \"Semantic\",\n",
    "        \"LLM Model\": llm_model_name,\n",
    "        \"System\": system,\n",
    "        \"Edit Distance\": edit_distances.get(system, None),\n",
    "        \"ROUGE-L\": rouge_l_scores.get(system, None),\n",
    "        \"Cosine Similarity\": cosine_similarity_scores.get(system, None),\n",
    "        \"BLEU\": bleu_scores.get(system, None),\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame and save to CSV\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.to_csv(evaluation_metrices_results_file_path, index=False)\n",
    "\n",
    "print(f\"Evaluation metrics saved to {evaluation_metrices_results_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualizations\n",
    "\n",
    "def create_visualizations(edit_distances, rouge_l_scores, cosine_similarity_scores, bleu_scores, output_directory):\n",
    "    systems = list(edit_distances.keys())\n",
    "    edit_values = list(edit_distances.values())\n",
    "    rouge_values = list(rouge_l_scores.values())\n",
    "    cosine_values = list(cosine_similarity_scores.values())\n",
    "    bleu_values = list(bleu_scores.values())\n",
    "\n",
    "    # Edit Distance Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, edit_values, color='skyblue')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Normalized Average Edit Distance')\n",
    "    plt.title('Normalized Average Edit Distance per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(edit_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'edit_distance_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # ROUGE-L Score Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, rouge_values, color='lightgreen')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average ROUGE-L Score')\n",
    "    plt.title('Average ROUGE-L Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(rouge_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'rouge_l_score_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Cosine Similarity Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, cosine_values, color='lightcoral')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average Cosine Similarity Score')\n",
    "    plt.title('Average Cosine Similarity Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(cosine_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'cosine_similarity_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # BLEU Score Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, bleu_values, color='lightblue')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average BLEU Score')\n",
    "    plt.title('Average BLEU Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(bleu_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'bleu_score_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Combined Metrics Bar Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    ax1.bar(systems, edit_values, color='skyblue', alpha=0.6, label='Edit Distance')\n",
    "    ax1.set_xlabel('System')\n",
    "    ax1.set_ylabel('Normalized Average Edit Distance', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    for i, value in enumerate(edit_values):\n",
    "        ax1.text(i, value + 0.02, f'{value:.2f}', ha='center', color='blue')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(systems, rouge_values, color='green', marker='o', linestyle='-', label='ROUGE-L Score')\n",
    "    ax2.plot(systems, cosine_values, color='red', marker='x', linestyle='-', label='Cosine Similarity')\n",
    "    ax2.plot(systems, bleu_values, color='purple', marker='^', linestyle='-', label='BLEU Score')\n",
    "    ax2.set_ylabel('Scores', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "    plt.title('Normalized Edit Distance, ROUGE-L, Cosine Similarity, and BLEU Score per System')\n",
    "    fig.tight_layout()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.savefig(os.path.join(output_directory, 'combined_scores_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Create and save visualizations\n",
    "create_visualizations(edit_distances, rouge_l_scores, cosine_similarity_scores, bleu_scores, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_1 = 0\n",
    "variables_and_anomalies_from_logs = []\n",
    "for log, semantic_prompt in zip(log_samples, semantic_prompts):\n",
    "    prompt = f\"\"\"You will be provided with a log message delimited by <MSG> and </MSG>. \n",
    "    You are also provided with the meaning or understanding from the log message as follows: {semantic_prompt}. \n",
    "    \n",
    "    I want you to categorize the variable(s) in each log message as a dictionary with each variable category as the key and the count of occurrences of that category as the value. \n",
    "    The variable should be classified within the categories as below:\n",
    "    1. Object ID [OID]\tIdentification information of an object\n",
    "    2. Location Indicator   [LOI]\tLocation information of an object\n",
    "    3. Object Name\t[OBN]\tName of an object\n",
    "    4. Type Indicator\t[TID]\tType information of an object or an action\n",
    "    5. Switch Indicator\t[SID]\tStatus of a switch variable\n",
    "    6. Time or Duration of an Action\t[TDA]\tTime or duration of an action\n",
    "    7. Computing Resources\t[CRS]\tInformation of computing resource\n",
    "    8. Object Amount\t[OBA]\tAmount of an object\n",
    "    9. Status Code\t[STC]\tStatus code of an object or an action\n",
    "    10. Other Parameters\t[OTP]\tOther information that does not belong to the above categories\n",
    "\n",
    "    Also, based on the variables found, and the understanding provided for each log as the input above, classify each log as either 1 if abnormal behaviour or 0 if normal behaviour.\n",
    "\n",
    "    Here is the input log message: <MSG>{log}</MSG>\n",
    "    \n",
    "    Please generate a dictionary where each key represents one of the categories listed above and the value represents the count of occurrences of that category in the log message and it's \"Class\" as either 1 or 0. \n",
    "    Always have the \"Class\" key:value pair but only include category key:value that are present in the log message.\n",
    "    Do not print anything other than the dictionary.\n",
    "    Never print the full name for categories just the code for example \"OID\".\n",
    "\n",
    "    Example of the format:\n",
    "    {{\n",
    "        \"OID\": 2,\n",
    "        \"LOI\": 1,\n",
    "        \"STC\": 1,\n",
    "        \"Class\": 1\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # response = get_completion_from_gpt(prompt)\n",
    "    response = get_completion_from_ollama(prompt)\n",
    "    variables_and_anomalies_from_logs.append(response)    \n",
    "\n",
    "    if counter_1 % 10 == 0:\n",
    "        print(f'{counter_1}: {response}')\n",
    "        \n",
    "    counter_1 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(variables_and_anomalies_from_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(variables_output_file_path, variables_and_anomalies_from_logs)\n",
    "\n",
    "fieldnames = ['Index', 'OID', 'LOI', 'OBN', 'TID', 'SID', 'TDA', 'CRS', 'OBA', 'STC', 'OTP', 'Class']\n",
    "\n",
    "# Writing the data to the CSV\n",
    "with open(variables_output_file_path, mode='w', newline='') as variable_file:\n",
    "    writer = csv.DictWriter(variable_file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Regular expression pattern to extract key-value pairs\n",
    "    pattern = re.compile(r'\"(\\w+)\":\\s*(\"?)([\\w\\s\\.,]+)\\2')\n",
    "\n",
    "    # Write each row of the dictionary results\n",
    "    for index, log_string in enumerate(variables_and_anomalies_from_logs, start=1):\n",
    "        # Clean up the string and remove extra formatting like ```json\n",
    "        cleaned_string = log_string.replace('```json', '').strip()\n",
    "\n",
    "        # Extract key-value pairs using the regular expression\n",
    "        matches = pattern.findall(cleaned_string)\n",
    "\n",
    "        # Create a dictionary from the matches\n",
    "        log_dict = {}\n",
    "        for key, _, value in matches:\n",
    "            # print(key)\n",
    "            value = value.strip().replace(',', '')  # Remove trailing newlines and whitespace\n",
    "            # print(value)\n",
    "            if value.isdigit():\n",
    "                log_dict[key] = int(value)\n",
    "            elif key == 'Class':\n",
    "                log_dict[key] = 0 if value in ['Normal'] else 1\n",
    "            else:\n",
    "                print(\"here\")\n",
    "                log_dict[key] = 0  # Replace any unexpected strings with 0\n",
    "\n",
    "        # Ensure that all keys are present, with missing ones having a value of 0 or an appropriate default\n",
    "        row_data = {key: log_dict.get(key, 0 if key != 'Class' else 'Normal') for key in fieldnames[1:]}\n",
    "        row_data['Index'] = index\n",
    "        writer.writerow(row_data)\n",
    "\n",
    "# Save all variables to a file\n",
    "print(f\"Variables are saved to: {variables_output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth and processed data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "processed_df = pd.read_csv(variables_output_file_path)\n",
    "\n",
    "# Ensure the dataframes are of the same length for comparison\n",
    "min_length = min(len(ground_truth_df), len(processed_df))\n",
    "ground_truth_df = ground_truth_df[:min_length]\n",
    "processed_df = processed_df[:min_length]\n",
    "\n",
    "# 1. Per Log Message Evaluation (Overall Accuracy per Log)\n",
    "log_message_accuracies = []\n",
    "for idx in range(min_length):\n",
    "    ground_truth_row = ground_truth_df.iloc[idx, 2:]  # Skip 'Index' column\n",
    "    processed_row = processed_df.iloc[idx, 1:-1]        # Skip 'Index' column\n",
    "    accuracy = (ground_truth_row == processed_row).mean()\n",
    "    log_message_accuracies.append(accuracy)\n",
    "\n",
    "overall_log_accuracy = sum(log_message_accuracies) / len(log_message_accuracies)\n",
    "print(f\"Overall Log Message Accuracy: {overall_log_accuracy * 100:.2f}%\")\n",
    "\n",
    "# 2. Evaluation Per Variable Type (Precision, Recall, F1 Score, and Accuracy for each variable type)\n",
    "variable_types = ['OID', 'LOI', 'OBN', 'TID', 'SID', 'TDA', 'CRS', 'OBA', 'STC', 'OTP']\n",
    "precision_scores = {}\n",
    "recall_scores = {}\n",
    "f1_scores = {}\n",
    "# accuracy_scores = {}\n",
    "\n",
    "for var in variable_types:\n",
    "    y_true = ground_truth_df[var]\n",
    "    y_pred = processed_df[var]\n",
    "    \n",
    "    precision_scores[var] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_scores[var] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1_scores[var] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    # accuracy_scores[var] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Print Per Variable Type Metrics\n",
    "print(\"\\nEvaluation Per Variable Type:\")\n",
    "for var in variable_types:\n",
    "    print(f\"{var}:\")\n",
    "    print(f\"  Precision: {precision_scores[var] * 100:.2f}%  Recall: {recall_scores[var] * 100:.2f}%  F1 Score: {f1_scores[var] * 100:.2f}%\")\n",
    "    # print(f\"  Accuracy: {accuracy_scores[var] * 100:.2f}%\")\n",
    "\n",
    "# 3. Overall Evaluation for All Variables\n",
    "ground_truth_flat = ground_truth_df[variable_types].values.flatten()\n",
    "processed_flat = processed_df[variable_types].values.flatten()\n",
    "\n",
    "overall_precision = precision_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "overall_recall = recall_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "overall_f1 = f1_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "# overall_accuracy = accuracy_score(ground_truth_flat, processed_flat)\n",
    "\n",
    "# Print Overall Metrics\n",
    "print(\"\\nOverall Evaluation for All Variables:\")\n",
    "print(f\"Overall Precision: {overall_precision * 100:.2f}%\")\n",
    "print(f\"Overall Recall: {overall_recall * 100:.2f}%\")\n",
    "print(f\"Overall F1 Score: {overall_f1 * 100:.2f}%\")\n",
    "# print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Distribution of Log Message Accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(log_message_accuracies, bins=5, kde=True)\n",
    "plt.title('Distribution of Log Message Accuracies', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Save the plot\n",
    "output_file_path = os.path.join(output_directory, 'distribution_of_log_message_accuracies.png')\n",
    "plt.savefig(output_file_path, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Plotting System-wise Average Accuracies\n",
    "\n",
    "# Add the calculated accuracies to the ground truth DataFrame\n",
    "ground_truth_df['Accuracy'] = log_message_accuracies\n",
    "\n",
    "# Group by 'System' and calculate average accuracy per system\n",
    "system_wise_accuracy = ground_truth_df.groupby('System')['Accuracy'].mean()\n",
    "\n",
    "# Normalize accuracy values for color mapping\n",
    "norm = mcolors.Normalize(vmin=system_wise_accuracy.min(), vmax=system_wise_accuracy.max())\n",
    "colors = [cm.viridis(norm(value)) for value in system_wise_accuracy.values]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(x=system_wise_accuracy.index, y=system_wise_accuracy.values)\n",
    "\n",
    "# Assign color to each bar manually to follow the computed color based on accuracy values\n",
    "for bar, color in zip(ax.patches, colors):\n",
    "    bar.set_facecolor(color)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('System-wise Average Accuracy', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('System')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add data labels to each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height() * 100:.2f}%', \n",
    "                (p.get_x() + p.get_width() / 2, p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(os.path.join(output_directory, 'system_wise_average_accuracy.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Per Variable Type Accuracy Comparison with Data Labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Set color manually using a color map rather than using the deprecated palette without hue\n",
    "colors = sns.color_palette('Blues', len(recall_scores))\n",
    "ax = sns.barplot(x=list(recall_scores.keys()), y=list(recall_scores.values()), palette=colors)\n",
    "\n",
    "# Add data labels on each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height() * 100:.2f}%', \n",
    "                (p.get_x() + p.get_width() / 2, p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "plt.title('Per Variable Type Recall Comparison', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Variable Type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Save the plot before showing it\n",
    "plt.savefig(os.path.join(output_directory, 'per_variable_type_recall_comparison.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Box Plot of Metrics Per Variable Type\n",
    "metrics_all = pd.DataFrame({'Precision': list(precision_scores.values()),\n",
    "                            'Recall': list(recall_scores.values()),\n",
    "                            'F1 Score': list(f1_scores.values())}, index=variable_types)\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.boxplot(data=metrics_all)\n",
    "\n",
    "# Add median value labels on each box\n",
    "medians = metrics_all.median()\n",
    "\n",
    "for i, median in enumerate(medians):\n",
    "    ax.text(i, median, f'{median:.2f}', horizontalalignment='center', color='black', size=10)\n",
    "\n",
    "plt.title('Box Plot of Metrics per Variable Type', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "# Save the plot before showing it\n",
    "plt.savefig(os.path.join(output_directory, 'box_plot_of_metrics_per_tariable_type.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Cumulative Distribution of Log Accuracy\n",
    "cumulative_accuracies = np.cumsum(log_message_accuracies) / np.arange(1, len(log_message_accuracies) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cumulative_accuracies) + 1), cumulative_accuracies, marker='o')\n",
    "plt.title('Cumulative Distribution of Log Accuracies')\n",
    "plt.xlabel('Log Index')\n",
    "plt.ylabel('Cumulative Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
