{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from config import config\n",
    "from gpt_model import get_completion_from_gpt\n",
    "from claude import get_completion_from_claude\n",
    "from format_output import Format_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR to your repository root.\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "# Set the DATA_DIR to the directory where your data resides.\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/loghub_2k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "\n",
    "now_time = datetime.datetime.now()\n",
    "date_string = now_time.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "save_dir_separator = now_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "save_dir_now = os.path.join(save_dir_path, save_dir_separator)\n",
    "raw_save_dir = os.path.join(save_dir_now, \"raw_results/\")\n",
    "Path(raw_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "raw_output_file_name = 'output.txt'\n",
    "raw_output_file_path = raw_save_dir + raw_output_file_name\n",
    "prompt_file_name = 'prompt.txt'\n",
    "prompt_file_path = raw_save_dir + prompt_file_name\n",
    "\n",
    "formatted_save_dir = os.path.join(save_dir_now, \"formatted_results/\")\n",
    "Path(formatted_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "formatted_output_file_name = 'output_processed.txt'\n",
    "formatted_output_file_path = formatted_save_dir + formatted_output_file_name\n",
    "\n",
    "# the application logs\n",
    "log_save_dir = os.path.join(save_dir_now, \"logs/\")  # for the outputs of logging module\n",
    "Path(log_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "log_file_name = 'output.log'\n",
    "app_log_file_path = log_save_dir + log_file_name\n",
    "logging.basicConfig(filename=app_log_file_path, filemode='a',\n",
    "                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "                        datefmt='%d-%b-%y %H:%M:%S', level=logging.INFO, force = True)\n",
    "\n",
    "# raw datasets to be analyzed\n",
    "log_file_path = os.path.join(DATA_DIR, 'sample_combined_raw_logs.txt')\n",
    "\n",
    "# #ground truth for the analyzed dataset\n",
    "# ground_truth = os.path.join(DATA_DIR, 'ground_truth_template.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load logs from log file line by line\n",
    "def load_logs(file_path):\n",
    "    logging.info(\"logs will be loaded from \" + str(file_path))\n",
    "    with open(file_path, 'r') as file:\n",
    "        log_message = file.readlines()\n",
    "    return log_message\n",
    "\n",
    "logs = load_logs(log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to keep the outputs of the model\n",
    "output_data = []\n",
    "index = 0 \n",
    "\n",
    "# logs per call\n",
    "logs_per_call = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run with few-shot prompt\n",
    "while index < len(logs):\n",
    "    log_message = str(logs[index])\n",
    "    logging.info(\"current index: \" + str(index))\n",
    "    logging.info(\"current log message: \")\n",
    "    logging.info(log_message)\n",
    "    \n",
    "    prompt_enh = f\"\"\"\n",
    "    You will be provided with a log message delimited by <MSG> and </MSG>. \n",
    "    The log texts describe various system events in a software system. \n",
    "    A log message usually contains a header that is automatically produced by the logging framework, including information such as timestamp, class, and logging level (INFO, DEBUG, WARN etc.). \n",
    "    The log message typically consists of two parts: \n",
    "    1. Template - message body, that contains constant strings (or keywords) describing the system events; \n",
    "    2. Parameters/Variables - dynamic variables, which reflect specific runtime status.\n",
    "    You must identify and abstract all the dynamic variables in the log message with suitable placeholders inside angle brackets to extract the corresponding template.\n",
    "    You must output the template corresponding to the log message. Print only the input log's template surrounded by <TPL> and </TPL>. \n",
    "    Never print an explanation of how the template is constructed.\n",
    "\n",
    "    Here are a few examples of log messages (labeled with Q:) and corresponding templates (labeled with A:):\n",
    "\n",
    "    Q: <MSG>[081109 204453 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.85:50010 is added to blk_2377150260128098806 size 67108864]</MSG>\n",
    "    A: <TPL>[BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>:<*> is added to <*> size <*>]</TPL>\n",
    "\n",
    "    Q: <MSG>- 1129734520 2005.10.19 R17-M0-N0-I:J18-U01 2005-10-19-08.08.40.058960 R17-M0-N0-I:J18-U01 RAS KERNEL INFO shutdown complete</MSG>\n",
    "    A: <TPL>shutdown complete</TPL>\n",
    "\n",
    "    Q: <MSG>20231114T101914E ERROR 14 while processing line 123: cannot find input '42'</MSG>\n",
    "    A: <TPL>ERROR <*> while processing line <*>: cannot find input <*></TPL>\n",
    "\n",
    "    Q: <MSG>2023-01-14 23:05:14 INFO: Reading data from /user/input/file.txt</MSG>\n",
    "    A: <TPL>Reading data from <*> </TPL>\n",
    "\n",
    "    Here is the input log message: <MSG>{log_message}</MSG>\n",
    "    Please print the corresponding template.\n",
    "    \"\"\"\n",
    "    #response = get_completion_from_claude(prompt_enh)\n",
    "    response = get_completion_from_gpt(prompt_enh)\n",
    "    logging.info(\"the response received from the model: \")\n",
    "    formatted_response = Format_output.format_response(response)\n",
    "    logging.info(formatted_response)\n",
    "    output_data.append(formatted_response)\n",
    "    if index == 0:\n",
    "        Format_output.save_prompt(prompt_file_path, prompt_enh)\n",
    "\n",
    "    index += logs_per_call\n",
    "\n",
    "    # Sleep for 0.5 second after every log message\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(raw_output_file_path, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed output saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/20241128133700/formatted_results/output_processed.txt\n"
     ]
    }
   ],
   "source": [
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(raw_output_file_path, formatted_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ground_truth_file_path = os.path.join(DATA_DIR, \"sample_ground_truth_template.csv\")  # Update the path if necessary\n",
    "processed_output_file_path = formatted_output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load ground truth data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "ground_truth_templates = ground_truth_df['EventTemplate'].tolist()\n",
    "ground_truth_systems = ground_truth_df['System'].tolist()\n",
    "\n",
    "# Load processed output data\n",
    "with open(processed_output_file_path, 'r') as processed_file:\n",
    "    processed_templates = [line.strip() for line in processed_file.readlines()]\n",
    "\n",
    "# Ensure the lists are of the same length for comparison\n",
    "min_length = min(len(ground_truth_templates), len(processed_templates))\n",
    "ground_truth_templates = ground_truth_templates[:min_length]\n",
    "processed_templates = processed_templates[:min_length]\n",
    "ground_truth_systems = ground_truth_systems[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Accuracy: 39.38%\n",
      "Precision: 39.38%\n",
      "Recall: 39.38%\n",
      "F1 Score: 39.38%\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(ground_truth_templates, processed_templates)\n",
    "precision = precision_score(ground_truth_templates, processed_templates, average='weighted', zero_division=0)\n",
    "recall = recall_score(ground_truth_templates, processed_templates, average='weighted', zero_division=0)\n",
    "f1 = f1_score(ground_truth_templates, processed_templates, average='weighted', zero_division=0)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Parsing Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correctly Parsed Templates per System:\n",
      "Apache: 4\n",
      "HPC: 1\n",
      "Linux: 8\n",
      "Zookeeper: 11\n",
      "BGL: 7\n",
      "Hadoop: 4\n",
      "Mac: 4\n",
      "HealthApp: 8\n",
      "OpenSSH: 8\n",
      "Spark: 11\n",
      "HDFS: 1\n",
      "OpenStack: 1\n",
      "Thunderbird: 8\n"
     ]
    }
   ],
   "source": [
    "# Calculate correctly parsed templates for each system\n",
    "correct_parsed_counts = {}\n",
    "for system, gt_template, processed_template in zip(ground_truth_systems, ground_truth_templates, processed_templates):\n",
    "    if gt_template == processed_template:\n",
    "        if system not in correct_parsed_counts:\n",
    "            correct_parsed_counts[system] = 0\n",
    "        correct_parsed_counts[system] += 1\n",
    "\n",
    "# Print correctly parsed templates for each system\n",
    "print(\"\\nCorrectly Parsed Templates per System:\")\n",
    "for system, count in correct_parsed_counts.items():\n",
    "    print(f\"{system}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
