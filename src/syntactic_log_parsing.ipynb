{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from config import config\n",
    "from gpt_model import get_completion_from_gpt\n",
    "from claude import get_completion_from_claude\n",
    "from ollama import get_completion_from_ollama\n",
    "from format_output import Format_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import seaborn as sns\n",
    "from math import pi\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "from rouge import Rouge\n",
    "from nltk.metrics import edit_distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR to your repository root.\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "# Set the DATA_DIR to the directory where your data resides.\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/loghub_2k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"gpt-3.5-turbo\" # change this everytime with new model\n",
    "\n",
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "now_time = datetime.datetime.now()\n",
    "date_string =  \"Syntactic_\" + llm_model_name + now_time.strftime('_%Y-%m-%d-%H-%M')\n",
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "\n",
    "save_dir_path_now = os.path.join(save_dir_path, date_string)\n",
    "\n",
    "raw_save_dir_path = os.path.join(save_dir_path_now, \"raw_results/\")\n",
    "Path(raw_save_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_templates_output_file_name = 'log_template_output.txt'\n",
    "variables_output_file_name = 'variables_output.csv'\n",
    "\n",
    "log_templates_output_file_path = raw_save_dir_path + log_templates_output_file_name\n",
    "variables_output_file_path = raw_save_dir_path + variables_output_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ground_truth_file_path = os.path.join(DATA_DIR, \"ground_truth_template.csv\")\n",
    "raw_logs_file_path = os.path.join(DATA_DIR, \"combined_raw_logs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw log messages\n",
    "with open(raw_logs_file_path, 'r') as raw_file:\n",
    "    log_samples = [line.strip() for line in raw_file.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <TPL>workerEnv.init() ok <*> </TPL>\n",
      "10: <TPL>action start <*> <*> boot  (command <*>)</TPL>\n",
      "20: <TPL>An ServerFileSystem domain panic has occurred on storage<*></TPL>\n",
      "30: <TPL>ServerFileSystem domain <*> is no longer served by node <*> </TPL>\n",
      "40: <TPL>temperature <*> <*> normal</TPL>\n",
      "50: <TPL>node node-148 has detected an available network connection on network <*> via interface alt0</TPL>\n",
      "60: <TPL>User unknown timed out after <*> seconds at Sat Jun <*> <*> <*> </TPL>\n",
      "70: <TPL>session opened for user <*> by LOGIN(uid=*)</TPL>\n",
      "80: <TPL>warning: can't get client address: Connection reset by peer</TPL>\n",
      "90: <TPL>klogd startup succeeded</TPL>\n",
      "100: <TPL>Kernel command line: ro root=LABEL=/ rhgb quiet</TPL>\n",
      "110: <TPL>Console: colour VGA+ 80x25</TPL>\n",
      "120: <TPL>rpc.idmapd startup succeeded</TPL>\n",
      "130: <TPL>Enabling unmasked SIMD FPU exception support... done.</TPL>\n",
      "140: <TPL>Linux Plug and Play Support v0.97 (c) Adam Belay</TPL>\n",
      "150: <TPL>apm: BIOS version <*> Flags <*> (Driver version <*>)</TPL>\n",
      "160: <TPL>Bringing up loopback interface:  succeeded</TPL>\n",
      "170: <TPL>chrome.exe - proxy.cse.cuhk.edu.hk:<*> open through proxy proxy.cse.cuhk.edu.hk:<*> HTTPS</TPL>\n",
      "180: <TPL>Send worker leaving thread</TPL>\n",
      "190: <TPL>Unexpected Exception:</TPL>\n",
      "200: <TPL>FOLLOWING</TPL>\n",
      "210: <TPL>tickTime set to <*> </TPL>\n",
      "220: <TPL>Notification: <*> (n.leader), <*> (n.zxid), <*> (n.round), FOLLOWING (n.state), <*> (n.sid), <*> (n.peerEPoch), LEADING (my state)</TPL>\n",
      "230: <TPL>CE sym <*>, at <*>, mask <*></TPL>\n",
      "240: <TPL>instruction address: 0x<*> </TPL>\n",
      "250: <TPL>machine state register: 0x<*> </TPL>\n",
      "260: <TPL>data address space................0</TPL>\n",
      "270: <TPL>special purpose registers:</TPL>\n",
      "280: <TPL>Node card is not fully functional</TPL>\n",
      "290: <TPL>ciod: Z coordinate <*> exceeds physical dimension <*> at line <*> of node map file <*> </TPL>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Generate log template using zero-shot learning\n",
    "counter_1 = 0\n",
    "syntax_based_log_templates = []\n",
    "for log in log_samples:\n",
    "    prompt = f\"\"\"\n",
    "    You will be provided with a log message delimited by <MSG> and </MSG>. \n",
    "    The log texts describe various system events in a software system. \n",
    "    A log message usually contains a header that is automatically produced by the logging framework, including information such as timestamp, class, and logging level (INFO, DEBUG, WARN etc.). \n",
    "    The log message typically consists of two parts: \n",
    "    1. Template - message body, that contains constant strings (or keywords) describing the system events; \n",
    "    2. Parameters/Variables - dynamic variables, which reflect specific runtime status.\n",
    "    You must identify and abstract all the dynamic variables in the log message with suitable placeholders inside angle brackets to extract the corresponding template.\n",
    "    You must output the template corresponding to the log message. Print only the input log's template surrounded by <TPL> and </TPL>. \n",
    "    Never print an explanation of how the template is constructed.\n",
    "\n",
    "    Here are a few examples of log messages (labeled with Q:) and corresponding templates (labeled with A:):\n",
    "\n",
    "    Q: <MSG>[081109 204453 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.85:50010 is added to blk_2377150260128098806 size 67108864]</MSG>\n",
    "    A: <TPL>[BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>:<*> is added to <*> size <*>]</TPL>\n",
    "\n",
    "    Q: <MSG>- 1129734520 2005.10.19 R17-M0-N0-I:J18-U01 2005-10-19-08.08.40.058960 R17-M0-N0-I:J18-U01 RAS KERNEL INFO shutdown complete</MSG>\n",
    "    A: <TPL>shutdown complete</TPL>\n",
    "\n",
    "    Q: <MSG>20231114T101914E ERROR 14 while processing line 123: cannot find input '42'</MSG>\n",
    "    A: <TPL>ERROR <*> while processing line <*>: cannot find input <*></TPL>\n",
    "\n",
    "    Q: <MSG>2023-01-14 23:05:14 INFO: Reading data from /user/input/file.txt</MSG>\n",
    "    A: <TPL>Reading data from <*> </TPL>\n",
    "\n",
    "    Here is the input log message: <MSG>{log}</MSG>\n",
    "    Please print the corresponding template.\n",
    "    \"\"\"\n",
    "\n",
    "    response = get_completion_from_gpt(prompt)\n",
    "    # response = get_completion_from_claude(prompt)\n",
    "    # response = get_completion_from_ollama(prompt)\n",
    "    \n",
    "    syntax_based_log_templates.append(response)\n",
    "    \n",
    "    if counter_1 % 10 == 0:\n",
    "        print(f'{counter_1}: {response}')\n",
    "        \n",
    "    counter_1+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(syntax_based_log_templates))\n",
    "\n",
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(log_templates_output_file_path, syntax_based_log_templates)\n",
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(log_templates_output_file_path, log_templates_output_file_path)\n",
    "print(f\"Syntax-based Log templates are saved to: {log_templates_output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "processed_log_templates_file_path = log_templates_output_file_path\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "ground_truth_log_templates = ground_truth_df['EventTemplate'].tolist()\n",
    "ground_truth_systems = ground_truth_df['System'].tolist()\n",
    "\n",
    "output_directory = os.path.dirname(processed_log_templates_file_path)\n",
    "\n",
    "\n",
    "# Load processed output data\n",
    "with open(processed_log_templates_file_path, 'r') as processed_file:\n",
    "    processed_log_templates = [line.strip() for line in processed_file.readlines()]\n",
    " \n",
    "\n",
    "# Ensure the lists are of the same length for comparison\n",
    "min_length = min(len(ground_truth_log_templates), len(processed_log_templates))\n",
    "ground_truth_log_templates = ground_truth_log_templates[:min_length]\n",
    "processed_log_templates = processed_log_templates[:min_length]\n",
    "ground_truth_systems = ground_truth_systems[:min_length]\n",
    "\n",
    "# Calculate evaluation metrics for processed_log_templates\n",
    "precision = precision_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "recall = recall_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "f1 = f1_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "\n",
    "# Print evaluation metrics for processed_log_templates\n",
    "print(f\"Log Templates Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Log Templates Recall: {recall * 100:.2f}%\")\n",
    "print(f\"Log Templates F1 Score: {f1 * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly parsed log templates for each system\n",
    "correct_parsed_counts = {}\n",
    "for system, gt_template, processed_template in zip(ground_truth_systems, ground_truth_log_templates, processed_log_templates):\n",
    "    if gt_template == processed_template:\n",
    "        if system not in correct_parsed_counts:\n",
    "            correct_parsed_counts[system] = 0\n",
    "        correct_parsed_counts[system] += 1\n",
    "\n",
    "# Print correctly parsed log templates for each system\n",
    "print(\"\\nCorrectly Parsed Log Templates per System:\")\n",
    "total=0\n",
    "for system, count in correct_parsed_counts.items():\n",
    "    total +=count\n",
    "    print(f\"{system}: {count}\")\n",
    "\n",
    "print(f\"Total correctly parsed log templates: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that NLTK and other required libraries are installed\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Description of Metrics\n",
    "print(\"Description of Metrics:\\n\")\n",
    "print(\"Edit Distance: Measures the number of character-level edit operations (insertion, deletion, substitution) needed to transform one string into another. A normalized score between 0 and 1 is calculated by dividing the edit distance by the total character length of the longer string, where 0 indicates identical strings and 1 indicates completely different strings.\\n\")\n",
    "print(\"ROUGE-L: Measures the similarity between two texts based on the longest common subsequence. The score ranges from 0 to 1, with 1 indicating high similarity. ROUGE-L focuses on recall and is useful for comparing the structure of sentences.\\n\")\n",
    "print(\"Cosine Similarity: Measures the cosine of the angle between two vectors, which represent the texts in a multi-dimensional space. The score ranges from 0 to 1, with 1 indicating that the vectors are identical, meaning high similarity.\\n\")\n",
    "print(\"BLEU (Bilingual Evaluation Understudy): Measures the similarity between a predicted sentence and one or more reference sentences by calculating the overlap of n-grams. The score ranges from 0 to 1, with 1 indicating high similarity. BLEU is commonly used for machine translation evaluation.\\n\")\n",
    "\n",
    "# 1. Edit Distance for Each System\n",
    "def calculate_edit_distance_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    system_distances = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        distance = edit_distance(gt, processed)\n",
    "        max_length = max(len(gt), len(processed))\n",
    "        normalized_distance = distance / max_length if max_length > 0 else 0\n",
    "        if system not in system_distances:\n",
    "            system_distances[system] = []\n",
    "        system_distances[system].append(normalized_distance)\n",
    "    \n",
    "    avg_distances = {}\n",
    "    for system, distances in system_distances.items():\n",
    "        avg_distance = sum(distances) / len(distances)\n",
    "        avg_distances[system] = avg_distance\n",
    "        print(f\"System '{system}': Normalized Average Edit Distance = {avg_distance:.2f} (Range: 0 to 1, where lower is better)\")\n",
    "    \n",
    "    return avg_distances\n",
    "\n",
    "# 2. ROUGE-L Score for Each System\n",
    "def calculate_rouge_l_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    rouge = Rouge()\n",
    "    system_rouge_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        if not gt.strip() or not processed.strip():\n",
    "            continue  # Skip empty ground truth or processed templates\n",
    "        scores = rouge.get_scores(processed, gt, avg=True)\n",
    "        rouge_l_score = scores['rouge-l']['f']\n",
    "        if system not in system_rouge_scores:\n",
    "            system_rouge_scores[system] = []\n",
    "        system_rouge_scores[system].append(rouge_l_score)\n",
    "    \n",
    "    avg_rouge_scores = {}\n",
    "    print(\"\\nAverage ROUGE-L Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, rouge_scores in system_rouge_scores.items():\n",
    "        avg_rouge_score = sum(rouge_scores) / len(rouge_scores)\n",
    "        avg_rouge_scores[system] = avg_rouge_score\n",
    "        print(f\"System '{system}':              Average ROUGE-L Score = {avg_rouge_score:.2f}\")\n",
    "    \n",
    "    return avg_rouge_scores\n",
    "\n",
    "# 3. Cosine Similarity for Each System\n",
    "def calculate_cosine_similarity_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    system_cosine_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        if not gt.strip() or not processed.strip():\n",
    "            continue  # Skip empty ground truth or processed templates\n",
    "        vectors = vectorizer.fit_transform([gt, processed])\n",
    "        cosine_sim = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "        if system not in system_cosine_scores:\n",
    "            system_cosine_scores[system] = []\n",
    "        system_cosine_scores[system].append(cosine_sim)\n",
    "    \n",
    "    avg_cosine_scores = {}\n",
    "    print(\"\\nAverage Cosine Similarity Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, cosine_scores in system_cosine_scores.items():\n",
    "        avg_cosine_score = sum(cosine_scores) / len(cosine_scores)\n",
    "        avg_cosine_scores[system] = avg_cosine_score\n",
    "        print(f\"System '{system}':              Average Cosine Similarity Score = {avg_cosine_score:.2f}\")\n",
    "    \n",
    "    return avg_cosine_scores\n",
    "\n",
    "# 4. BLEU Score for Each System\n",
    "def calculate_bleu_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    system_bleu_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        reference_tokens = nltk.word_tokenize(gt)\n",
    "        hypothesis_tokens = nltk.word_tokenize(processed)\n",
    "        bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "        if system not in system_bleu_scores:\n",
    "            system_bleu_scores[system] = []\n",
    "        system_bleu_scores[system].append(bleu_score)\n",
    "    \n",
    "    avg_bleu_scores = {}\n",
    "    print(\"\\nAverage BLEU Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, bleu_scores in system_bleu_scores.items():\n",
    "        avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "        avg_bleu_scores[system] = avg_bleu_score\n",
    "        print(f\"System '{system}':              Average BLEU Score = {avg_bleu_score:.2f}\")\n",
    "    \n",
    "    return avg_bleu_scores\n",
    "\n",
    "# Calculating metrics for the given log templates\n",
    "print(\"Comparing the processed log templates to the ground truth log templates:\\n\")\n",
    "edit_distances = calculate_edit_distance_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "rouge_l_scores = calculate_rouge_l_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "cosine_similarity_scores = calculate_cosine_similarity_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "bleu_scores = calculate_bleu_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualizations\n",
    "\n",
    "def create_visualizations(edit_distances, rouge_l_scores, cosine_similarity_scores, bleu_scores, output_directory):\n",
    "    systems = list(edit_distances.keys())\n",
    "    edit_values = list(edit_distances.values())\n",
    "    rouge_values = list(rouge_l_scores.values())\n",
    "    cosine_values = list(cosine_similarity_scores.values())\n",
    "    bleu_values = list(bleu_scores.values())\n",
    "\n",
    "    # Edit Distance Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, edit_values, color='skyblue')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Normalized Average Edit Distance')\n",
    "    plt.title('Normalized Average Edit Distance per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(edit_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'edit_distance_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # ROUGE-L Score Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, rouge_values, color='lightgreen')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average ROUGE-L Score')\n",
    "    plt.title('Average ROUGE-L Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(rouge_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'rouge_l_score_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Cosine Similarity Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, cosine_values, color='lightcoral')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average Cosine Similarity Score')\n",
    "    plt.title('Average Cosine Similarity Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(cosine_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'cosine_similarity_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # BLEU Score Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, bleu_values, color='lightblue')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average BLEU Score')\n",
    "    plt.title('Average BLEU Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(bleu_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'bleu_score_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Combined Metrics Bar Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    ax1.bar(systems, edit_values, color='skyblue', alpha=0.6, label='Edit Distance')\n",
    "    ax1.set_xlabel('System')\n",
    "    ax1.set_ylabel('Normalized Average Edit Distance', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    for i, value in enumerate(edit_values):\n",
    "        ax1.text(i, value + 0.02, f'{value:.2f}', ha='center', color='blue')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(systems, rouge_values, color='green', marker='o', linestyle='-', label='ROUGE-L Score')\n",
    "    ax2.plot(systems, cosine_values, color='red', marker='x', linestyle='-', label='Cosine Similarity')\n",
    "    ax2.plot(systems, bleu_values, color='purple', marker='^', linestyle='-', label='BLEU Score')\n",
    "    ax2.set_ylabel('Scores', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "    plt.title('Normalized Edit Distance, ROUGE-L, Cosine Similarity, and BLEU Score per System')\n",
    "    fig.tight_layout()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.savefig(os.path.join(output_directory, 'combined_scores_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Create and save visualizations\n",
    "create_visualizations(edit_distances, rouge_l_scores, cosine_similarity_scores, bleu_scores, output_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
