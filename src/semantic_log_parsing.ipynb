{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from config import config\n",
    "from gpt_model import get_completion_from_gpt\n",
    "from claude import get_completion_from_claude\n",
    "from format_output import Format_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import ast \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR to your repository root.\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "# Set the DATA_DIR to the directory where your data resides.\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/loghub_2k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "\n",
    "now_time = datetime.datetime.now()\n",
    "date_string = \"Semantic_\" + now_time.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "save_dir_separator = \"Semantic_\" + now_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "save_dir_now = os.path.join(save_dir_path, save_dir_separator)\n",
    "raw_save_dir = os.path.join(save_dir_now, \"semantic_raw_results/\")\n",
    "Path(raw_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "semantic_template_file_name = 'semantic_output.txt'\n",
    "variables_output_file_name = 'variables_output.csv'\n",
    "semantic_template_output_file_path = raw_save_dir + semantic_template_file_name\n",
    "variables_output_file_path = raw_save_dir + variables_output_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ground_truth_file_path = os.path.join(DATA_DIR, \"sample_ground_truth_template.csv\")\n",
    "raw_log_file_path = os.path.join(DATA_DIR, \"sample_combined_raw_logs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "ground_truth_log_templates = ground_truth_df['EventTemplate'].tolist()\n",
    "ground_truth_variable_templates = ground_truth_df['OID'].tolist()\n",
    "ground_truth_systems = ground_truth_df['System'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw log messages\n",
    "with open(raw_log_file_path, 'r') as raw_file:\n",
    "    raw_logs = [line.strip() for line in raw_file.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <TPL> Initialization of worker environment successful for workers2 properties file at /etc/httpd/conf/workers2.properties. </TPL>\n",
      "10: <TPL> High temperature warning on gige5 interface. </TPL>\n",
      "20: <TPL> Command has been aborted. </TPL>\n",
      "30: <TPL> CPU has an L2 cache size of 256K. </TPL>\n",
      "40: <TPL> Connection error while trying to connect to tcpconn4.tencent.com through proxy proxy.cse.cuhk.edu.hk:5070. </TPL>\n",
      "50: <TPL> autopurge.snapRetainCount set to 3 </TPL>\n",
      "60: <TPL> RAS KERNEL FATAL disable store gathering </TPL>\n",
      "70: <TPL> Z coordinate exceeds physical dimension in node map file. </TPL>\n",
      "80: <TPL> ContainerLauncher received Shuffle port for a specific map task attempt. </TPL>\n",
      "90: <TPL> Path not allowed in target domain for Safari SearchHelper service due to missing bundle service. </TPL>\n",
      "100: <TPL> Display woke up notification posted by WindowServer. </TPL>\n",
      "110: <TPL> insertHiHealthData() bulkSaveDetailHiHealthData fail </TPL>\n",
      "\n",
      "<TPL> errorCode = 4, errorMessage = ERR_DATA_INSERT </TPL>\n",
      "120: <TPL> User requested to close the SSH connection. </TPL>\n",
      "130: <TPL> Multiple authentication failures for root user from IP address 5.36.59.76. </TPL>\n",
      "140: <TPL> Reading broadcast variable took 160 ms. </TPL>\n",
      "150: <TPL> Block blk_7128370237687728475 with size 67108864 is added to the block map by the NameSystem. </TPL>\n",
      "160: <TPL> Request sent to delete a specific block from a data node. </TPL>\n",
      "170: <TPL> Instance with ID b9000564-fe1a-409b-b8cc-1e88b294cd1d took 19.84 seconds to build. </TPL>\n",
      "180: <TPL> Identification of i8042 AUX port at memory addresses 0x60, 0x64 with IRQ 12 on tbird-admin1 at 12:10:43. </TPL>\n",
      "190: <TPL> ACPI: LAPIC enabled </TPL>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Reformulate log messages with semantic understanding\n",
    "counter=0\n",
    "enhanced_prompts = []\n",
    "enhanced_prompts_file_path = os.path.join(save_dir_now, \"enhanced_prompts.txt\")\n",
    "for raw_log in raw_logs:\n",
    "    new_prompt=f\"\"\"You are provided with a log message. Your task is to understand and extract the meaning behind the semi-structured log message.\n",
    "                      \n",
    "                    Log message: {raw_log}. \n",
    "\n",
    "                    A log message usually contains a header that is automatically produced by the logging framework, including information such as timestamp, class, and logging level (INFO, DEBUG, WARN etc.).\n",
    "                    Ignore all these details and just understand meaning behind the natural languagae text which is in the log content.\n",
    "\n",
    "                    The log content typically consists of many parts: \n",
    "                    1. Template - message body, that contains constant strings (or keywords) describing the system events; \n",
    "                    2. Parameters/Variables - dynamic variables, which reflect specific runtime status;\n",
    "\n",
    "                    Please capture the essential context and meaning from the log message to understand the reasoning behind each raw log.\n",
    "                    Provide only the meaning in 20-25 words from each log message surrounded by <TPL> and </TPL>. \n",
    "                    Never provide an explanation of how the meaning is constructed.\n",
    "                \"\"\"\n",
    "    \n",
    "    enhanced_prompt = get_completion_from_gpt(new_prompt)\n",
    "    enhanced_prompts.append(enhanced_prompt)\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter}: {enhanced_prompt}')\n",
    "    counter+=1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n"
     ]
    }
   ],
   "source": [
    "print(len(enhanced_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic log templates saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_20241203225023/enhanced_prompts.txt\n"
     ]
    }
   ],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(enhanced_prompts_file_path, enhanced_prompts)\n",
    "\n",
    "# Save all semantic log templates to a file\n",
    "print(f\"Semantic log templates saved to: {enhanced_prompts_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed output saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_20241203225023/enhanced_prompts.txt\n"
     ]
    }
   ],
   "source": [
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(enhanced_prompts_file_path, enhanced_prompts_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <TPL>workerEnv.init() ok <*> </TPL>\n",
      "10: <TPL>gige temperature <*> <*> warning</TPL>\n",
      "20: <TPL>boot_cmd abort <*> <*> Command has been aborted</TPL>\n",
      "30: <TPL>CPU: L2 cache: <*> </TPL>\n",
      "40: <TPL>Could not connect through proxy <*> - Proxy closed the connection unexpectedly.</TPL>\n",
      "50: <TPL>autopurge.snapRetainCount set to <*> </TPL>\n",
      "60: <TPL>FATAL disable store gathering</TPL>\n",
      "70: <TPL>ciod: Z coordinate <*> exceeds physical dimension <*> at line <*> of node map file <*> </TPL>\n",
      "80: <TPL>Shuffle port returned by ContainerManager for attempt_<*>_<*>_<*>_<*> : <*></TPL>\n",
      "90: <TPL>Path not allowed in target domain for Safari SearchHelper service due to missing bundle service.</TPL>\n",
      "100: <TPL>CGXDisplayDidWakeNotification [*]: posting kCGSDisplayDidWake</TPL>\n",
      "110: <TPL>insertHiHealthData() bulkSaveDetailHiHealthData fail errorCode = <*>,errorMessage = <*></TPL>\n",
      "120: <TPL>Closed due to user request.</TPL>\n",
      "130: <TPL>Multiple authentication failures for root user from IP address <*>.</TPL>\n",
      "140: <TPL>Reading broadcast variable <*> took <*> ms</TPL>\n",
      "150: <TPL>BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>:<*> is added to <*> size <*> </TPL>\n",
      "160: <TPL>BLOCK* ask <*>:<*> to delete  <*></TPL>\n",
      "170: <TPL>Took <*> seconds to build instance.</TPL>\n",
      "180: <TPL>serio: i8042 AUX port at 0x*,0x* irq *</TPL>\n",
      "190: <TPL>ACPI: LAPIC (acpi_id[*] lapic_id[*] enabled)</TPL>\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Generate log template using zero-shot learning\n",
    "counter_1 = 0\n",
    "semantic_based_templates = []\n",
    "for raw_log, enhanced_prompt in zip(raw_logs, enhanced_prompts):\n",
    "    semantic_prompt = f\"\"\"You will be provided with a log message delimited by <MSG> and </MSG>. \n",
    "    You are also provided with the meaning or understanding from the log message as follow: {enhanced_prompt}. \n",
    "    \n",
    "    The log message typically consists of two parts: \n",
    "    1. Template - message body, that contains constant strings (or keywords) describing the system events; \n",
    "    2. Parameters/Variables - dynamic variables, which reflect specific runtime status.\n",
    "    You must identify and abstract all the dynamic variables in the log message with suitable placeholders inside angle brackets to extract the corresponding template.\n",
    "    You must output the template corresponding to the log message. Print only the input log's template surrounded by <TPL> and </TPL>. \n",
    "    Never print an explanation of how the template is constructed.\n",
    "    Here are a few examples of log messages (labeled with Q:) and corresponding templates (labeled with A:):\n",
    "\n",
    "    Q: <MSG>[081109 204453 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.85:50010 is added to blk_2377150260128098806 size 67108864]</MSG>\n",
    "    A: <TPL>[BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>:<*> is added to <*> size <*>]</TPL>\n",
    "\n",
    "    Q: <MSG>- 1129734520 2005.10.19 R17-M0-N0-I:J18-U01 2005-10-19-08.08.40.058960 R17-M0-N0-I:J18-U01 RAS KERNEL INFO shutdown complete</MSG>\n",
    "    A: <TPL>shutdown complete</TPL>\n",
    "\n",
    "    Q: <MSG>20231114T101914E ERROR 14 while processing line 123: cannot find input '42'</MSG>\n",
    "    A: <TPL>ERROR <*> while processing line <*>: cannot find input <*></TPL>\n",
    "\n",
    "    Q: <MSG>2023-01-14 23:05:14 INFO: Reading data from /user/input/file.txt</MSG>\n",
    "    A: <TPL>Reading data from <*> </TPL>\n",
    "    Here is the input log message: <MSG>{raw_log}</MSG>\n",
    "    Please print the corresponding template.\n",
    "    \"\"\"\n",
    "    response = get_completion_from_gpt(semantic_prompt)\n",
    "    semantic_based_templates.append(response)\n",
    "    \n",
    "    if counter_1 % 10 == 0:\n",
    "        print(f'{counter_1}: {response}')\n",
    "        \n",
    "    counter_1+=1   \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter_1 = 0\n",
    "# variable_from_logs = []\n",
    "# for raw_log, enhanced_prompt in zip(raw_logs, enhanced_prompts):\n",
    "#     semantic_prompt = f\"\"\"You will be provided with a log message delimited by <MSG> and </MSG>. \n",
    "#     You are also provided with the meaning or understanding from the log message as follow: {enhanced_prompt}. \n",
    "    \n",
    "#     I want you to categorize the variable(s) in each log message as variable template. \n",
    "#     The variable should be classified within the category as below:\n",
    "#     1. Object ID [OID]\tIdentification information of an object\n",
    "#     2. Location Indicator   [LOI]\tLocation information of an object\n",
    "#     3. Object Name\t[OBN]\tName of an object\n",
    "#     4.Type Indicator\t[TID]\tType information of an object or an action\n",
    "#     5. Switch Indicator\t[SID]\tStatus of a switch variable\n",
    "#     6. Time or Duration of an Action\t[TDA]\tTime or duration of an action\n",
    "#     7. Computing Resources\t[CRS]\tInformation of computing resource\n",
    "#     8. Object Amount\t[OBA]\tAmount of an object\n",
    "#     9. Status Code\t[STC]\tStatus code of an object or an action\n",
    "#     10. Other Parameters\t[OTP]\tOther information does not belong to the above categories]. \n",
    "    \n",
    "\n",
    "\n",
    "#     Static words/parts of the log message are to be annotated with O strictly\n",
    "#     Organize your variable template within <TPL></TPL> in the following format for each Q: \n",
    "\n",
    "#     Q: <MSG>[Jul  1 22:08:16 calvisitor-10-105-163-202 WindowServer[184]: device_generate_desktop_screenshot: authw 0x7fa823c89600(2000), shield 0x7fa8258cac00(2001)]</MSG>\n",
    "#     A: <TPL>[O O O O O O O O OID OID OID OID OBA]</TPL>\n",
    "\n",
    "#     Q: <MSG>[nova-scheduler.log.1.2017-05-16_13:53:08 2017-05-16 00:00:57.129 25998 INFO nova.scheduler.host_manager [req-d724a3bd-e314-4f81-a41c-460aa91f24ae - - - - -] Successfully synced instances from host 'cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us']</MSG>\n",
    "#     A: <TPL>[O O O O O O O O O O O O LOI O LOI OBN O STC O OBA O TDA]</TPL>\n",
    "\n",
    "#     Above is an example of our log annotation process. Static words are annotated with O, object ID is annotated with OID, \n",
    "#     and two location indicators are annotated with LOI.\n",
    "\n",
    "#     Here is the input log message: <MSG>{raw_log}</MSG>\n",
    "#     Print only the variable template surrounded by <TPL> and </TPL> and nothing else for each log message. \n",
    "#     Never print an explanation of how the variable_template is constructed.\n",
    "#     \"\"\"\n",
    "#     response = get_completion_from_gpt(semantic_prompt)\n",
    "#     variable_from_logs.append(response)    \n",
    "\n",
    "#     if counter_1 % 10 == 0:\n",
    "#         print(f'{counter_1}: {response}')\n",
    "        \n",
    "#     counter_1+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: {\n",
      "    \"TID\": 1,\n",
      "    \"LOI\": 1,\n",
      "    \"OBN\": 1\n",
      "}\n",
      "10: {\n",
      "    \"OID\": 2,\n",
      "    \"OBN\": 1,\n",
      "    \"TID\": 1,\n",
      "    \"CRS\": 1,\n",
      "    \"STC\": 1,\n",
      "    \"OTP\": 1\n",
      "}\n",
      "20: {\n",
      "    \"OID\": 2,\n",
      "    \"TID\": 1,\n",
      "    \"STC\": 1\n",
      "}\n",
      "30: {\n",
      "    \"CRS\": 1,\n",
      "    \"OBN\": 1,\n",
      "    \"OTP\": 1\n",
      "}\n",
      "40: {\n",
      "    \"TID\": 1,\n",
      "    \"OBN\": 1,\n",
      "    \"LOI\": 2,\n",
      "    \"OTP\": 1\n",
      "}\n",
      "50: {\n",
      "    \"TID\": 1,\n",
      "    \"OTP\": 1\n",
      "}\n",
      "60: {\n",
      "    \"TID\": 1,\n",
      "    \"OTP\": 1\n",
      "}\n",
      "70: {\n",
      "    \"OTP\": 1,\n",
      "    \"OBN\": 1\n",
      "}\n",
      "80: {\n",
      "    \"TID\": 1,\n",
      "    \"OBN\": 1,\n",
      "    \"OTP\": 1\n",
      "}\n",
      "90: {\n",
      "    \"TID\": 1,\n",
      "    \"OTP\": 1\n",
      "}\n",
      "100: {\n",
      "    \"OID\": 1,\n",
      "    \"OBN\": 1,\n",
      "    \"TID\": 2,\n",
      "    \"OTP\": 1\n",
      "}\n",
      "110: {\n",
      "    \"OTP\": 1,\n",
      "    \"TID\": 1,\n",
      "    \"STC\": 1\n",
      "}\n",
      "120: {\n",
      "    \"TID\": 1,\n",
      "    \"LOI\": 1,\n",
      "    \"OTP\": 1\n",
      "}\n",
      "130: {\n",
      "    \"TID\": 1,\n",
      "    \"OTP\": 1,\n",
      "    \"STC\": 1\n",
      "}\n",
      "140: {\n",
      "    \"TID\": 1,\n",
      "    \"OTP\": 1,\n",
      "    \"OID\": 1,\n",
      "    \"TDA\": 1\n",
      "}\n",
      "150: {\n",
      "    \"OID\": 2,\n",
      "    \"LOI\": 1,\n",
      "    \"OBN\": 1,\n",
      "    \"TID\": 1,\n",
      "    \"OTP\": 1\n",
      "}\n",
      "160: {\n",
      "    \"TID\": 1,\n",
      "    \"LOI\": 1,\n",
      "    \"OID\": 1\n",
      "}\n",
      "170: {\n",
      "    \"OID\": 1,\n",
      "    \"TDA\": 1,\n",
      "    \"OTP\": 1\n",
      "}\n",
      "180: {\n",
      "    \"OID\": 1,\n",
      "    \"LOI\": 1,\n",
      "    \"TID\": 1,\n",
      "    \"SID\": 1,\n",
      "    \"TDA\": 1,\n",
      "    \"CRS\": 1\n",
      "}\n",
      "190: {\n",
      "    \"OID\": 2,\n",
      "    \"TID\": 1,\n",
      "    \"OTP\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "counter_1 = 0\n",
    "variable_count_from_logs = []\n",
    "for raw_log, enhanced_prompt in zip(raw_logs, enhanced_prompts):\n",
    "    semantic_prompt = f\"\"\"You will be provided with a log message delimited by <MSG> and </MSG>. \n",
    "    You are also provided with the meaning or understanding from the log message as follows: {enhanced_prompt}. \n",
    "    \n",
    "    I want you to categorize the variable(s) in each log message as a dictionary with each variable category as the key and the count of occurrences of that category as the value. \n",
    "    The variable should be classified within the categories as below:\n",
    "    1. Object ID [OID]\tIdentification information of an object\n",
    "    2. Location Indicator   [LOI]\tLocation information of an object\n",
    "    3. Object Name\t[OBN]\tName of an object\n",
    "    4. Type Indicator\t[TID]\tType information of an object or an action\n",
    "    5. Switch Indicator\t[SID]\tStatus of a switch variable\n",
    "    6. Time or Duration of an Action\t[TDA]\tTime or duration of an action\n",
    "    7. Computing Resources\t[CRS]\tInformation of computing resource\n",
    "    8. Object Amount\t[OBA]\tAmount of an object\n",
    "    9. Status Code\t[STC]\tStatus code of an object or an action\n",
    "    10. Other Parameters\t[OTP]\tOther information that does not belong to the above categories\n",
    "\n",
    "    Here is the input log message: <MSG>{raw_log}</MSG>\n",
    "    \n",
    "    Please generate a dictionary where each key represents one of the categories listed above and the value represents the count of occurrences of that category in the log message. \n",
    "    Only include categories that are present in the log message and do not print anything other than the dictionary. Never print the full name for categories just the code like \"OID\":2, etc.\n",
    "\n",
    "    Example of the format:\n",
    "    {{\n",
    "        \"OID\": 2,\n",
    "        \"LOI\": 1,\n",
    "        \"STC\": 1\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion_from_gpt(semantic_prompt)\n",
    "    variable_count_from_logs.append(response)    \n",
    "\n",
    "    if counter_1 % 10 == 0:\n",
    "        print(f'{counter_1}: {response}')\n",
    "        \n",
    "    counter_1 += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Log templates and Variable templates are saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_20241203225023/semantic_raw_results/\n"
     ]
    }
   ],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(semantic_template_output_file_path, semantic_based_templates)\n",
    "Format_output.save_raw_output(variables_output_file_path, variable_count_from_logs)\n",
    "# Save all semantic log templates to a file\n",
    "print(f\"Semantic Log templates and Variable templates are saved to: {raw_save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed output saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_20241203225023/semantic_raw_results/semantic_output.txt\n"
     ]
    }
   ],
   "source": [
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(semantic_template_output_file_path, semantic_template_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file '/Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_20241203225023/semantic_raw_results/variables_output.csv' created successfully.\n"
     ]
    }
   ],
   "source": [
    "import ast \n",
    "fieldnames = ['Index', 'OID', 'LOI', 'OBN', 'TID', 'SID', 'TDA', 'CRS', 'OBA', 'STC', 'OTP']\n",
    "\n",
    "# Writing the data to the CSV\n",
    "with open(variables_output_file_path, mode='w', newline='') as variable_file:\n",
    "    writer = csv.DictWriter(variable_file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write each row of the dictionary results\n",
    "    for index, log_dict in enumerate(variable_count_from_logs):\n",
    "        # Convert response (string) to a dictionary if necessary\n",
    "        if isinstance(log_dict, str):\n",
    "            log_dict = ast.literal_eval(log_dict)  # Safely convert string to dict\n",
    "\n",
    "        # Ensure that all keys are present, with missing ones having a value of 0\n",
    "        row_data = {key: log_dict.get(key, 0) for key in fieldnames[1:]}\n",
    "        row_data['Index'] = index + 1\n",
    "        writer.writerow(row_data)\n",
    "\n",
    "print(f\"CSV file '{variables_output_file_path}' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "processed_log_template_file_path = semantic_template_output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed output data\n",
    "with open(processed_log_template_file_path, 'r') as processed_file:\n",
    "    processed_log_templates = [line.strip() for line in processed_file.readlines()]\n",
    " \n",
    "\n",
    "# Ensure the lists are of the same length for comparison\n",
    "min_length = min(len(ground_truth_log_templates), len(processed_log_templates))\n",
    "ground_truth_log_templates = ground_truth_log_templates[:min_length]\n",
    "processed_log_templates = processed_log_templates[:min_length]\n",
    "ground_truth_systems = ground_truth_systems[:min_length]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Template Parsing Accuracy: 36.27%\n",
      "Log Template Precision: 36.79%\n",
      "Log Template Recall: 36.27%\n",
      "Log Template F1 Score: 36.44%\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics for processed_log_templates\n",
    "accuracy = accuracy_score(ground_truth_log_templates, processed_log_templates)\n",
    "precision = precision_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "recall = recall_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "f1 = f1_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "\n",
    "# Print evaluation metrics for processed_log_templates\n",
    "print(f\"Log Template Parsing Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Log Template Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Log Template Recall: {recall * 100:.2f}%\")\n",
    "print(f\"Log Template F1 Score: {f1 * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correctly Parsed Log Templates per System:\n",
      "Apache: 5\n",
      "Linux: 9\n",
      "Zookeeper: 10\n",
      "BGL: 5\n",
      "Hadoop: 4\n",
      "Mac: 3\n",
      "HealthApp: 9\n",
      "OpenSSH: 7\n",
      "Spark: 12\n",
      "HDFS: 1\n",
      "OpenStack: 2\n",
      "Thunderbird: 3\n",
      "Total correctly parsed log templates: 70\n"
     ]
    }
   ],
   "source": [
    "# Calculate correctly parsed log templates for each system\n",
    "correct_parsed_counts = {}\n",
    "for system, gt_template, processed_template in zip(ground_truth_systems, ground_truth_log_templates, processed_log_templates):\n",
    "    if gt_template == processed_template:\n",
    "        if system not in correct_parsed_counts:\n",
    "            correct_parsed_counts[system] = 0\n",
    "        correct_parsed_counts[system] += 1\n",
    "\n",
    "# Print correctly parsed log templates for each system\n",
    "print(\"\\nCorrectly Parsed Log Templates per System:\")\n",
    "total=0\n",
    "for system, count in correct_parsed_counts.items():\n",
    "    total +=count\n",
    "    print(f\"{system}: {count}\")\n",
    "\n",
    "print(f\"Total correctly parsed log templates: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Log Message Accuracy: 62.44%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m y_true \u001b[38;5;241m=\u001b[39m ground_truth_df[var]\n\u001b[1;32m     29\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m processed_df[var]\n\u001b[0;32m---> 31\u001b[0m precision_scores[var] \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m recall_scores[var] \u001b[38;5;241m=\u001b[39m recall_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     33\u001b[0m f1_scores[var] \u001b[38;5;241m=\u001b[39m f1_score(y_true, y_pred, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m, zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:2204\u001b[0m, in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   2037\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[1;32m   2038\u001b[0m     {\n\u001b[1;32m   2039\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2064\u001b[0m     zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarn\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2065\u001b[0m ):\n\u001b[1;32m   2066\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the precision.\u001b[39;00m\n\u001b[1;32m   2067\u001b[0m \n\u001b[1;32m   2068\u001b[0m \u001b[38;5;124;03m    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;124;03m    array([0.5, 1. , 1. ])\u001b[39;00m\n\u001b[1;32m   2203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2204\u001b[0m     p, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mprecision_recall_fscore_support\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2205\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2206\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpos_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2209\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwarn_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprecision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2211\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzero_division\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2213\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m p\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1789\u001b[0m, in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute precision, recall, F-measure and support for each class.\u001b[39;00m\n\u001b[1;32m   1627\u001b[0m \n\u001b[1;32m   1628\u001b[0m \u001b[38;5;124;03mThe precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;124;03m array([2, 2, 2]))\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m _check_zero_division(zero_division)\n\u001b[0;32m-> 1789\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[43m_check_set_wise_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maverage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001b[39;00m\n\u001b[1;32m   1792\u001b[0m samplewise \u001b[38;5;241m=\u001b[39m average \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/.venv/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1578\u001b[0m, in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1576\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1577\u001b[0m             average_options\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msamples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1578\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1579\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget is \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m but average=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Please \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1580\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoose another average setting, one of \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (y_type, average_options)\n\u001b[1;32m   1581\u001b[0m         )\n\u001b[1;32m   1582\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pos_label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1583\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1584\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNote that pos_label (set to \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) is ignored when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage != \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m). You may use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1589\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "# Load ground truth and processed data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "processed_df = pd.read_csv(variables_output_file_path)\n",
    "\n",
    "# Ensure the dataframes are of the same length for comparison\n",
    "min_length = min(len(ground_truth_df), len(processed_df))\n",
    "ground_truth_df = ground_truth_df[:min_length]\n",
    "processed_df = processed_df[:min_length]\n",
    "\n",
    "# 1. Per Log Message Evaluation (Overall Accuracy per Log)\n",
    "log_message_accuracies = []\n",
    "for idx in range(min_length):\n",
    "    ground_truth_row = ground_truth_df.iloc[idx, 2:]  # Skip 'Index' column\n",
    "    processed_row = processed_df.iloc[idx, 1:]        # Skip 'Index' column\n",
    "    accuracy = (ground_truth_row == processed_row).mean()\n",
    "    log_message_accuracies.append(accuracy)\n",
    "\n",
    "overall_log_accuracy = sum(log_message_accuracies) / len(log_message_accuracies)\n",
    "print(f\"Overall Log Message Accuracy: {overall_log_accuracy * 100:.2f}%\")\n",
    "\n",
    "# 2. Evaluation Per Variable Type (Precision, Recall, F1 Score for each variable type)\n",
    "variable_types = ['OID', 'LOI', 'OBN', 'TID', 'SID', 'TDA', 'CRS', 'OBA', 'STC', 'OTP']\n",
    "precision_scores = {}\n",
    "recall_scores = {}\n",
    "f1_scores = {}\n",
    "\n",
    "for var in variable_types:\n",
    "    y_true = ground_truth_df[var]\n",
    "    y_pred = processed_df[var]\n",
    "    \n",
    "    precision_scores[var] = precision_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    recall_scores[var] = recall_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "    f1_scores[var] = f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "\n",
    "# Print Per Variable Type Metrics\n",
    "print(\"\\nEvaluation Per Variable Type:\")\n",
    "for var in variable_types:\n",
    "    print(f\"{var}:\")\n",
    "    print(f\"  Precision: {precision_scores[var] * 100:.2f}%\")\n",
    "    print(f\"  Recall: {recall_scores[var] * 100:.2f}%\")\n",
    "    print(f\"  F1 Score: {f1_scores[var] * 100:.2f}%\")\n",
    "\n",
    "# 3. Overall Evaluation for All Variables\n",
    "ground_truth_flat = ground_truth_df[variable_types].values.flatten()\n",
    "processed_flat = processed_df[variable_types].values.flatten()\n",
    "\n",
    "overall_precision = precision_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "overall_recall = recall_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "overall_f1 = f1_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "\n",
    "# Print Overall Metrics\n",
    "print(\"\\nOverall Evaluation for All Variables:\")\n",
    "print(f\"Overall Precision: {overall_precision * 100:.2f}%\")\n",
    "print(f\"Overall Recall: {overall_recall * 100:.2f}%\")\n",
    "print(f\"Overall F1 Score: {overall_f1 * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
