{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from config import config\n",
    "from gpt_model import get_completion_from_gpt\n",
    "from claude import get_completion_from_claude\n",
    "from ollama import get_completion_from_ollama\n",
    "from format_output import Format_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import ast \n",
    "import seaborn as sns\n",
    "from math import pi\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "from rouge import Rouge\n",
    "from nltk.metrics import edit_distance\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR to your repository root.\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "# Set the DATA_DIR to the directory where your data resides.\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/loghub_2k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "\n",
    "now_time = datetime.datetime.now()\n",
    "date_string = \"Semantic_\" + now_time.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "save_dir_separator = \"Semantic_\" + now_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "save_dir_now = os.path.join(save_dir_path, save_dir_separator)\n",
    "raw_save_dir = os.path.join(save_dir_now, \"semantic_raw_results/\")\n",
    "Path(raw_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "semantic_template_file_name = 'semantic_output.txt'\n",
    "variables_output_file_name = 'variables_output.csv'\n",
    "semantic_template_output_file_path = raw_save_dir + semantic_template_file_name\n",
    "variables_output_file_path = raw_save_dir + variables_output_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ground_truth_file_path = os.path.join(DATA_DIR, \"sample_ground_truth_template.csv\")\n",
    "raw_log_file_path = os.path.join(DATA_DIR, \"sample_combined_raw_logs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw log messages\n",
    "with open(raw_log_file_path, 'r') as raw_file:\n",
    "    raw_logs = [line.strip() for line in raw_file.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "<TPL>The worker environment was initialized successfully.</TPL>\n",
      "10: \n",
      "Understood! I'll do my best to extract the meaning behind the semi-structured log message you provided. Here are the key points:\n",
      "\n",
      "<TPL>The gige5 temperature is 1072878243 and there is a warning.</TPL>\n",
      "\n",
      "In this log message, the most important information is the temperature value of \"1072878243\", which suggests that something related to temperature is happening or has happened. The \"warning\" part of the message indicates that there is a potential issue or problem that needs to be addressed.\n",
      "20: \n",
      "<TPL>The command has been aborted.</TPL>\n",
      "30: \n",
      "Understood! I'll do my best to extract the meaning behind each log message. Please provide the log messages one by one, and I'll respond with the essential context and meaning in a sentence surrounded by <TPL> and </TPL>.\n",
      "\n",
      "Go ahead and provide the first log message.\n",
      "40: \n",
      "<TPL>The application was unable to connect through a proxy server due to an unexpected closure of the connection.</TPL>\n",
      "50: \n",
      "<TPL>The autopurge feature has been set to retain 3 snapshots.</</TPL>\n",
      "60: \n",
      "Understood! I'll do my best to extract the meaning behind the semi-structured log message you provided. Here are the key points:\n",
      "\n",
      "<TPL>The RAS kernel fatal error occurred due to disable store gathering.</TPL>\n",
      "70: \n",
      "Understood! I'll do my best to extract the meaning behind the semi-structured log message you provided. Here are the key points:\n",
      "\n",
      "<TPL>The RAS kernel info indicates that the Z coordinate 32 exceeds the physical dimension 32 at line 33 of the node map file.</TPL>\n",
      "\n",
      "In simpler terms, there is a mismatch between the value stored in the Z coordinate (32) and the physical dimension (32) in a specific location (line 33) of a file called \"node map file\".\n",
      "80: \n",
      "<TPL>The ContainerLauncher returned a shuffle port of 13562 for attempt_1445144423722_0020_m_000000.</TPL>\n",
      "\n",
      "The log message indicates that the ContainerLauncher returned a specific shuffle port number (13562) for a particular container attempt.\n",
      "90: <TPL>The Path not allowed in target domain error occurred due to a mismatch between the path specified in the XPC service request and the location of the service within the target domain.</TPL>\n",
      "100: \n",
      "<TPL>The CGXDisplayDidWakeNotification was posted, indicating that a display has awoken.</TPL>\n",
      "110: \n",
      "<TPL>The \"insertHiHealthData() bulkSaveDetailHiHealthData\" operation failed with error code 4 and error message \"ERR_DATA_INSERT\".</TPL>\n",
      "120: \n",
      "<TPL>The user requested to disconnect from the server.</TPL>\n",
      "130: \n",
      "<TPL>Dec 10 07:13:56 LabSZ sshd[24227]: PAM 5 more authentication failures;</TPL>\n",
      "The SSH daemon detected 5 additional failed login attempts using the PAM (Pluggable Authentication Module) system.\n",
      "140: \n",
      "<TPL>The system took 160 milliseconds to read a broadcast variable.</TPL>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Reformulate log messages with semantic understanding\n",
    "counter=0\n",
    "enhanced_prompts = []\n",
    "enhanced_prompts_file_path = os.path.join(save_dir_now, \"enhanced_prompts.txt\")\n",
    "for raw_log in raw_logs:\n",
    "    new_prompt=f\"\"\"You are provided with a log message. Your task is to understand and extract the meaning behind the semi-structured log message.\n",
    "                      \n",
    "                    Log message: {raw_log}. \n",
    "\n",
    "                    A log message usually contains a header that is automatically produced by the logging framework, including information such as timestamp, class, and logging level (INFO, DEBUG, WARN etc.).\n",
    "                    Ignore all these details and just understand meaning behind the natural languagae text which is in the log content.\n",
    "\n",
    "                    The log content typically consists of many parts: \n",
    "                    1. Template - message body, that contains constant strings (or keywords) describing the system events; \n",
    "                    2. Parameters/Variables - dynamic variables, which reflect specific runtime status;\n",
    "\n",
    "                    Please capture the essential context and meaning from the log message to understand the reasoning behind each raw log.\n",
    "                    Provide only the meaning in one sentence from each log message surrounded by <TPL> and </TPL> with no empty lines.\n",
    "                    Never provide any text other than just the understanding from the log message\n",
    "                \"\"\"\n",
    "    \n",
    "    # enhanced_prompt = get_completion_from_gpt(new_prompt)\n",
    "    # enhanced_prompt = get_completion_from_claude(new_prompt)\n",
    "    enhanced_prompt = get_completion_from_ollama(new_prompt)\n",
    "    enhanced_prompts.append(enhanced_prompt)\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter}: {enhanced_prompt}')\n",
    "    counter+=1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(enhanced_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(enhanced_prompts_file_path, enhanced_prompts)\n",
    "\n",
    "# Save all semantic log templates to a file\n",
    "print(f\"Semantic log templates saved to: {enhanced_prompts_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(enhanced_prompts_file_path, enhanced_prompts_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate log template using zero-shot learning\n",
    "counter_1 = 0\n",
    "semantic_based_templates = []\n",
    "for raw_log, enhanced_prompt in zip(raw_logs, enhanced_prompts):\n",
    "    semantic_prompt = f\"\"\"You will be provided with a log message delimited by <MSG> and </MSG>. \n",
    "    You are also provided with the meaning or understanding from the log message as follow: {enhanced_prompt}. \n",
    "    \n",
    "    The log message typically consists of two parts: \n",
    "    1. Template - message body, that contains constant strings (or keywords) describing the system events; \n",
    "    2. Parameters/Variables - dynamic variables, which reflect specific runtime status.\n",
    "    You must identify and abstract all the dynamic variables in the log message with suitable placeholders inside angle brackets to extract the corresponding template.\n",
    "    You must output the template corresponding to the log message. Print only the input log's template surrounded by <TPL> and </TPL>. \n",
    "    Never print an explanation of how the template is constructed.\n",
    "    Here are a few examples of log messages (labeled with Q:) and corresponding templates (labeled with A:):\n",
    "\n",
    "    Q: <MSG>[081109 204453 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.85:50010 is added to blk_2377150260128098806 size 67108864]</MSG>\n",
    "    A: <TPL>[BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>:<*> is added to <*> size <*>]</TPL>\n",
    "\n",
    "    Q: <MSG>- 1129734520 2005.10.19 R17-M0-N0-I:J18-U01 2005-10-19-08.08.40.058960 R17-M0-N0-I:J18-U01 RAS KERNEL INFO shutdown complete</MSG>\n",
    "    A: <TPL>shutdown complete</TPL>\n",
    "\n",
    "    Q: <MSG>20231114T101914E ERROR 14 while processing line 123: cannot find input '42'</MSG>\n",
    "    A: <TPL>ERROR <*> while processing line <*>: cannot find input <*></TPL>\n",
    "\n",
    "    Q: <MSG>2023-01-14 23:05:14 INFO: Reading data from /user/input/file.txt</MSG>\n",
    "    A: <TPL>Reading data from <*> </TPL>\n",
    "    Here is the input log message: <MSG>{raw_log}</MSG>\n",
    "    Please print the corresponding template.\n",
    "    \"\"\"\n",
    "    # response = get_completion_from_gpt(semantic_prompt)\n",
    "    # response = get_completion_from_claude(semantic_prompt)\n",
    "    response = get_completion_from_ollama(semantic_prompt)\n",
    "    semantic_based_templates.append(response)\n",
    "    \n",
    "    if counter_1 % 10 == 0:\n",
    "        print(f'{counter_1}: {response}')\n",
    "        \n",
    "    counter_1+=1   \n",
    "   \n",
    "print(len(semantic_based_templates))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(semantic_template_output_file_path, semantic_based_templates)\n",
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(semantic_template_output_file_path, semantic_template_output_file_path)\n",
    "print(f\"Semantic Log templates are saved to: {semantic_template_output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "processed_log_template_file_path = semantic_template_output_file_path\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "ground_truth_log_templates = ground_truth_df['EventTemplate'].tolist()\n",
    "ground_truth_systems = ground_truth_df['System'].tolist()\n",
    "\n",
    "# Load processed output data\n",
    "with open(processed_log_template_file_path, 'r') as processed_file:\n",
    "    processed_log_templates = [line.strip() for line in processed_file.readlines()]\n",
    " \n",
    "\n",
    "# Ensure the lists are of the same length for comparison\n",
    "min_length = min(len(ground_truth_log_templates), len(processed_log_templates))\n",
    "ground_truth_log_templates = ground_truth_log_templates[:min_length]\n",
    "processed_log_templates = processed_log_templates[:min_length]\n",
    "ground_truth_systems = ground_truth_systems[:min_length]\n",
    "\n",
    "# Calculate evaluation metrics for processed_log_templates\n",
    "accuracy = accuracy_score(ground_truth_log_templates, processed_log_templates)\n",
    "precision = precision_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "recall = recall_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "f1 = f1_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "\n",
    "# Print evaluation metrics for processed_log_templates\n",
    "print(f\"Log Template Parsing Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Log Template Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Log Template Recall: {recall * 100:.2f}%\")\n",
    "print(f\"Log Template F1 Score: {f1 * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly parsed log templates for each system\n",
    "correct_parsed_counts = {}\n",
    "for system, gt_template, processed_template in zip(ground_truth_systems, ground_truth_log_templates, processed_log_templates):\n",
    "    if gt_template == processed_template:\n",
    "        if system not in correct_parsed_counts:\n",
    "            correct_parsed_counts[system] = 0\n",
    "        correct_parsed_counts[system] += 1\n",
    "\n",
    "# Print correctly parsed log templates for each system\n",
    "print(\"\\nCorrectly Parsed Log Templates per System:\")\n",
    "total=0\n",
    "for system, count in correct_parsed_counts.items():\n",
    "    total +=count\n",
    "    print(f\"{system}: {count}\")\n",
    "\n",
    "print(f\"Total correctly parsed log templates: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that NLTK and other required libraries are installed\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 1. Edit Distance for Each System\n",
    "def calculate_edit_distance_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    system_distances = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        distance = edit_distance(gt, processed)\n",
    "        if system not in system_distances:\n",
    "            system_distances[system] = []\n",
    "        system_distances[system].append(distance)\n",
    "    \n",
    "    avg_distances = {}\n",
    "    print(\"\\nAverage Edit Distance Range: 0 to infinity, where lower is better\")\n",
    "    for system, distances in system_distances.items():\n",
    "        avg_distance = sum(distances) / len(distances)\n",
    "        avg_distances[system] = avg_distance\n",
    "        print(f\"System '{system}':              Average Edit Distance = {avg_distance:.2f}\")\n",
    "    \n",
    "    return avg_distances\n",
    "\n",
    "# 2. ROUGE-L Score for Each System\n",
    "def calculate_rouge_l_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    rouge = Rouge()\n",
    "    system_rouge_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        scores = rouge.get_scores(processed, gt, avg=True)\n",
    "        rouge_l_score = scores['rouge-l']['f']\n",
    "        if system not in system_rouge_scores:\n",
    "            system_rouge_scores[system] = []\n",
    "        system_rouge_scores[system].append(rouge_l_score)\n",
    "    \n",
    "    avg_rouge_scores = {}\n",
    "    print(\"\\nAverage ROUGE-L Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, rouge_scores in system_rouge_scores.items():\n",
    "        avg_rouge_score = sum(rouge_scores) / len(rouge_scores)\n",
    "        avg_rouge_scores[system] = avg_rouge_score\n",
    "        print(f\"System '{system}':              Average ROUGE-L Score = {avg_rouge_score:.2f}\")\n",
    "    \n",
    "    return avg_rouge_scores\n",
    "\n",
    "# Calculating metrics for the given log templates\n",
    "print(\"Comparing the processed log templates to the ground truth log templates:\\n\")\n",
    "edit_distances = calculate_edit_distance_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "rouge_l_scores = calculate_rouge_l_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = os.path.dirname(processed_log_template_file_path)\n",
    "\n",
    "# 3. Visualizations\n",
    "def create_visualizations(edit_distances, rouge_l_scores, output_directory):\n",
    "    systems = list(edit_distances.keys())\n",
    "    edit_values = list(edit_distances.values())\n",
    "    rouge_values = list(rouge_l_scores.values())\n",
    "\n",
    "    # Edit Distance Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, edit_values, color='skyblue')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average Edit Distance')\n",
    "    plt.title('Average Edit Distance per System', fontsize=16, fontweight='normal', pad=15)\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(edit_values):\n",
    "        plt.text(i, value + 0.5, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'edit_distance_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # ROUGE-L Score Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, rouge_values, color='lightgreen')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average ROUGE-L Score')\n",
    "    plt.title('Average ROUGE-L Score per System', fontsize=16, fontweight='normal', pad=15)\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(rouge_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'rouge_l_score_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Combined Edit Distance and ROUGE-L Score Bar Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    ax1.bar(systems, edit_values, color='skyblue', alpha=0.6, label='Edit Distance')\n",
    "    ax1.set_xlabel('System')\n",
    "    ax1.set_ylabel('Average Edit Distance', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    for i, value in enumerate(edit_values):\n",
    "        ax1.text(i, value + 0.5, f'{value:.2f}', ha='center', color='blue')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(systems, rouge_values, color='green', marker='o', linestyle='-', label='ROUGE-L Score')\n",
    "    ax2.set_ylabel('Average ROUGE-L Score', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "    for i, value in enumerate(rouge_values):\n",
    "        ax2.text(i, value + 0.02, f'{value:.2f}', ha='center', color='green')\n",
    "    \n",
    "    plt.title('Edit Distance and ROUGE-L Score per System', fontsize=16, fontweight='normal', pad=15)\n",
    "    fig.tight_layout()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(os.path.join(output_directory, 'combined_scores_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Create and save visualizations\n",
    "create_visualizations(edit_distances, rouge_l_scores, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_1 = 0\n",
    "variable_count_from_logs = []\n",
    "for raw_log, enhanced_prompt in zip(raw_logs, enhanced_prompts):\n",
    "    semantic_prompt = f\"\"\"You will be provided with a log message delimited by <MSG> and </MSG>. \n",
    "    You are also provided with the meaning or understanding from the log message as follows: {enhanced_prompt}. \n",
    "    \n",
    "    I want you to categorize the variable(s) in each log message as a dictionary with each variable category as the key and the count of occurrences of that category as the value. \n",
    "    The variable should be classified within the categories as below:\n",
    "    1. Object ID [OID]\tIdentification information of an object\n",
    "    2. Location Indicator   [LOI]\tLocation information of an object\n",
    "    3. Object Name\t[OBN]\tName of an object\n",
    "    4. Type Indicator\t[TID]\tType information of an object or an action\n",
    "    5. Switch Indicator\t[SID]\tStatus of a switch variable\n",
    "    6. Time or Duration of an Action\t[TDA]\tTime or duration of an action\n",
    "    7. Computing Resources\t[CRS]\tInformation of computing resource\n",
    "    8. Object Amount\t[OBA]\tAmount of an object\n",
    "    9. Status Code\t[STC]\tStatus code of an object or an action\n",
    "    10. Other Parameters\t[OTP]\tOther information that does not belong to the above categories\n",
    "\n",
    "    Here is the input log message: <MSG>{raw_log}</MSG>\n",
    "    \n",
    "    Please generate a dictionary where each key represents one of the categories listed above and the value represents the count of occurrences of that category in the log message. \n",
    "    Only include categories that are present in the log message and do not print anything other than the dictionary. Never print the full name for categories just the code like \"OID\":2, etc.\n",
    "\n",
    "    Example of the format:\n",
    "    {{\n",
    "        \"OID\": 2,\n",
    "        \"LOI\": 1,\n",
    "        \"STC\": 1\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # response = get_completion_from_gpt(semantic_prompt)\n",
    "    # response = get_completion_from_claude(semantic_prompt)\n",
    "    response = get_completion_from_ollama(semantic_prompt)\n",
    "    variable_count_from_logs.append(response)    \n",
    "\n",
    "    if counter_1 % 10 == 0:\n",
    "        print(f'{counter_1}: {response}')\n",
    "        \n",
    "    counter_1 += 1\n",
    "\n",
    "print(len(variable_count_from_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(variables_output_file_path, variable_count_from_logs)\n",
    "\n",
    "fieldnames = ['Index', 'OID', 'LOI', 'OBN', 'TID', 'SID', 'TDA', 'CRS', 'OBA', 'STC', 'OTP']\n",
    "\n",
    "# Writing the data to the CSV\n",
    "with open(variables_output_file_path, mode='w', newline='') as variable_file:\n",
    "    writer = csv.DictWriter(variable_file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write each row of the dictionary results\n",
    "    for index, log_dict in enumerate(variable_count_from_logs):\n",
    "        # Convert response (string) to a dictionary if necessary\n",
    "        if isinstance(log_dict, str):\n",
    "            try:\n",
    "                log_dict = ast.literal_eval(log_dict)  # Safely convert string to dict\n",
    "            except (SyntaxError, ValueError) as e:\n",
    "                print(f\"Error parsing log_dict at index {index}: {e}\")\n",
    "                log_dict = {key: 0 for key in fieldnames[1:]}  # Set all values to 0 for this row to keep the order\n",
    "\n",
    "        # Ensure that all keys are present, with missing ones having a value of 0\n",
    "        row_data = {key: log_dict.get(key, 0) for key in fieldnames[1:]}\n",
    "        row_data['Index'] = index + 1\n",
    "        writer.writerow(row_data)\n",
    "\n",
    "# Save all variables to a file\n",
    "print(f\"Variables are saved to: {variables_output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth and processed data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "processed_df = pd.read_csv(variables_output_file_path)\n",
    "\n",
    "# Ensure the dataframes are of the same length for comparison\n",
    "min_length = min(len(ground_truth_df), len(processed_df))\n",
    "ground_truth_df = ground_truth_df[:min_length]\n",
    "processed_df = processed_df[:min_length]\n",
    "\n",
    "# 1. Per Log Message Evaluation (Overall Accuracy per Log)\n",
    "log_message_accuracies = []\n",
    "for idx in range(min_length):\n",
    "    ground_truth_row = ground_truth_df.iloc[idx, 2:]  # Skip 'Index' column\n",
    "    processed_row = processed_df.iloc[idx, 1:]        # Skip 'Index' column\n",
    "    accuracy = (ground_truth_row == processed_row).mean()\n",
    "    log_message_accuracies.append(accuracy)\n",
    "\n",
    "overall_log_accuracy = sum(log_message_accuracies) / len(log_message_accuracies)\n",
    "print(f\"Overall Log Message Accuracy: {overall_log_accuracy * 100:.2f}%\")\n",
    "\n",
    "# 2. Evaluation Per Variable Type (Precision, Recall, F1 Score, and Accuracy for each variable type)\n",
    "variable_types = ['OID', 'LOI', 'OBN', 'TID', 'SID', 'TDA', 'CRS', 'OBA', 'STC', 'OTP']\n",
    "precision_scores = {}\n",
    "recall_scores = {}\n",
    "f1_scores = {}\n",
    "accuracy_scores = {}\n",
    "\n",
    "for var in variable_types:\n",
    "    y_true = ground_truth_df[var]\n",
    "    y_pred = processed_df[var]\n",
    "    \n",
    "    precision_scores[var] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_scores[var] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1_scores[var] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    accuracy_scores[var] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Print Per Variable Type Metrics\n",
    "print(\"\\nEvaluation Per Variable Type:\")\n",
    "for var in variable_types:\n",
    "    print(f\"{var}:\")\n",
    "    print(f\"  Precision: {precision_scores[var] * 100:.2f}%\")\n",
    "    print(f\"  Recall: {recall_scores[var] * 100:.2f}%\")\n",
    "    print(f\"  F1 Score: {f1_scores[var] * 100:.2f}%\")\n",
    "    print(f\"  Accuracy: {accuracy_scores[var] * 100:.2f}%\")\n",
    "\n",
    "# 3. Overall Evaluation for All Variables\n",
    "ground_truth_flat = ground_truth_df[variable_types].values.flatten()\n",
    "processed_flat = processed_df[variable_types].values.flatten()\n",
    "\n",
    "overall_precision = precision_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "overall_recall = recall_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "overall_f1 = f1_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "overall_accuracy = accuracy_score(ground_truth_flat, processed_flat)\n",
    "\n",
    "# Print Overall Metrics\n",
    "print(\"\\nOverall Evaluation for All Variables:\")\n",
    "print(f\"Overall Precision: {overall_precision * 100:.2f}%\")\n",
    "print(f\"Overall Recall: {overall_recall * 100:.2f}%\")\n",
    "print(f\"Overall F1 Score: {overall_f1 * 100:.2f}%\")\n",
    "\n",
    "print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the calculated accuracies to the ground truth DataFrame\n",
    "ground_truth_df['Accuracy'] = log_message_accuracies\n",
    "\n",
    "# Group by 'System' and calculate average accuracy per system\n",
    "system_wise_accuracy = ground_truth_df.groupby('System')['Accuracy'].mean()\n",
    "\n",
    "# Normalize accuracy values for color mapping\n",
    "norm = mcolors.Normalize(vmin=system_wise_accuracy.min(), vmax=system_wise_accuracy.max())\n",
    "colors = [cm.Blues(norm(value)) for value in system_wise_accuracy.values]\n",
    "\n",
    "# Plotting System-wise Average Accuracies\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(x=system_wise_accuracy.index, y=system_wise_accuracy.values, palette=\"Blues\")\n",
    "\n",
    "# Assign color to each bar manually to follow the computed color based on accuracy values\n",
    "for bar, color in zip(ax.patches, colors):\n",
    "    bar.set_facecolor(color)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('System-wise Average Accuracy', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('System')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add data labels to each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height() * 100:.2f}%', \n",
    "                (p.get_x() + p.get_width() / 2, p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(os.path.join(output_directory, 'system_wise_average_accuracy.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Per Variable Type Accuracy Comparison with Data Labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Set color manually using a color map rather than using the deprecated palette without hue\n",
    "colors = sns.color_palette('Blues', len(accuracy_scores))\n",
    "ax = sns.barplot(x=list(accuracy_scores.keys()), y=list(accuracy_scores.values()), palette=colors)\n",
    "\n",
    "# Add data labels on each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height() * 100:.2f}%', \n",
    "                (p.get_x() + p.get_width() / 2, p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "plt.title('Per Variable Type Accuracy Comparison', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Variable Type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Save the plot before showing it\n",
    "plt.savefig(os.path.join(output_directory, 'per_variable_type_accuracy_comparison.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Overall Precision, Recall, F1 Score, and Accuracy (Radar Chart)\n",
    "overall_metrics = {'Precision': overall_precision, 'Recall': overall_recall, 'F1 Score': overall_f1, 'Accuracy': overall_accuracy}  # Replace with actual overall metrics\n",
    "categories = list(overall_metrics.keys())\n",
    "values = list(overall_metrics.values())\n",
    "# Plotting Overall Metrics Using a Grouped Bar Chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(categories, values, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Overall Metrics Comparison', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Metric', fontsize=14)\n",
    "plt.ylabel('Value', fontsize=14)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Add data labels on each bar\n",
    "for bar in bars:\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.02,\n",
    "             f'{bar.get_height() * 100:.2f}%', ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "# Save the plot\n",
    "output_file_path = os.path.join(output_directory, 'overall_metrics_comparison_bar_chart.png')\n",
    "plt.savefig(output_file_path, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Distribution of Log Message Accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(log_message_accuracies, bins=10, kde=True)\n",
    "plt.title('Distribution of Log Message Accuracies', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Save the plot\n",
    "output_file_path = os.path.join(output_directory, 'distribution_of_log_message_accuracies.png')\n",
    "plt.savefig(output_file_path, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.Box Plot of Metrics Per Variable Type\n",
    "metrics_all = pd.DataFrame({'Precision': list(precision_scores.values()),\n",
    "                            'Recall': list(recall_scores.values()),\n",
    "                            'F1 Score': list(f1_scores.values()),\n",
    "                            'Accuracy': list(accuracy_scores.values())}, index=variable_types)\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.boxplot(data=metrics_all)\n",
    "\n",
    "# Add median value labels on each box\n",
    "medians = metrics_all.median()\n",
    "\n",
    "for i, median in enumerate(medians):\n",
    "    ax.text(i, median, f'{median:.2f}', horizontalalignment='center', fontweight='bold', color='black', size=10)\n",
    "\n",
    "plt.title('Box Plot of Metrics per Variable Type', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "# Save the plot before showing it\n",
    "plt.savefig(os.path.join(output_directory, 'box_plot_of_metrics_per_tariable_type.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Cumulative Distribution of Log Accuracy\n",
    "cumulative_accuracies = np.cumsum(log_message_accuracies) / np.arange(1, len(log_message_accuracies) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cumulative_accuracies) + 1), cumulative_accuracies, marker='o')\n",
    "plt.title('Cumulative Distribution of Log Accuracies')\n",
    "plt.xlabel('Log Index')\n",
    "plt.ylabel('Cumulative Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
