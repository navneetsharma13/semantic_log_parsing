{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from config import config\n",
    "from gpt_model import get_completion_from_gpt\n",
    "from claude import get_completion_from_claude\n",
    "from ollama import get_completion_from_ollama\n",
    "from format_output import Format_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import seaborn as sns\n",
    "from math import pi\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "from rouge import Rouge\n",
    "from nltk.metrics import edit_distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR to your repository root.\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "# Set the DATA_DIR to the directory where your data resides.\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/loghub_2k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"codegemma:7b\" # change this everytime with new model\n",
    "# Model options - deepseek-coder-v2, deepseek-v2:16b, codegemma:7b, qwen2.5-coder:7b, codellama:7b\n",
    "\n",
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "now_time = datetime.datetime.now()\n",
    "date_string = \"Semantic_\" + llm_model_name + now_time.strftime('_%Y-%m-%d-%H-%M')\n",
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "\n",
    "save_dir_path_now = os.path.join(save_dir_path, date_string)\n",
    "\n",
    "raw_save_dir_path = os.path.join(save_dir_path_now, \"raw_results/\")\n",
    "Path(raw_save_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_templates_output_file_name = 'log_template_output.txt'\n",
    "variables_output_file_name = 'variables_output.csv'\n",
    "\n",
    "\n",
    "semantic_prompts_file_path = os.path.join(raw_save_dir_path, \"semantic_prompts.txt\")\n",
    "log_templates_output_file_path = raw_save_dir_path + log_templates_output_file_name\n",
    "variables_output_file_path = raw_save_dir_path + variables_output_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ground_truth_file_path = os.path.join(DATA_DIR, \"ground_truth_template.csv\")\n",
    "raw_logs_file_path = os.path.join(DATA_DIR, \"combined_raw_logs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw log messages\n",
    "with open(raw_logs_file_path, 'r') as raw_file:\n",
    "    log_samples = [line.strip() for line in raw_file.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <TPL>The log message indicates that the system is unable to find a child with ID 5622 in the scoreboard.</TPL>\n",
      "10: <TPL>The node with ID 129 initiated an action with ID 1142537875, using the command 4185.</TPL>\n",
      "20: <TPL>ServerFileSystem domain panic has occurred on storage442.</TPL>\n",
      "30: <TPL>The partition with ID 1126818053 is closing.</TPL>\n",
      "40: <TPL>The temperature of the gige4 interface is above the warning threshold.</TPL>\n",
      "50: <TPL>Node node-73 has detected an available network connection on network 0.0.0.0 via interface alt0.</TPL>\n",
      "60: <TPL>The user root has successfully logged in.</TPL>\n",
      "70: <TPL>cupsd shutdown succeeded.</TPL>\n",
      "80: <TPL>The system has restarted.</TPL>\n",
      "90: <TPL>klogd startup succeeded.</TPL>\n",
      "100: <TPL>The kernel command line includes options for booting with a read-only root filesystem, setting the root partition label, and disabling graphics boot.</TPL>\n",
      "110: <TPL>The console is using colour VGA+ with a resolution of 80x25.</TPL>\n",
      "120: <TPL>rpc.idmapd startup succeeded.</TPL>\n",
      "130: <TPL>Enabling unmasked SIMD FPU exception support is complete.</TPL>\n",
      "140: <TPL>Linux Plug and Play Support v0.97 is running.</TPL>\n",
      "150: <TPL>The BIOS version is 1.2 with flags 0x03, indicating driver version 1.16ac.</TPL>\n",
      "160: <TPL>The loopback interface was successfully brought up.</TPL>\n",
      "170: <TPL>MobaXterm connected to 183.62.156.108:22 through proxy socks.cse.cuhk.edu.hk:5070.</TPL>\n",
      "180: <TPL>The server with identifier 2 is dropping the connection with another server with identifier 1 due to having a smaller server identifier.</TPL>\n",
      "190: <TPL>The user encountered a KeeperException when trying to create a session with a sessionid of 0x34ed93485090001 due to a node already existing at the specified path.</TPL>\n",
      "200: <TPL>The request processor for node 2 has completed its shutdown process.</TPL>\n",
      "210: <TPL>The SendWorker is being interrupted due to a quorum connection issue.</TPL>\n",
      "220: <TPL>The QuorumPeer with ID 3 is getting a snapshot from the leader.</TPL>\n",
      "230: <TPL>Message code 0 is not 51 or 4294967295.</TPL>\n",
      "240: <TPL>RAS KERNEL FATAL guaranteed instruction cache block touch.</TPL>\n",
      "250: <TPL>FATAL floating point instruction enabled.</TPL>\n",
      "260: <TPL>Error creating node map from file due to missing child processes.</TPL>\n",
      "270: <TPL>Error creating node map from file due to bad file descriptor.</TPL>\n",
      "280: <TPL> Node card status is ALERT due to active alerts for clock mode, clock select, and temperature. </TPL>\n",
      "290: <TPL>Torus receiver y+ input pipe error(s) detected and corrected.</TPL>\n",
      "300: <TPL>The system has completed a shutdown process.</TPL>\n",
      "310: <TPL>1 DDR errors(s) detected and corrected on rank 0, symbol 8, bit 7.</TPL>\n",
      "320: <TPL>The system received a signal 15, indicating a termination request.</TPL>\n",
      "330: <TPL>Node card is not fully functional.</TPL>\n",
      "340: <TPL>RAS KERNEL INFO: EDRAM error(s) detected and corrected over 282 seconds.</TPL>\n",
      "350: <TPL>The log message indicates that an application attempt with ID \"appAttemptId { application_id { id: 20 cluster_timestamp: 1445144423722 } attemptId: 1 }\" has been created.</TPL>\n",
      "360: <TPL>Adding job token for job_1445144423722_0020 to jobTokenSecretManager.</TPL>\n",
      "370: <TPL>The server added a global filter named 'safety' with the class 'org.apache.hadoop.http.HttpServer2$QuotingInputFilter'.</TPL>\n",
      "380: <TPL>The log message indicates that the CallQueueManager is using the LinkedBlockingQueue class for its call queue.</TPL>\n",
      "390: <TPL>The log message indicates that the default queue is being used for container allocation.</TPL>\n",
      "400: <TPL>The job-jar file on the remote FS is located at hdfs://msra-sa-41:9000/tmp/hadoop-yarn/staging/msrabi/.staging/job_1445144423722_0020/job.jar.</TPL>\n",
      "410: <TPL>Task with ID attempt_1445144423722_0020_m_000009_0 transitioned from assigned to running.</TPL>\n",
      "420: <TPL>The task attempt with ID attempt_1445144423722_0020_m_000003_0 has received a done acknowledgement.</TPL>\n",
      "430: <TPL>DefaultSpeculator launched 1 speculation and is sleeping for 15 seconds.</TPL>\n",
      "440: <TPL>Error writing History Event: TaskAttemptUnsuccessfulCompletionEvent@7317849d.</TPL>\n",
      "450: <TPL>The task has been aborted due to an unspecified error.</TPL>\n",
      "460: <TPL>ERROR IN CONTACTING RM.</TPL>\n",
      "470: <TPL>The GoogleSoftwareUpdateAgent is checking with a local engine to see if there are any updates available.</TPL>\n",
      "480: <TPL>BTLE scanning stopped.</TPL>\n",
      "490: <TPL>The WebKit Networking service exited with an abnormal code of 1, indicating an unexpected error or failure.</TPL>\n",
      "500: <TPL>Frequent transitions for interface en0 (10.105.162.32).</>\n",
      "510: <TPL>Asked to exit for Diskarb.</TPL>\n",
      "520: <TPL>A new session with ID 101921 has been created.</TPL>\n",
      "530: <TPL>BTLE scanning started.</TPL>\n",
      "540: <TPL>Wireless network scan failed because the central is not powered on.</TPL>\n",
      "550: <TPL>CCProfileMonitor::freeResources done.</TPL>\n",
      "560: <TPL>Manual intervention is required on the network interface en0.</TPL>\n",
      "570: <TPL>The country code for the 802.11d network interface on the authorMacBook-Pro is set to 'X3'.</TPL>\n",
      "580: <TPL>Assertion failure in com.apple.telemetry due to invalid memory address 0x7fc235807b90. </TPL>\n",
      "590: <TPL>Hibernate page list set all time: 603 ms.</TPL>\n",
      "600: <TPL>The internet connection appears to be offline.</TPL>\n",
      "610: <TPL>The user visited the website www.baidu.com.</TPL>\n",
      "620: <TPL>Host controller is published.</TPL>\n",
      "630: <TPL>The kernel is updating the TCP keepalive sequence for the wl0 interface with an original sequence number of 1092633597, an acknowledgment number of 2572586285, and a window size of 4096.</TPL>\n",
      "640: <TPL>The Preview class with ID 11512 has set the page bounds to {{0, 0}, {400, 400}}. </TPL>\n",
      "650: <TPL>KSOutOfProcessFetcher starts fetching software updates from the URL \"https://tools.google.com/service/update2?cup2hreq=c30943ccd5e0a03e93b6be3e2b7e2127989f08f1bde99263ffee091de8b8bc39&cup2key=7:1039900771\".</TPL>\n",
      "660: <TPL>Error in CoreDragRemoveTrackingHandler with code -1856.</TPL>\n",
      "670: <TPL>The network interface en0 has changed IP address to 10.105.160.205 and added a new IPv6 address 2607:f140:6000:8:c6b3:1ff:fecd:467f. The network configuration has been updated with DNS settings and proxy information for SMB.</TPL>\n",
      "680: <TPL> AirPort link is down on interface awdl0 due to unspecified reason. </TPL>\n",
      "690: <TPL>Unexpected preroll-complete notification encountered in figPlaybackBossPrerollCompleted method.</TPL>\n",
      "700: <TPL> The log message indicates that the SOAPParser encountered an error when parsing an ExchangePersonIdGuid element, as the corresponding type is not found in the EWSItemType.</TPL>\n",
      "710: <TPL>GPUToolsAgent detected a lost transport connection with DYTransport 0x7f89e8c00520, indicating a potential issue with the underlying hardware or network connection.</TPL>\n",
      "720: <TPL>The log message indicates that the application encountered an issue when attempting to connect a view outlet from an NSApplication object to an NSColorPickerGridView object. This is likely due to missing setter or instance variable methods in the NSColorPickerGridView class.</TPL>\n",
      "730: <TPL>Unexpected switch value 2 encountered in NetworkAnalyticsEngine.</TPL>\n",
      "740: <TPL>The log message indicates that a capture notice with ID 1499506366.010075 was received, but the reason for the capture is due to authentication failure with a specific reason code (sts:5_rsn:0).</TPL>\n",
      "750: <TPL> The NetworkAnalyticsEngine encountered an error while hashing the primary key of a journal record, leading to its dropping.</TPL>\n",
      "760: <TPL>The user pressed the button with the ID 0x8002bdf.</TPL>\n",
      "770: <TPL>The hostname of the system is being set to \"calvisitor-10-105-162-124.calvisitor.1918.berkeley.edu\".</TPL>\n",
      "780: <TPL> The connection from pid 30318 doesn't have account access. </TPL>\n",
      "790: <TPL>The AWDL operation mode is being set from AUTO to SUSPENDED.</TPL>\n",
      "800: <TPL>The AppleCamIn module received a system wake call with a message type of 0xE0000340.</TPL>\n",
      "810: <TPL>The log message indicates that a stat data record with type 40004, time 1513958400000, and client 2 was saved by user 1.</TPL>\n",
      "820: <TPL>The log message indicates that a new date with a value of 20171223 and type of 40006,7140.0 has been created, replacing an old date with a value of 6900.0.</TPL>\n",
      "830: <TPL>The log message indicates that the checkInsertStatus step is unable to proceed because either stepStatSum or calorieStatSum is sufficient.</TPL>\n",
      "840: <TPL>The log message indicates that a day change event occurred at 1514044800216.</TPL>\n",
      "850: <TPL>The user has completed 1514044800223 steps on 2017-12-24.</TPL>\n",
      "860: <TPL>The LSC step is being extended with a timestamp of 1514046505000 and a duration of 0.</TPL>\n",
      "870: <TPL>The log message indicates that the autoSyncSwitch is open, suggesting that an automatic synchronization process is enabled.</TPL>\n",
      "880: <TPL>SSH authentication failed due to 5 consecutive failures by user root from IP 106.5.5.195.</TPL>\n",
      "890: <TPL>Too many authentication failures for admin [preauth].</TPL>\n",
      "900: <TPL> SSH login attempt failed for root user from IP address 183.62.140.253 with port 36300 using SSH protocol version 2. </TPL>\n",
      "910: <TPL>Remoting process initiated and listening on addresses specified in the configuration.</TPL>\n",
      "920: <TPL>Registered BlockManager.</TPL>\n",
      "930: <TPL>Input split: hdfs://10.10.34.11:9000/pjhe/logs/2kSOSP.log:284388+7303.</TPL>\n",
      "940: <TPL>The DataNode is starting a thread to transfer block blk_4292382298896622412 to another DataNode.</TPL>\n",
      "950: <TPL>The IP address 10.251.90.64:50010 is added to block with ID blk_1441315972355360459 with a size of 67108864.</TPL>\n",
      "960: <TPL>The instance was destroyed on the hypervisor after 1.02 seconds.</TPL>\n",
      "970: <TPL>The instance has a total disk size of 15 GB and currently uses 0.00 GB of disk space.</TPL>\n",
      "980: <TPL>Compute_service record updated for cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us:cp-1.slowvm1.tcloud-pg0.utah.cloudlab.us.</TPL>\n",
      "990: <TPL>The image with UUID 0673dd71-34c5-4fbb-86c4-40623fbe45b4 is being checked at the path /var/lib/nova/instances/_base/a489c868f0c37da93b76227c91bb03908ac0e742.</TPL>\n",
      "1000: <TPL>A DHCP client with MAC address 00:11:43:e3:ba:c3 discovered a DHCP server on interface eth1.</TPL>\n",
      "1010: <TPL>The script \"get_temps e\" is running on the system.</TPL>\n",
      "1020: <TPL>LAPIC (acpi_id[0x04] lapic_id[0x07] enabled) is detected in the system.</TPL>\n",
      "1030: <TPL>The ACPI firmware reports that CPU3 supports C1 state.</TPL>\n",
      "1040: <TPL>The filesystem has been mounted with ordered data mode.</TPL>\n",
      "1050: <TPL>The log message indicates that the MMCONFIG device is being used at the address e0000000.</TPL>\n",
      "1060: <TPL>The log message indicates that the ANSI SCSI revision is 02.</TPL>\n",
      "1070: <TPL>The USB HID core driver is running version 2.0.</TPL>\n",
      "1080: <TPL>autorun DONE..</TPL>\n",
      "1090: <TPL>portmap service successfully started.</TPL>\n",
      "1100: <TPL>AUX port at 0x60,0x64 irq 12.</TPL>\n",
      "1110: <TPL>The log message indicates that a new USB device, identified as \"usbfs\", has been registered.</TPL>\n",
      "1120: <TPL>User #29# authenticated from #30#. </TPL>\n",
      "1130: <TPL>The xinetd service has started working with 1 available service.</TPL>\n",
      "1140: <TPL>Device /dev/sda temperature changed from -2 Celsius to 26 Celsius since last report.</TPL>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Reformulate log messages with semantic understanding\n",
    "counter=0\n",
    "semantic_prompts = []\n",
    "for log in log_samples:\n",
    "    prompt=f\"\"\"You are provided with a log message. Your task is to understand and extract the meaning behind the semi-structured log message.\n",
    "                      \n",
    "                    Log message: {log}. \n",
    "\n",
    "                    A log message usually contains a header that is automatically produced by the logging framework, including information such as timestamp, class, and logging level (INFO, DEBUG, WARN etc.).\n",
    "                    Ignore all these details and just understand meaning behind the natural languagae text which is in the log content.\n",
    "\n",
    "                    The log content typically consists of many parts: \n",
    "                    1. Template - message body, that contains constant strings (or keywords) describing the system events; \n",
    "                    2. Parameters/Variables - dynamic variables, which reflect specific runtime status;\n",
    "\n",
    "                    Please capture the essential context and meaning from the log message to understand the reasoning behind each raw log.\n",
    "                    Provide only the meaning in just one sentence from each log message surrounded by <TPL> and </TPL> in a single line.\n",
    "                    Never provide any text other than just the understanding from the log message\n",
    "                \"\"\"\n",
    "    \n",
    "    # semantic_prompt = get_completion_from_gpt(prompt)\n",
    "    semantic_prompt = get_completion_from_ollama(prompt, model=llm_model_name)\n",
    "    semantic_prompts.append(semantic_prompt)\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter}: {semantic_prompt}')\n",
    "    counter+=1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1146\n"
     ]
    }
   ],
   "source": [
    "print(len(semantic_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed output saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_codegemma:7b_2024-12-11-21-41/raw_results/semantic_prompts.txt\n",
      "Semantics from logs saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_codegemma:7b_2024-12-11-21-41/raw_results/semantic_prompts.txt\n"
     ]
    }
   ],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(semantic_prompts_file_path, semantic_prompts)\n",
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(semantic_prompts_file_path, semantic_prompts_file_path)\n",
    "\n",
    "# Save all semantic log templates to a file\n",
    "print(f\"Semantics from logs saved to: {semantic_prompts_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <TPL>[error] jk2_init() Can't find child <*> in scoreboard</TPL>\n",
      "10: <TPL>401417 node-<*> action start <*> <*>  (command <*>)</TPL>\n",
      "20: <TPL>ServerFileSystem domain panic has occurred on <*></TPL>\n",
      "30: <TPL>full partition status <*> <*> closing</TPL>\n",
      "40: <TPL>479981 gige4 gige temperature <*> 1 warning</TPL>\n",
      "50: <TPL>Node <*> has detected an available network connection on network <*> via interface <*></TPL>\n",
      "60: <TPL>session opened for user <*> by <*></TPL>\n",
      "70: <TPL>cupsd shutdown succeeded</TPL>\n",
      "80: <TPL>restart.</TPL>\n",
      "90: <TPL>klogd startup succeeded</TPL>\n",
      "100: <TPL>Kernel command line: ro root=LABEL/<*> rhgb quiet</TPL>\n",
      "110: <TPL>Console: colour VGA+ <*>x<*></TPL>\n",
      "120: <TPL>rpc.idmapd startup succeeded</TPL>\n",
      "130: <TPL>Enabling unmasked SIMD FPU exception support... done.</TPL>\n",
      "140: <TPL>Linux Plug and Play Support v0.97 (c) Adam Belay</TPL>\n",
      "150: <TPL>apm: BIOS version <*> Flags <*> (Driver version <*>)</TPL>\n",
      "160: <TPL>Bringing up loopback interface:  succeeded</TPL>\n",
      "170: <TPL>MobaXterm connected to <*>:<*> through proxy socks.cse.cuhk.edu.hk:<*>.</TPL>\n",
      "180: <TPL>Have smaller server identifier, so dropping the connection: (<*, *>)</</TPL>\n",
      "190: <TPL>Got user-level KeeperException when processing sessionid:<*> type:create cxid:<*> zxid:<*> txntype:-1 reqpath:n/a Error Path:<*> Error:KeeperErrorCode = NodeExists for <*></TPL>\n",
      "200: <TPL>shutdown of request processor complete</TPL>\n",
      "210: <TPL>Interrupting SendWorker</TPL>\n",
      "220: <TPL>Getting a snapshot from leader</TPL>\n",
      "230: <TPL>ciod: Message code 0 is not 51 or 4294967295</TPL>\n",
      "240: <TPL>RAS KERNEL FATAL guaranteed instruction cache block touch.</TPL>\n",
      "250: <TPL>FATAL floating point instruction enabled.</TPL>\n",
      "260: <TPL>Error creating node map from file due to missing child processes.</TPL>\n",
      "270: <TPL>Error creating node map from file due to bad file descriptor</TPL>\n",
      "280: <TPL>Node card status is ALERT due to active alerts for clock mode, clock select, and temperature.</TPL>\n",
      "290: <TPL>1 torus receiver y+ input pipe error(s) (dcr 0x02ee) detected and corrected</TPL>\n",
      "300: <TPL>shutdown complete</TPL>\n",
      "310: <TPL>1 DDR errors(s) detected and corrected on rank <*> symbol <*> bit <*> </TPL>\n",
      "320: <TPL>ciod: Received signal 15, code=<*>, errno=<*>, address=<*>]</TPL>\n",
      "330: <TPL>Node card is not fully functional</TPL>\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Generate log template using 4-shot learning\n",
    "counter_1 = 0\n",
    "semantic_based_log_templates = []\n",
    "for log, semantic_prompt in zip(log_samples, semantic_prompts):\n",
    "    prompt =  f\"\"\"You will be given a log message enclosed by <MSG> and </MSG> tags, along with its semantic understanding: {semantic_prompt}.\n",
    "\n",
    "                The log message consists of:\n",
    "                1. A log_template (constant strings describing system events)\n",
    "                2. Dynamic variables (runtime-specific values)\n",
    "\n",
    "                Your task is to:\n",
    "                - Identify all dynamic variables in the log message.\n",
    "                - Replace each dynamic variable with a placeholder surrounded by angle brackets, like <*>, to produce a log_template.\n",
    "                - Output only the transformed log_template, enclosed in <TPL> and </TPL> tags.\n",
    "                - The final output must be a single line with no leading/trailing spaces, no extra lines, and no explanations.\n",
    "\n",
    "                Here are examples:\n",
    "\n",
    "                Q: <MSG>[081109 204453 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.85:50010 is added to blk_2377150260128098806 size 67108864]</MSG>  \n",
    "                A: <TPL>[BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>:<*> is added to <*> size <*>]</TPL>\n",
    "\n",
    "                Q: <MSG>- 1129734520 2005.10.19 R17-M0-N0-I:J18-U01 2005-10-19-08.08.40.058960 R17-M0-N0-I:J18-U01 RAS KERNEL INFO shutdown complete</MSG>  \n",
    "                A: <TPL>shutdown complete</TPL>\n",
    "\n",
    "                Q: <MSG>20231114T101914E ERROR 14 while processing line 123: cannot find input '42'</MSG>  \n",
    "                A: <TPL>ERROR <*> while processing line <*>: cannot find input <*></TPL>\n",
    "\n",
    "                Q: <MSG>2023-01-14 23:05:14 INFO: Reading data from /user/input/file.txt</MSG>  \n",
    "                A: <TPL>Reading data from <*></TPL>\n",
    "\n",
    "                Now, consider the following log message: <MSG>{log}</MSG>  \n",
    "                Using the same process as above, output only the single-line log_template enclosed in <TPL> and </TPL>, and nothing else.\n",
    "                \"\"\"\n",
    "\n",
    "    # response = get_completion_from_gpt(prompt)\n",
    "    response = get_completion_from_ollama(prompt, model=llm_model_name)\n",
    "    semantic_based_log_templates.append(response)\n",
    "    if counter_1 % 10 == 0:\n",
    "        print(f'{counter_1}: {response}')\n",
    "    counter_1+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(semantic_based_log_templates))\n",
    "\n",
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(log_templates_output_file_path, semantic_based_log_templates)\n",
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(log_templates_output_file_path, log_templates_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "processed_log_templates_file_path = log_templates_output_file_path\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "ground_truth_log_templates = ground_truth_df['EventTemplate'].tolist()\n",
    "ground_truth_systems = ground_truth_df['System'].tolist()\n",
    "\n",
    "output_directory = os.path.dirname(processed_log_templates_file_path)\n",
    "\n",
    "\n",
    "# Load processed output data\n",
    "with open(processed_log_templates_file_path, 'r') as processed_file:\n",
    "    processed_log_templates = [line.strip() for line in processed_file.readlines()]\n",
    " \n",
    "\n",
    "# Ensure the lists are of the same length for comparison\n",
    "min_length = min(len(ground_truth_log_templates), len(processed_log_templates))\n",
    "ground_truth_log_templates = ground_truth_log_templates[:min_length]\n",
    "processed_log_templates = processed_log_templates[:min_length]\n",
    "ground_truth_systems = ground_truth_systems[:min_length]\n",
    "\n",
    "# Calculate evaluation metrics for processed_log_templates\n",
    "precision = precision_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "recall = recall_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "f1 = f1_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "\n",
    "# Print evaluation metrics for processed_log_templates\n",
    "print(f\"Log Templates Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Log Templates Recall: {recall * 100:.2f}%\")\n",
    "print(f\"Log Templates F1 Score: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly parsed log templates for each system\n",
    "correct_parsed_counts = {}\n",
    "for system, gt_template, processed_template in zip(ground_truth_systems, ground_truth_log_templates, processed_log_templates):\n",
    "    if gt_template == processed_template:\n",
    "        if system not in correct_parsed_counts:\n",
    "            correct_parsed_counts[system] = 0\n",
    "        correct_parsed_counts[system] += 1\n",
    "\n",
    "# Print correctly parsed log templates for each system\n",
    "print(\"\\nCorrectly Parsed Log Templates per System:\")\n",
    "total=0\n",
    "for system, count in correct_parsed_counts.items():\n",
    "    total +=count\n",
    "    print(f\"{system}: {count}\")\n",
    "\n",
    "print(f\"Total correctly parsed log templates: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that NLTK and other required libraries are installed\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Description of Metrics\n",
    "print(\"Description of Metrics:\\n\")\n",
    "print(\"Edit Distance: Measures the number of character-level edit operations (insertion, deletion, substitution) needed to transform one string into another. A normalized score between 0 and 1 is calculated by dividing the edit distance by the total character length of the longer string, where 0 indicates identical strings and 1 indicates completely different strings.\\n\")\n",
    "print(\"ROUGE-L: Measures the similarity between two texts based on the longest common subsequence. The score ranges from 0 to 1, with 1 indicating high similarity. ROUGE-L focuses on recall and is useful for comparing the structure of sentences.\\n\")\n",
    "print(\"Cosine Similarity: Measures the cosine of the angle between two vectors, which represent the texts in a multi-dimensional space. The score ranges from 0 to 1, with 1 indicating that the vectors are identical, meaning high similarity.\\n\")\n",
    "print(\"BLEU (Bilingual Evaluation Understudy): Measures the similarity between a predicted sentence and one or more reference sentences by calculating the overlap of n-grams. The score ranges from 0 to 1, with 1 indicating high similarity. BLEU is commonly used for machine translation evaluation.\\n\")\n",
    "\n",
    "# 1. Edit Distance for Each System\n",
    "def calculate_edit_distance_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    system_distances = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        distance = edit_distance(gt, processed)\n",
    "        max_length = max(len(gt), len(processed))\n",
    "        normalized_distance = distance / max_length if max_length > 0 else 0\n",
    "        if system not in system_distances:\n",
    "            system_distances[system] = []\n",
    "        system_distances[system].append(normalized_distance)\n",
    "    \n",
    "    avg_distances = {}\n",
    "    for system, distances in system_distances.items():\n",
    "        avg_distance = sum(distances) / len(distances)\n",
    "        avg_distances[system] = avg_distance\n",
    "        print(f\"System '{system}': Normalized Average Edit Distance = {avg_distance:.2f} (Range: 0 to 1, where lower is better)\")\n",
    "    \n",
    "    return avg_distances\n",
    "\n",
    "# 2. ROUGE-L Score for Each System\n",
    "def calculate_rouge_l_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    rouge = Rouge()\n",
    "    system_rouge_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        if not gt.strip() or not processed.strip():\n",
    "            continue  # Skip empty ground truth or processed templates\n",
    "        scores = rouge.get_scores(processed, gt, avg=True)\n",
    "        rouge_l_score = scores['rouge-l']['f']\n",
    "        if system not in system_rouge_scores:\n",
    "            system_rouge_scores[system] = []\n",
    "        system_rouge_scores[system].append(rouge_l_score)\n",
    "    \n",
    "    avg_rouge_scores = {}\n",
    "    print(\"\\nAverage ROUGE-L Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, rouge_scores in system_rouge_scores.items():\n",
    "        avg_rouge_score = sum(rouge_scores) / len(rouge_scores)\n",
    "        avg_rouge_scores[system] = avg_rouge_score\n",
    "        print(f\"System '{system}':              Average ROUGE-L Score = {avg_rouge_score:.2f}\")\n",
    "    \n",
    "    return avg_rouge_scores\n",
    "\n",
    "# 3. Cosine Similarity for Each System\n",
    "def calculate_cosine_similarity_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    system_cosine_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        if not gt.strip() or not processed.strip():\n",
    "            continue  # Skip empty ground truth or processed templates\n",
    "        vectors = vectorizer.fit_transform([gt, processed])\n",
    "        cosine_sim = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "        if system not in system_cosine_scores:\n",
    "            system_cosine_scores[system] = []\n",
    "        system_cosine_scores[system].append(cosine_sim)\n",
    "    \n",
    "    avg_cosine_scores = {}\n",
    "    print(\"\\nAverage Cosine Similarity Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, cosine_scores in system_cosine_scores.items():\n",
    "        avg_cosine_score = sum(cosine_scores) / len(cosine_scores)\n",
    "        avg_cosine_scores[system] = avg_cosine_score\n",
    "        print(f\"System '{system}':              Average Cosine Similarity Score = {avg_cosine_score:.2f}\")\n",
    "    \n",
    "    return avg_cosine_scores\n",
    "\n",
    "# 4. BLEU Score for Each System\n",
    "def calculate_bleu_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    system_bleu_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        reference_tokens = nltk.word_tokenize(gt)\n",
    "        hypothesis_tokens = nltk.word_tokenize(processed)\n",
    "        bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "        if system not in system_bleu_scores:\n",
    "            system_bleu_scores[system] = []\n",
    "        system_bleu_scores[system].append(bleu_score)\n",
    "    \n",
    "    avg_bleu_scores = {}\n",
    "    print(\"\\nAverage BLEU Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, bleu_scores in system_bleu_scores.items():\n",
    "        avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "        avg_bleu_scores[system] = avg_bleu_score\n",
    "        print(f\"System '{system}':              Average BLEU Score = {avg_bleu_score:.2f}\")\n",
    "    \n",
    "    return avg_bleu_scores\n",
    "\n",
    "# Calculating metrics for the given log templates\n",
    "print(\"Comparing the processed log templates to the ground truth log templates:\\n\")\n",
    "edit_distances = calculate_edit_distance_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "rouge_l_scores = calculate_rouge_l_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "cosine_similarity_scores = calculate_cosine_similarity_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "bleu_scores = calculate_bleu_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluation_metrices_results = 'evaluation-log_templates_results.csv'\n",
    "evaluation_metrices_results_file_path = os.path.join(save_dir_path, evaluation_metrices_results)\n",
    "\n",
    "# Storing results into a CSV file\n",
    "all_results = []\n",
    "\n",
    "# Add Syntactic results\n",
    "for system in ground_truth_systems:\n",
    "    all_results.append({\n",
    "        \"Parsing Technique\": \"Semantic\",\n",
    "        \"LLM Model\": llm_model_name,\n",
    "        \"System\": system,\n",
    "        \"Edit Distance\": edit_distances.get(system, None),\n",
    "        \"ROUGE-L\": rouge_l_scores.get(system, None),\n",
    "        \"Cosine Similarity\": cosine_similarity_scores.get(system, None),\n",
    "        \"BLEU\": bleu_scores.get(system, None),\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame and save to CSV\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.to_csv(evaluation_metrices_results_file_path, index=False)\n",
    "\n",
    "print(f\"Evaluation metrics saved to {evaluation_metrices_results_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Visualizations\n",
    "\n",
    "def create_visualizations(edit_distances, rouge_l_scores, cosine_similarity_scores, bleu_scores, output_directory):\n",
    "    systems = list(edit_distances.keys())\n",
    "    edit_values = list(edit_distances.values())\n",
    "    rouge_values = list(rouge_l_scores.values())\n",
    "    cosine_values = list(cosine_similarity_scores.values())\n",
    "    bleu_values = list(bleu_scores.values())\n",
    "\n",
    "    # Edit Distance Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, edit_values, color='skyblue')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Normalized Average Edit Distance')\n",
    "    plt.title('Normalized Average Edit Distance per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(edit_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'edit_distance_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # ROUGE-L Score Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, rouge_values, color='lightgreen')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average ROUGE-L Score')\n",
    "    plt.title('Average ROUGE-L Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(rouge_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'rouge_l_score_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Cosine Similarity Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, cosine_values, color='lightcoral')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average Cosine Similarity Score')\n",
    "    plt.title('Average Cosine Similarity Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(cosine_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'cosine_similarity_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # BLEU Score Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, bleu_values, color='lightblue')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average BLEU Score')\n",
    "    plt.title('Average BLEU Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(bleu_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'bleu_score_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Combined Metrics Bar Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    ax1.bar(systems, edit_values, color='skyblue', alpha=0.6, label='Edit Distance')\n",
    "    ax1.set_xlabel('System')\n",
    "    ax1.set_ylabel('Normalized Average Edit Distance', color='blue')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    for i, value in enumerate(edit_values):\n",
    "        ax1.text(i, value + 0.02, f'{value:.2f}', ha='center', color='blue')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(systems, rouge_values, color='green', marker='o', linestyle='-', label='ROUGE-L Score')\n",
    "    ax2.plot(systems, cosine_values, color='red', marker='x', linestyle='-', label='Cosine Similarity')\n",
    "    ax2.plot(systems, bleu_values, color='purple', marker='^', linestyle='-', label='BLEU Score')\n",
    "    ax2.set_ylabel('Scores', color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "    plt.title('Normalized Edit Distance, ROUGE-L, Cosine Similarity, and BLEU Score per System')\n",
    "    fig.tight_layout()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.savefig(os.path.join(output_directory, 'combined_scores_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Create and save visualizations\n",
    "create_visualizations(edit_distances, rouge_l_scores, cosine_similarity_scores, bleu_scores, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_1 = 0\n",
    "variables_and_anomalies_from_logs = []\n",
    "for log, semantic_prompt in zip(log_samples, semantic_prompts):\n",
    "    prompt = f\"\"\"You will be provided with a log message delimited by <MSG> and </MSG>. \n",
    "    You are also provided with the meaning or understanding from the log message as follows: {semantic_prompt}. \n",
    "    \n",
    "    I want you to categorize the variable(s) in each log message as a dictionary with each variable category as the key and the count of occurrences of that category as the value. \n",
    "    The variable should be classified within the categories as below:\n",
    "    1. Object ID [OID]\tIdentification information of an object\n",
    "    2. Location Indicator   [LOI]\tLocation information of an object\n",
    "    3. Object Name\t[OBN]\tName of an object\n",
    "    4. Type Indicator\t[TID]\tType information of an object or an action\n",
    "    5. Switch Indicator\t[SID]\tStatus of a switch variable\n",
    "    6. Time or Duration of an Action\t[TDA]\tTime or duration of an action\n",
    "    7. Computing Resources\t[CRS]\tInformation of computing resource\n",
    "    8. Object Amount\t[OBA]\tAmount of an object\n",
    "    9. Status Code\t[STC]\tStatus code of an object or an action\n",
    "    10. Other Parameters\t[OTP]\tOther information that does not belong to the above categories\n",
    "\n",
    "    Also, based on the variables found, and the understanding provided for each log as the input above, classify each log as either 1 if abnormal behaviour or 0 if normal behaviour.\n",
    "\n",
    "    Here is the input log message: <MSG>{log}</MSG>\n",
    "    \n",
    "    Please generate a dictionary where each key represents one of the categories listed above and the value represents the count of occurrences of that category in the log message and it's \"Class\" as either 1 or 0. \n",
    "    Always have the \"Class\" key:value pair but only include category key:value that are present in the log message.\n",
    "    Do not print anything other than the dictionary.\n",
    "    Never print the full name for categories just the code for example \"OID\".\n",
    "\n",
    "    Example of the format:\n",
    "    {{\n",
    "        \"OID\": 2,\n",
    "        \"LOI\": 1,\n",
    "        \"STC\": 1,\n",
    "        \"Class\": 1\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    # response = get_completion_from_gpt(prompt)\n",
    "    response = get_completion_from_ollama(prompt, model=llm_model_name)\n",
    "    variables_and_anomalies_from_logs.append(response)    \n",
    "\n",
    "    if counter_1 % 10 == 0:\n",
    "        print(f'{counter_1}: {response}')\n",
    "        \n",
    "    counter_1 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(variables_and_anomalies_from_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(variables_output_file_path, variables_and_anomalies_from_logs)\n",
    "\n",
    "fieldnames = ['Index', 'OID', 'LOI', 'OBN', 'TID', 'SID', 'TDA', 'CRS', 'OBA', 'STC', 'OTP', 'Class']\n",
    "\n",
    "# Writing the data to the CSV\n",
    "with open(variables_output_file_path, mode='w', newline='') as variable_file:\n",
    "    writer = csv.DictWriter(variable_file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Regular expression pattern to extract key-value pairs\n",
    "    pattern = re.compile(r'\"(\\w+)\":\\s*(\"?)([\\w\\s\\.,]+)\\2')\n",
    "\n",
    "    # Write each row of the dictionary results\n",
    "    for index, log_string in enumerate(variables_and_anomalies_from_logs, start=1):\n",
    "        # Clean up the string and remove extra formatting like ```json\n",
    "        cleaned_string = log_string.replace('```json', '').strip()\n",
    "\n",
    "        # Extract key-value pairs using the regular expression\n",
    "        matches = pattern.findall(cleaned_string)\n",
    "\n",
    "        # Create a dictionary from the matches\n",
    "        log_dict = {}\n",
    "        for key, _, value in matches:\n",
    "            # print(key)\n",
    "            value = value.strip().replace(',', '')  # Remove trailing newlines and whitespace\n",
    "            # print(value)\n",
    "            if value.isdigit():\n",
    "                log_dict[key] = int(value)\n",
    "            elif key == 'Class':\n",
    "                log_dict[key] = 0 if value in ['Normal'] else 1\n",
    "            else:\n",
    "                print(\"here\")\n",
    "                log_dict[key] = 0  # Replace any unexpected strings with 0\n",
    "\n",
    "        # Ensure that all keys are present, with missing ones having a value of 0 or an appropriate default\n",
    "        row_data = {key: log_dict.get(key, 0 if key != 'Class' else 'Normal') for key in fieldnames[1:]}\n",
    "        row_data['Index'] = index\n",
    "        writer.writerow(row_data)\n",
    "\n",
    "# Save all variables to a file\n",
    "print(f\"Variables are saved to: {variables_output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth and processed data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "processed_df = pd.read_csv(variables_output_file_path)\n",
    "\n",
    "# Ensure the dataframes are of the same length for comparison\n",
    "min_length = min(len(ground_truth_df), len(processed_df))\n",
    "ground_truth_df = ground_truth_df[:min_length]\n",
    "processed_df = processed_df[:min_length]\n",
    "\n",
    "# 1. Per Log Message Evaluation (Overall Accuracy per Log)\n",
    "log_message_accuracies = []\n",
    "for idx in range(min_length):\n",
    "    ground_truth_row = ground_truth_df.iloc[idx, 2:]  # Skip 'Index' column\n",
    "    processed_row = processed_df.iloc[idx, 1:-1]        # Skip 'Index' column\n",
    "    accuracy = (ground_truth_row == processed_row).mean()\n",
    "    log_message_accuracies.append(accuracy)\n",
    "\n",
    "overall_log_accuracy = sum(log_message_accuracies) / len(log_message_accuracies)\n",
    "print(f\"Overall Log Message Accuracy: {overall_log_accuracy * 100:.2f}%\")\n",
    "\n",
    "# 2. Evaluation Per Variable Type (Precision, Recall, F1 Score, and Accuracy for each variable type)\n",
    "variable_types = ['OID', 'LOI', 'OBN', 'TID', 'SID', 'TDA', 'CRS', 'OBA', 'STC', 'OTP']\n",
    "precision_scores = {}\n",
    "recall_scores = {}\n",
    "f1_scores = {}\n",
    "# accuracy_scores = {}\n",
    "\n",
    "for var in variable_types:\n",
    "    y_true = ground_truth_df[var]\n",
    "    y_pred = processed_df[var]\n",
    "    \n",
    "    precision_scores[var] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_scores[var] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1_scores[var] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    # accuracy_scores[var] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Print Per Variable Type Metrics\n",
    "print(\"\\nEvaluation Per Variable Type:\")\n",
    "for var in variable_types:\n",
    "    print(f\"{var}:\")\n",
    "    print(f\"  Precision: {precision_scores[var] * 100:.2f}%  Recall: {recall_scores[var] * 100:.2f}%  F1 Score: {f1_scores[var] * 100:.2f}%\")\n",
    "    # print(f\"  Accuracy: {accuracy_scores[var] * 100:.2f}%\")\n",
    "\n",
    "# 3. Overall Evaluation for All Variables\n",
    "ground_truth_flat = ground_truth_df[variable_types].values.flatten()\n",
    "processed_flat = processed_df[variable_types].values.flatten()\n",
    "\n",
    "overall_precision = precision_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "overall_recall = recall_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "overall_f1 = f1_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "# overall_accuracy = accuracy_score(ground_truth_flat, processed_flat)\n",
    "\n",
    "# Print Overall Metrics\n",
    "print(\"\\nOverall Evaluation for All Variables:\")\n",
    "print(f\"Overall Precision: {overall_precision * 100:.2f}%\")\n",
    "print(f\"Overall Recall: {overall_recall * 100:.2f}%\")\n",
    "print(f\"Overall F1 Score: {overall_f1 * 100:.2f}%\")\n",
    "# print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Distribution of Log Message Accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(log_message_accuracies, bins=5, kde=True)\n",
    "plt.title('Distribution of Log Message Accuracies', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Save the plot\n",
    "output_file_path = os.path.join(output_directory, 'distribution_of_log_message_accuracies.png')\n",
    "plt.savefig(output_file_path, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Plotting System-wise Average Accuracies\n",
    "\n",
    "# Add the calculated accuracies to the ground truth DataFrame\n",
    "ground_truth_df['Accuracy'] = log_message_accuracies\n",
    "\n",
    "# Group by 'System' and calculate average accuracy per system\n",
    "system_wise_accuracy = ground_truth_df.groupby('System')['Accuracy'].mean()\n",
    "\n",
    "# Normalize accuracy values for color mapping\n",
    "norm = mcolors.Normalize(vmin=system_wise_accuracy.min(), vmax=system_wise_accuracy.max())\n",
    "colors = [cm.viridis(norm(value)) for value in system_wise_accuracy.values]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(x=system_wise_accuracy.index, y=system_wise_accuracy.values)\n",
    "\n",
    "# Assign color to each bar manually to follow the computed color based on accuracy values\n",
    "for bar, color in zip(ax.patches, colors):\n",
    "    bar.set_facecolor(color)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('System-wise Average Accuracy', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('System')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add data labels to each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height() * 100:.2f}%', \n",
    "                (p.get_x() + p.get_width() / 2, p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(os.path.join(output_directory, 'system_wise_average_accuracy.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Per Variable Type Accuracy Comparison with Data Labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Set color manually using a color map rather than using the deprecated palette without hue\n",
    "colors = sns.color_palette('Blues', len(recall_scores))\n",
    "ax = sns.barplot(x=list(recall_scores.keys()), y=list(recall_scores.values()), palette=colors)\n",
    "\n",
    "# Add data labels on each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height() * 100:.2f}%', \n",
    "                (p.get_x() + p.get_width() / 2, p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "plt.title('Per Variable Type Recall Comparison', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Variable Type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Save the plot before showing it\n",
    "plt.savefig(os.path.join(output_directory, 'per_variable_type_recall_comparison.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Box Plot of Metrics Per Variable Type\n",
    "metrics_all = pd.DataFrame({'Precision': list(precision_scores.values()),\n",
    "                            'Recall': list(recall_scores.values()),\n",
    "                            'F1 Score': list(f1_scores.values())}, index=variable_types)\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.boxplot(data=metrics_all)\n",
    "\n",
    "# Add median value labels on each box\n",
    "medians = metrics_all.median()\n",
    "\n",
    "for i, median in enumerate(medians):\n",
    "    ax.text(i, median, f'{median:.2f}', horizontalalignment='center', color='black', size=10)\n",
    "\n",
    "plt.title('Box Plot of Metrics per Variable Type', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "# Save the plot before showing it\n",
    "plt.savefig(os.path.join(output_directory, 'box_plot_of_metrics_per_tariable_type.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Cumulative Distribution of Log Accuracy\n",
    "cumulative_accuracies = np.cumsum(log_message_accuracies) / np.arange(1, len(log_message_accuracies) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cumulative_accuracies) + 1), cumulative_accuracies, marker='o')\n",
    "plt.title('Cumulative Distribution of Log Accuracies')\n",
    "plt.xlabel('Log Index')\n",
    "plt.ylabel('Cumulative Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
