{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from config import config\n",
    "from gpt_model import get_completion_from_gpt\n",
    "from claude import get_completion_from_claude\n",
    "from format_output import Format_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR to your repository root.\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "# Set the DATA_DIR to the directory where your data resides.\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/loghub_2k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "\n",
    "now_time = datetime.datetime.now()\n",
    "date_string = \"Semantic_\" + now_time.strftime('%Y-%m-%d-%H-%M-%S')\n",
    "save_dir_separator = \"Semantic_\" + now_time.strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "save_dir_now = os.path.join(save_dir_path, save_dir_separator)\n",
    "raw_save_dir = os.path.join(save_dir_now, \"semantic_raw_results/\")\n",
    "Path(raw_save_dir).mkdir(parents=True, exist_ok=True)\n",
    "semantic_raw_output_file_name = 'semantic_output.txt'\n",
    "semantic_raw_output_file_path = raw_save_dir + semantic_raw_output_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ground_truth_file_path = os.path.join(DATA_DIR, \"sample_ground_truth_template.csv\")\n",
    "raw_log_file_path = os.path.join(DATA_DIR, \"sample_combined_raw_logs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "ground_truth_templates = ground_truth_df['EventTemplate'].tolist()\n",
    "ground_truth_systems = ground_truth_df['System'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw log messages\n",
    "with open(raw_log_file_path, 'r') as raw_file:\n",
    "    raw_logs = [line.strip() for line in raw_file.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the lists are of the same length for comparison\n",
    "min_length = min(len(ground_truth_templates), len(raw_logs))\n",
    "ground_truth_templates = ground_truth_templates[:min_length]\n",
    "raw_logs = raw_logs[:min_length]\n",
    "ground_truth_systems = ground_truth_systems[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <TPL> Initialization of worker environment successful for workers2 properties file located at /etc/httpd/conf/workers2.properties. </TPL>\n",
      "10: <TPL> gige5 temperature warning: high temperature detected (1072878243). </TPL>\n",
      "20: <TPL> Command abort: 1111858191. </TPL>\n",
      "30: <TPL> CPU has an L2 cache size of 256K. </TPL>\n",
      "40: <TPL> Connection error while trying to connect to tcpconn4.tencent.com:80 through proxy proxy.cse.cuhk.edu.hk:5070. Proxy closed the connection unexpectedly. </TPL>\n",
      "50: <TPL> autopurge.snapRetainCount set to 3 </TPL>\n",
      "60: <TPL>RAS KERNEL FATAL disable store gathering.</TPL>\n",
      "70: <TPL>RAS KERNEL INFO: Z coordinate 32 exceeds physical dimension 32 at line 33 of node map file.</TPL>\n",
      "80: <TPL> ContainerLauncher retrieved Shuffle port for specific map task attempt. </TPL>\n",
      "90: <TPL> Path not allowed in target domain for Safari SearchHelper service due to missing bundle service in WebKit origin. </TPL>\n",
      "100: <TPL> Display woke up notification posted by WindowServer for the specific display ID. </TPL>\n",
      "110: <TPL> insertHiHealthData() bulkSaveDetailHiHealthData fail errorCode = 4, errorMessage = ERR_DATA_INSERT. </TPL>\n",
      "120: <TPL> User requested to close SSH connection from IP address 103.207.39.165. </TPL>\n",
      "130: <TPL> Authentication failure for user 'root' via SSH from IP address 5.36.59.76 after 5 unsuccessful attempts. </TPL>\n",
      "140: <TPL> Reading broadcast variable 9 took 160 ms. </TPL>\n",
      "150: <TPL> NameSystem.addStoredBlock: blockMap updated: 10.251.73.220:50010 is added to blk_7128370237687728475 size 67108864. </TPL>\n",
      "160: <TPL> Request to delete a block with a specific ID from a data node at IP address 10.250.18.114:50010. </TPL>\n",
      "170: <TPL> Instance with ID b9000564-fe1a-409b-b8cc-1e88b294cd1d took 19.84 seconds to build. </TPL>\n",
      "180: <TPL> i8042 AUX port at 0x60,0x64 irq 12. </TPL>\n",
      "190: <TPL> ACPI: LAPIC enabled for ACPI ID 0x01 and LAPIC ID 0x00 on tbird-admin1 at 12:10:43 local time. </TPL>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Reformulate log messages with semantic understanding\n",
    "counter=0\n",
    "enhanced_prompts = []\n",
    "enhanced_prompts_file_path = os.path.join(save_dir_now, \"enhanced_prompts.txt\")\n",
    "for raw_log in raw_logs:\n",
    "    new_prompt=f\"\"\"You are provided with a log message. Your task is to understand and extract the meaning behind the semi-structured log message based on the given raw log.\n",
    "                      \n",
    "                    Raw log: {raw_log}. \n",
    "\n",
    "                    A raw log usually contains a header that is automatically produced by the logging framework, including information such as timestamp, class, and logging level (INFO, DEBUG, WARN etc.).\n",
    "                    Ignore all these details and just understand the natural languagae text which is in the log content.\n",
    "\n",
    "                    The log content typically consists of many parts: \n",
    "                    1. Template - message body, that contains constant strings (or keywords) describing the system events; \n",
    "                    2. Parameters/Variables - dynamic variables, which reflect specific runtime status;\n",
    "\n",
    "                    Please capture the essential context and meaning from the log message to understand the reasoning behind each raw log.\n",
    "                    Provide only the meaning in 25 words from each raw log surrounded by <TPL> and </TPL>. \n",
    "                    Never provide an explanation of how the semantic knowledge is constructed.\n",
    "                \"\"\"\n",
    "    \n",
    "    enhanced_prompt = get_completion_from_gpt(new_prompt)\n",
    "    enhanced_prompts.append(enhanced_prompt)\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter}: {enhanced_prompt}')\n",
    "    counter+=1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n"
     ]
    }
   ],
   "source": [
    "print(len(enhanced_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic log templates saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_20241128133042/enhanced_prompts.txt\n"
     ]
    }
   ],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(enhanced_prompts_file_path, enhanced_prompts)\n",
    "\n",
    "# Save all semantic log templates to a file\n",
    "print(f\"Semantic log templates saved to: {enhanced_prompts_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed output saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_20241128133042/enhanced_prompts.txt\n"
     ]
    }
   ],
   "source": [
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(enhanced_prompts_file_path, enhanced_prompts_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate log template using zero-shot learning\n",
    "counter = 0\n",
    "enhanced_templates = []\n",
    "semantic_log_template_output_file_path = semantic_raw_output_file_path\n",
    "for raw_log, enhanced_prompt in zip(raw_logs, enhanced_prompts):\n",
    "    semantic_prompt = f\"\"\"You will be provided with a log message delimited by <MSG> and </MSG>. \n",
    "    You are also provided with a new prompt that has the semantic undertanding of the raw log given. New prompt: {enhanced_prompt}. \n",
    "    \n",
    "    The log message typically consists of two parts: \n",
    "    1. Template - message body, that contains constant strings (or keywords) describing the system events; \n",
    "    2. Parameters/Variables - dynamic variables, which reflect specific runtime status.\n",
    "    You must identify and abstract all the dynamic variables in the log message with suitable placeholders inside angle brackets to extract the corresponding template.\n",
    "    You must output the template corresponding to the log message. Print only the input log's template surrounded by <TPL> and </TPL>. \n",
    "    Never print an explanation of how the template is constructed.\n",
    "    Here are a few examples of log messages (labeled with Q:) and corresponding templates (labeled with A:):\n",
    "\n",
    "    Q: <MSG>[081109 204453 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.85:50010 is added to blk_2377150260128098806 size 67108864]</MSG>\n",
    "    A: <TPL>[BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>:<*> is added to <*> size <*>]</TPL>\n",
    "\n",
    "    Q: <MSG>- 1129734520 2005.10.19 R17-M0-N0-I:J18-U01 2005-10-19-08.08.40.058960 R17-M0-N0-I:J18-U01 RAS KERNEL INFO shutdown complete</MSG>\n",
    "    A: <TPL>shutdown complete</TPL>\n",
    "\n",
    "    Q: <MSG>20231114T101914E ERROR 14 while processing line 123: cannot find input '42'</MSG>\n",
    "    A: <TPL>ERROR <*> while processing line <*>: cannot find input <*></TPL>\n",
    "\n",
    "    Q: <MSG>2023-01-14 23:05:14 INFO: Reading data from /user/input/file.txt</MSG>\n",
    "    A: <TPL>Reading data from <*> </TPL>\n",
    "    Here is the input log message: <MSG>{raw_log}</MSG>\n",
    "    Please print the corresponding template.\n",
    "    \"\"\"\n",
    "    response = get_completion_from_gpt(semantic_prompt)\n",
    "    enhanced_templates.append(response)\n",
    "\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter}: {response}')\n",
    "    counter+=1      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic log templates saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_20241128133042/enhanced_prompts.txt\n"
     ]
    }
   ],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(semantic_log_template_output_file_path, enhanced_templates)\n",
    "\n",
    "# Save all semantic log templates to a file\n",
    "print(f\"Semantic log templates saved to: {enhanced_prompts_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed output saved to: /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/results/Semantic_20241128133042/semantic_raw_results/semantic_output.txt\n"
     ]
    }
   ],
   "source": [
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(semantic_log_template_output_file_path, semantic_log_template_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "processed_output_file_path = semantic_log_template_output_file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed output data\n",
    "with open(processed_output_file_path, 'r') as processed_file:\n",
    "    processed_templates = [line.strip() for line in processed_file.readlines()]\n",
    "\n",
    "# Ensure the lists are of the same length for comparison\n",
    "min_length = min(len(ground_truth_templates), len(processed_templates))\n",
    "ground_truth_templates = ground_truth_templates[:min_length]\n",
    "processed_templates = processed_templates[:min_length]\n",
    "ground_truth_systems = ground_truth_systems[:min_length]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Accuracy: 32.12%\n",
      "Precision: 32.64%\n",
      "Recall: 32.12%\n",
      "F1 Score: 32.30%\n"
     ]
    }
   ],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(ground_truth_templates, processed_templates)\n",
    "precision = precision_score(ground_truth_templates, processed_templates, average='weighted', zero_division=0)\n",
    "recall = recall_score(ground_truth_templates, processed_templates, average='weighted', zero_division=0)\n",
    "f1 = f1_score(ground_truth_templates, processed_templates, average='weighted', zero_division=0)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Parsing Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Correctly Parsed Templates per System:\n",
      "Apache: 5\n",
      "Linux: 9\n",
      "Zookeeper: 5\n",
      "BGL: 3\n",
      "Hadoop: 5\n",
      "Mac: 4\n",
      "HealthApp: 6\n",
      "OpenSSH: 8\n",
      "Spark: 10\n",
      "HDFS: 1\n",
      "OpenStack: 3\n",
      "Thunderbird: 3\n"
     ]
    }
   ],
   "source": [
    "# Calculate correctly parsed templates for each system\n",
    "correct_parsed_counts = {}\n",
    "for system, gt_template, processed_template in zip(ground_truth_systems, ground_truth_templates, processed_templates):\n",
    "    if gt_template == processed_template:\n",
    "        if system not in correct_parsed_counts:\n",
    "            correct_parsed_counts[system] = 0\n",
    "        correct_parsed_counts[system] += 1\n",
    "\n",
    "# Print correctly parsed templates for each system\n",
    "print(\"\\nCorrectly Parsed Templates per System:\")\n",
    "for system, count in correct_parsed_counts.items():\n",
    "    print(f\"{system}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
