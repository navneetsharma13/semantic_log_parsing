{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "data_folder = \"/Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/data/deepLoglizer_data\"\n",
    "# Path to your Excel file\n",
    "excel_file_path = os.path.join(data_folder, \"anomaly_label.xls\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sheets into DataFrames\n",
    "blocks_df = pd.read_excel(excel_file_path, sheet_name='hdfs_anomaly_label')\n",
    "logs_df = pd.read_excel(excel_file_path, sheet_name='hdfs_logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing blocks: 100%|██████████| 65535/65535 [35:35<00:00, 30.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Prepare a mapping list\n",
    "matches = []\n",
    "\n",
    "# Use tqdm in the outer loop to track progress\n",
    "for index, block_row in tqdm(blocks_df.iterrows(), total=blocks_df.shape[0], desc=\"Processing blocks\"):\n",
    "    block_id = block_row['BlockId']\n",
    "\n",
    "    for log_index, log_row in logs_df.iterrows():\n",
    "        if block_id in log_row['logs_hdfs']:\n",
    "            # Store the match, log entry, and any additional details you need\n",
    "            matches.append({\n",
    "                'BlockId': block_id,\n",
    "                'Label': block_row['Label'],\n",
    "                'LogEntry': log_row['logs_hdfs']\n",
    "            })\n",
    "            flag=1\n",
    "            \n",
    "    if(len(matches))>400:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    BlockId   Label  \\\n",
      "0   blk_8376667364205250596  Normal   \n",
      "1     blk_38865049064139660  Normal   \n",
      "2  blk_-4980916519894289629  Normal   \n",
      "3   blk_8145984793403459836  Normal   \n",
      "4  blk_-6520030462660619051  Normal   \n",
      "\n",
      "                                            LogEntry  \n",
      "0  081109 214402 2677 WARN dfs.DataNode$DataXceiv...  \n",
      "1  081109 203615 148 INFO dfs.DataNode$PacketResp...  \n",
      "2  081109 205931 13 INFO dfs.DataBlockScanner: Ve...  \n",
      "3  081110 103749 19 INFO dfs.FSDataset: Deleting ...  \n",
      "4  081109 214919 2899 INFO dfs.DataNode$DataXceiv...  \n",
      "Matches saved to /Users/navneetsharma/Documents/NMBU/MS Data Science @ NMBU/Master's Thesis/semantic_log_parsing/data/deepLoglizer_data/hdfs_annotations.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert matches to DataFrame\n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "# Optionally, save or print the matches\n",
    "print(matches_df.head())\n",
    "\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_path = os.path.join(data_folder, \"hdfs_annotations.csv\") # Change this to your preferred path\n",
    "matches_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f'Matches saved to {output_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
