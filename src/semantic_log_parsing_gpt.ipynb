{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from config import config\n",
    "from gpt_model import get_completion_from_gpt\n",
    "from claude import get_completion_from_claude\n",
    "from ollama import get_completion_from_ollama\n",
    "from format_output import Format_output\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import seaborn as sns\n",
    "from math import pi\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "from rouge import Rouge\n",
    "from nltk.metrics import edit_distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import pandas as pd\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ROOT_DIR to your repository root.\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(''))\n",
    "# Set the DATA_DIR to the directory where your data resides.\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'data/loghub_2k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model_name = \"gpt3.5_turbo\" # change this everytime with new model\n",
    "# Model options - deepseek-coder-v2, deepseek-v2:16b, codegemma:7b, qwen2.5-coder:7b, codellama:7b\n",
    "\n",
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "now_time = datetime.datetime.now()\n",
    "date_string = \"Semantic_\" + llm_model_name + now_time.strftime('_%Y-%m-%d-%H-%M')\n",
    "save_dir_path = os.path.join(ROOT_DIR, 'results')\n",
    "\n",
    "save_dir_path_now = os.path.join(save_dir_path, date_string)\n",
    "\n",
    "raw_save_dir_path = os.path.join(save_dir_path_now, \"raw_results/\")\n",
    "Path(raw_save_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "log_templates_output_file_name = 'log_template_output.txt'\n",
    "variables_output_file_name = 'variables_output.csv'\n",
    "\n",
    "\n",
    "semantic_prompts_file_path = os.path.join(raw_save_dir_path, \"semantic_prompts.txt\")\n",
    "log_templates_output_file_path = raw_save_dir_path + log_templates_output_file_name\n",
    "variables_output_file_path = raw_save_dir_path + variables_output_file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "ground_truth_file_path = os.path.join(DATA_DIR, \"ground_truth_template.csv\")\n",
    "raw_logs_file_path = os.path.join(DATA_DIR, \"combined_raw_logs.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw log messages\n",
    "with open(raw_logs_file_path, 'r') as raw_file:\n",
    "    log_samples = [line.strip() for line in raw_file.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: <TPL> Error occurred during initialization as child process with ID 5622 was not found in the scoreboard. </TPL>\n",
      "10: <TPL> The log indicates that a node with ID 129 has initiated the action of starting a process named bootGenvmunix with a command ID 4185. </TPL>\n",
      "20: <TPL> A ServerFileSystem domain panic has occurred on storage442. </TPL>\n",
      "30: <TPL> The partition with ID 1126818053 is at full status and is currently being closed. </TPL>\n",
      "40: <TPL> The temperature of gige4 port is 1146058156 with a warning. </TPL>\n",
      "50: <TPL> Node node-73 has detected an available network connection on network 0.0.0.0 via interface alt0. </TPL>\n",
      "60: <TPL> A session was opened for the user 'root' by the 'LOGIN' process. </TPL>\n",
      "70: <TPL>cupsd shutdown succeeded.</TPL>\n",
      "80: <TPL> The syslogd service was restarted. </TPL>\n",
      "90: <TPL> The system successfully started the klogd process. </TPL>\n",
      "100: <TPL> The kernel command line specifies read-only root with the label \"/\". </TPL>\n",
      "110: <TPL> The system console is set to display in color VGA with a resolution of 80x25. </TPL>\n",
      "120: <TPL> The rpc.idmapd service successfully started up. </TPL>\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Reformulate log messages with semantic understanding\n",
    "counter=0\n",
    "semantic_prompts = []\n",
    "for log in log_samples:\n",
    "    prompt=f\"\"\"You are provided with a log message. Your task is to understand and extract the meaning behind the semi-structured log message.\n",
    "                      \n",
    "                    Log message: {log}. \n",
    "\n",
    "                    A log message usually contains a header that is automatically produced by the logging framework, including information such as timestamp, class, and logging level (INFO, DEBUG, WARN etc.).\n",
    "                    Ignore all these details and just understand meaning behind the natural languagae text which is in the log content.\n",
    "\n",
    "                    The log content typically consists of many parts: \n",
    "                    1. Template - message body, that contains constant strings (or keywords) describing the system events; \n",
    "                    2. Parameters/Variables - dynamic variables, which reflect specific runtime status;\n",
    "\n",
    "                    Please capture the essential context and meaning from the log message to understand the reasoning behind each raw log.\n",
    "                    Provide only the meaning in just one sentence from each log message surrounded by <TPL> and </TPL> in a single line.\n",
    "                    Never provide any text other than just the understanding from the log message\n",
    "                \"\"\"\n",
    "    \n",
    "    semantic_prompt = get_completion_from_gpt(prompt)\n",
    "    # semantic_prompt = get_completion_from_ollama(prompt, model=llm_model_name)\n",
    "    semantic_prompts.append(semantic_prompt)\n",
    "    if counter % 10 == 0:\n",
    "        print(f'{counter}: {semantic_prompt}')\n",
    "    counter+=1    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(semantic_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(semantic_prompts_file_path, semantic_prompts)\n",
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(semantic_prompts_file_path, semantic_prompts_file_path)\n",
    "\n",
    "# Save all semantic log templates to a file\n",
    "print(f\"Semantics from logs saved to: {semantic_prompts_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Generate log template using 4-shot learning\n",
    "counter_1 = 0\n",
    "semantic_based_log_templates = []\n",
    "for log, semantic_prompt in zip(log_samples, semantic_prompts):\n",
    "    prompt = f\"\"\"You will be given a log message enclosed by <MSG> and </MSG> tags, along with its semantic understanding: {semantic_prompt}.\n",
    "\n",
    "                The log message consists of:\n",
    "                1. A log_template (message body, that contains constant strings (or keywords) describing the system events.)\n",
    "                2. Dynamic variables (dynamic variables, which reflect specific runtime status.)\n",
    "\n",
    "                Your task is to:\n",
    "                - Identify all dynamic variables in the log message.\n",
    "                - Replace each dynamic variable with a placeholder surrounded by angle brackets, like <*>, to produce a log_template.\n",
    "                - Output only the transformed log_template, enclosed in <TPL> and </TPL> tags.\n",
    "                - The final output must be a single line with no leading/trailing spaces, no extra lines, and no explanations.\n",
    "\n",
    "                Here are examples:\n",
    "\n",
    "                Q: <MSG>[081109 204453 34 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.11.85:50010 is added to blk_2377150260128098806 size 67108864]</MSG>  \n",
    "                A: <TPL>[BLOCK* NameSystem.addStoredBlock: blockMap updated: <*>:<*> is added to <*> size <*>]</TPL>\n",
    "\n",
    "                Q: <MSG>- 1129734520 2005.10.19 R17-M0-N0-I:J18-U01 2005-10-19-08.08.40.058960 R17-M0-N0-I:J18-U01 RAS KERNEL INFO shutdown complete</MSG>  \n",
    "                A: <TPL>shutdown complete</TPL>\n",
    "\n",
    "                Q: <MSG>20231114T101914E ERROR 14 while processing line 123: cannot find input '42'</MSG>  \n",
    "                A: <TPL>ERROR <*> while processing line <*>: cannot find input <*></TPL>\n",
    "\n",
    "                Q: <MSG>2023-01-14 23:05:14 INFO: Reading data from /user/input/file.txt</MSG>  \n",
    "                A: <TPL>Reading data from <*></TPL>\n",
    "\n",
    "                Now, consider the following log message: <MSG>{log}</MSG>  \n",
    "                Using the same process as above, output only the single-line log_template enclosed in <TPL> and </TPL>, and nothing else.\n",
    "            \"\"\"\n",
    "    response = get_completion_from_gpt(prompt)\n",
    "    # response = get_completion_from_ollama(prompt, model=llm_model_name)\n",
    "    semantic_based_log_templates.append(response)\n",
    "    if counter_1 % 10 == 0:\n",
    "        print(f'{counter_1}: {response}')\n",
    "    counter_1+=1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(semantic_based_log_templates))\n",
    "\n",
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(log_templates_output_file_path, semantic_based_log_templates)\n",
    "# convert raw output into formatted file \n",
    "Format_output.remove_TPL_from_output(log_templates_output_file_path, log_templates_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "processed_log_templates_file_path = log_templates_output_file_path\n",
    "\n",
    "# Load ground truth data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "ground_truth_log_templates = ground_truth_df['EventTemplate'].tolist()\n",
    "ground_truth_systems = ground_truth_df['System'].tolist()\n",
    "\n",
    "output_directory = os.path.dirname(processed_log_templates_file_path)\n",
    "\n",
    "\n",
    "# Load processed output data\n",
    "with open(processed_log_templates_file_path, 'r') as processed_file:\n",
    "    processed_log_templates = [line.strip() for line in processed_file.readlines()]\n",
    " \n",
    "\n",
    "# Ensure the lists are of the same length for comparison\n",
    "min_length = min(len(ground_truth_log_templates), len(processed_log_templates))\n",
    "ground_truth_log_templates = ground_truth_log_templates[:min_length]\n",
    "processed_log_templates = processed_log_templates[:min_length]\n",
    "ground_truth_systems = ground_truth_systems[:min_length]\n",
    "\n",
    "# Calculate evaluation metrics for processed_log_templates\n",
    "precision = precision_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "recall = recall_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "f1 = f1_score(ground_truth_log_templates, processed_log_templates, average='weighted', zero_division=0)\n",
    "\n",
    "# Print evaluation metrics for processed_log_templates\n",
    "print(f\"Log Templates Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Log Templates Recall: {recall * 100:.2f}%\")\n",
    "print(f\"Log Templates F1 Score: {f1 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correctly parsed log templates for each system\n",
    "correct_parsed_counts = {}\n",
    "for system, gt_template, processed_template in zip(ground_truth_systems, ground_truth_log_templates, processed_log_templates):\n",
    "    if gt_template == processed_template:\n",
    "        if system not in correct_parsed_counts:\n",
    "            correct_parsed_counts[system] = 0\n",
    "        correct_parsed_counts[system] += 1\n",
    "\n",
    "# Print correctly parsed log templates for each system\n",
    "print(\"\\nCorrectly Parsed Log Templates per System:\")\n",
    "total=0\n",
    "for system, count in correct_parsed_counts.items():\n",
    "    total +=count\n",
    "    print(f\"{system}: {count}\")\n",
    "\n",
    "print(f\"Total correctly parsed log templates: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that NLTK and other required libraries are installed\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Description of Metrics\n",
    "print(\"Description of Metrics:\\n\")\n",
    "print(\"Edit Distance: Measures the number of character-level edit operations (insertion, deletion, substitution) needed to transform one string into another. A normalized score between 0 and 1 is calculated by dividing the edit distance by the total character length of the longer string, where 0 indicates identical strings and 1 indicates completely different strings.\\n\")\n",
    "print(\"ROUGE-L: Measures the similarity between two texts based on the longest common subsequence. The score ranges from 0 to 1, with 1 indicating high similarity. ROUGE-L focuses on recall and is useful for comparing the structure of sentences.\\n\")\n",
    "print(\"Cosine Similarity: Measures the cosine of the angle between two vectors, which represent the texts in a multi-dimensional space. The score ranges from 0 to 1, with 1 indicating that the vectors are identical, meaning high similarity.\\n\")\n",
    "print(\"BLEU (Bilingual Evaluation Understudy): Measures the similarity between a predicted sentence and one or more reference sentences by calculating the overlap of n-grams. The score ranges from 0 to 1, with 1 indicating high similarity. BLEU is commonly used for machine translation evaluation.\\n\")\n",
    "\n",
    "# 1. Edit Distance for Each System\n",
    "def calculate_edit_distance_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    system_distances = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        distance = edit_distance(gt, processed)\n",
    "        max_length = max(len(gt), len(processed))\n",
    "        normalized_distance = distance / max_length if max_length > 0 else 0\n",
    "        if system not in system_distances:\n",
    "            system_distances[system] = []\n",
    "        system_distances[system].append(normalized_distance)\n",
    "    \n",
    "    avg_distances = {}\n",
    "    for system, distances in system_distances.items():\n",
    "        avg_distance = sum(distances) / len(distances)\n",
    "        avg_distances[system] = avg_distance\n",
    "        print(f\"System '{system}': Normalized Average Edit Distance = {avg_distance:.2f} (Range: 0 to 1, where lower is better)\")\n",
    "    \n",
    "    return avg_distances\n",
    "\n",
    "# 2. ROUGE-L Score for Each System\n",
    "def calculate_rouge_l_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    rouge = Rouge()\n",
    "    system_rouge_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        if not gt.strip() or not processed.strip():\n",
    "            continue  # Skip empty ground truth or processed templates\n",
    "        scores = rouge.get_scores(processed, gt, avg=True)\n",
    "        rouge_l_score = scores['rouge-l']['f']\n",
    "        if system not in system_rouge_scores:\n",
    "            system_rouge_scores[system] = []\n",
    "        system_rouge_scores[system].append(rouge_l_score)\n",
    "    \n",
    "    avg_rouge_scores = {}\n",
    "    print(\"\\nAverage ROUGE-L Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, rouge_scores in system_rouge_scores.items():\n",
    "        avg_rouge_score = sum(rouge_scores) / len(rouge_scores)\n",
    "        avg_rouge_scores[system] = avg_rouge_score\n",
    "        print(f\"System '{system}':              Average ROUGE-L Score = {avg_rouge_score:.2f}\")\n",
    "    \n",
    "    return avg_rouge_scores\n",
    "\n",
    "# 3. Cosine Similarity for Each System\n",
    "\n",
    "\n",
    "def calculate_cosine_similarity_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    system_cosine_scores = {}\n",
    "\n",
    "    for gt, processed, system in zip(ground_truth_log_templates, processed_log_templates, ground_truth_systems):\n",
    "        # Skip empty or whitespace-only templates\n",
    "        if not gt.strip() or not processed.strip():\n",
    "            continue  # Skip this pair if either is empty\n",
    "\n",
    "        # Check if the cleaned templates result in non-empty input for the vectorizer\n",
    "        cleaned_gt = \" \".join(gt.split())\n",
    "        cleaned_processed = \" \".join(processed.split())\n",
    "\n",
    "        if not cleaned_gt or not cleaned_processed:\n",
    "            continue  # Skip if cleaning leaves the strings empty\n",
    "\n",
    "        try:\n",
    "            vectors = vectorizer.fit_transform([cleaned_gt, cleaned_processed])\n",
    "            cosine_sim = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "\n",
    "            # Add cosine similarity score to the corresponding system\n",
    "            if system not in system_cosine_scores:\n",
    "                system_cosine_scores[system] = []\n",
    "            system_cosine_scores[system].append(cosine_sim)\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"Error with system {system}: {e}\")\n",
    "            continue  # Skip problematic inputs\n",
    "\n",
    "    print(\"\\nAverage Cosine Similarity Range: 0 to 1, where 1 means highly similar\")\n",
    "    avg_cosine_scores = {}\n",
    "\n",
    "    for system, cosine_scores in system_cosine_scores.items():\n",
    "        if cosine_scores:  # Ensure there are scores to calculate the average\n",
    "            avg_cosine_score = sum(cosine_scores) / len(cosine_scores)\n",
    "            avg_cosine_scores[system] = avg_cosine_score\n",
    "            print(f\"System '{system}':              Average Cosine Similarity = {avg_cosine_score:.2f}\")\n",
    "        else:\n",
    "            print(f\"System '{system}':              No Cosine Similarity Scores Available\")\n",
    "        \n",
    "\n",
    "    return system_cosine_scores\n",
    "\n",
    "\n",
    "# 4. BLEU Score for Each System\n",
    "def calculate_bleu_for_each_system(ground_truth_list, processed_list, system_list):\n",
    "    system_bleu_scores = {}\n",
    "    for system, gt, processed in zip(system_list, ground_truth_list, processed_list):\n",
    "        reference_tokens = nltk.word_tokenize(gt)\n",
    "        hypothesis_tokens = nltk.word_tokenize(processed)\n",
    "        bleu_score = sentence_bleu([reference_tokens], hypothesis_tokens)\n",
    "        if system not in system_bleu_scores:\n",
    "            system_bleu_scores[system] = []\n",
    "        system_bleu_scores[system].append(bleu_score)\n",
    "    \n",
    "    avg_bleu_scores = {}\n",
    "    print(\"\\nAverage BLEU Score Range: 0 to 1, where 1 means highly similar\")\n",
    "    for system, bleu_scores in system_bleu_scores.items():\n",
    "        avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "        avg_bleu_scores[system] = avg_bleu_score\n",
    "        print(f\"System '{system}':              Average BLEU Score = {avg_bleu_score:.2f}\")\n",
    "    \n",
    "    return avg_bleu_scores\n",
    "\n",
    "# Calculating metrics for the given log templates\n",
    "print(\"Comparing the processed log templates to the ground truth log templates:\\n\")\n",
    "edit_distances = calculate_edit_distance_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "rouge_l_scores = calculate_rouge_l_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "cosine_similarity_scores = calculate_cosine_similarity_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "bleu_scores = calculate_bleu_for_each_system(ground_truth_log_templates, processed_log_templates, ground_truth_systems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_metrices_results = 'gpt_semantic_combined_evaluation-log_templates_results.csv'\n",
    "evaluation_metrices_results_file_path = os.path.join(save_dir_path, evaluation_metrices_results)\n",
    "\n",
    "# Storing results into a CSV file\n",
    "all_results = []\n",
    "\n",
    "# Add Syntactic results\n",
    "for system in ground_truth_systems:\n",
    "    all_results.append({\n",
    "        \"Parsing Technique\": \"Semantic\",\n",
    "        \"LLM Model\": llm_model_name,\n",
    "        \"System\": system,\n",
    "        \"Edit Distance\": edit_distances.get(system, None),\n",
    "        \"ROUGE-L\": rouge_l_scores.get(system, None),\n",
    "        \"Cosine Similarity\": cosine_similarity_scores.get(system, None),\n",
    "        \"BLEU\": bleu_scores.get(system, None),\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame and save to CSV\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.to_csv(evaluation_metrices_results_file_path, index=False)\n",
    "\n",
    "print(f\"Evaluation metrics saved to {evaluation_metrices_results_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_visualizations(edit_distances, rouge_l_scores, cosine_similarity_scores, bleu_scores, output_directory):\n",
    "    # Ensure all metrics have the same systems\n",
    "    systems = set(edit_distances.keys()) & set(rouge_l_scores.keys()) & set(cosine_similarity_scores.keys()) & set(bleu_scores.keys())\n",
    "    systems = sorted(systems)  # Sort systems for consistent plotting\n",
    "\n",
    "    # Extract values for each system\n",
    "    edit_values = [edit_distances[system] for system in systems]\n",
    "    rouge_values = [rouge_l_scores[system] for system in systems]\n",
    "    cosine_values = [np.mean(cosine_similarity_scores[system]) for system in systems]\n",
    "    bleu_values = [bleu_scores[system] for system in systems]\n",
    "\n",
    "    # Ensure directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Edit Distance Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, edit_values, color='skyblue')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Normalized Average Edit Distance')\n",
    "    plt.title('Normalized Average Edit Distance per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(edit_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'edit_distance_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # ROUGE-L Score Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, rouge_values, color='lightgreen')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average ROUGE-L Score')\n",
    "    plt.title('Average ROUGE-L Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(rouge_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'rouge_l_score_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Cosine Similarity Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, cosine_values, color='lightcoral')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average Cosine Similarity Score')\n",
    "    plt.title('Average Cosine Similarity Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(cosine_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'cosine_similarity_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # BLEU Score Bar Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(systems, bleu_values, color='lightblue')\n",
    "    plt.xlabel('System')\n",
    "    plt.ylabel('Average BLEU Score')\n",
    "    plt.title('Average BLEU Score per System')\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, value in enumerate(bleu_values):\n",
    "        plt.text(i, value + 0.02, f'{value:.2f}', ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_directory, 'bleu_score_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Combined Metrics Bar Plot\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "    ax1.bar(systems, edit_values, color='skyblue', alpha=0.6, label='Edit Distance')\n",
    "    ax1.set_xlabel('System')\n",
    "    ax1.set_ylabel('Normalized Average Edit Distance', color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    for i, value in enumerate(edit_values):\n",
    "        ax1.text(i, value + 0.02, f'{value:.2f}', ha='center', color='blue')\n",
    "    \n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(systems, rouge_values, color='green', marker='o', linestyle='-', label='ROUGE-L Score')\n",
    "    ax2.plot(systems, cosine_values, color='red', marker='x', linestyle='-', label='Cosine Similarity')\n",
    "    ax2.plot(systems, bleu_values, color='purple', marker='^', linestyle='-', label='BLEU Score')\n",
    "    ax2.set_ylabel('Scores', color='green')\n",
    "    plt.xticks(rotation=45)\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "    plt.title('Normalized Edit Distance, ROUGE-L, Cosine Similarity, and BLEU Score per System')\n",
    "    fig.tight_layout()\n",
    "    plt.xticks(rotation=45)\n",
    "    ax2.legend(loc='upper left')\n",
    "    plt.savefig(os.path.join(output_directory, 'combined_scores_per_system.png'))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# Example Usage\n",
    "# Make sure `edit_distances`, `rouge_l_scores`, `cosine_similarity_scores`, and `bleu_scores` are dictionaries with system names as keys.\n",
    "# Call the function to generate plots:\n",
    "create_visualizations(edit_distances, rouge_l_scores, cosine_similarity_scores, bleu_scores, output_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter_1 = 0\n",
    "variables_and_anomalies_from_logs = []\n",
    "for log, semantic_prompt in zip(log_samples, semantic_prompts):\n",
    "    prompt = f\"\"\"You will be provided with a log message delimited by <MSG> and </MSG>. \n",
    "    You are also provided with the meaning or understanding from the log message as follows: {semantic_prompt}. \n",
    "    \n",
    "    I want you to categorize the variable(s) in each log message as a dictionary with each variable category as the key and the count of occurrences of that category as the value. \n",
    "    The variable should be classified within the categories as below:\n",
    "    1. Object ID [OID]\tIdentification information of an object\n",
    "    2. Location Indicator   [LOI]\tLocation information of an object\n",
    "    3. Object Name\t[OBN]\tName of an object\n",
    "    4. Type Indicator\t[TID]\tType information of an object or an action\n",
    "    5. Switch Indicator\t[SID]\tStatus of a switch variable\n",
    "    6. Time or Duration of an Action\t[TDA]\tTime or duration of an action\n",
    "    7. Computing Resources\t[CRS]\tInformation of computing resource\n",
    "    8. Object Amount\t[OBA]\tAmount of an object\n",
    "    9. Status Code\t[STC]\tStatus code of an object or an action\n",
    "    10. Other Parameters\t[OTP]\tOther information that does not belong to the above categories\n",
    "\n",
    "    Also, based on the variables found, and the understanding provided for each log as the input above, classify each log as either 1 if abnormal behaviour or 0 if normal behaviour.\n",
    "\n",
    "    Here is the input log message: <MSG>{log}</MSG>\n",
    "    \n",
    "    Please generate a dictionary where each key represents one of the categories listed above and the value represents the count of occurrences of that category in the log message and it's \"Class\" as either 1 or 0. \n",
    "    Always have the \"Class\" key:value pair but only include category key:value that are present in the log message.\n",
    "    Do not print anything other than the dictionary.\n",
    "    Never print the full name for categories just the code for example \"OID\".\n",
    "\n",
    "    Example of the format:\n",
    "    {{\n",
    "        \"OID\": 2,\n",
    "        \"LOI\": 1,\n",
    "        \"STC\": 1,\n",
    "        \"Class\": 1\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion_from_gpt(prompt)\n",
    "    variables_and_anomalies_from_logs.append(response)    \n",
    "\n",
    "    if counter_1 % 10 == 0:\n",
    "        print(f'{counter_1}: {response}')\n",
    "        \n",
    "    counter_1 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(variables_and_anomalies_from_logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and format output data in a csv file\n",
    "Format_output.save_raw_output(variables_output_file_path, variables_and_anomalies_from_logs)\n",
    "\n",
    "fieldnames = ['Index', 'OID', 'LOI', 'OBN', 'TID', 'SID', 'TDA', 'CRS', 'OBA', 'STC', 'OTP', 'Class']\n",
    "\n",
    "# Writing the data to the CSV\n",
    "with open(variables_output_file_path, mode='w', newline='') as variable_file:\n",
    "    writer = csv.DictWriter(variable_file, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Regular expression pattern to extract key-value pairs\n",
    "    pattern = re.compile(r'\"(\\w+)\":\\s*(\"?)([\\w\\s\\.,]+)\\2')\n",
    "\n",
    "    # Write each row of the dictionary results\n",
    "    for index, log_string in enumerate(variables_and_anomalies_from_logs, start=1):\n",
    "        # Clean up the string and remove extra formatting like ```json\n",
    "        cleaned_string = log_string.replace('```json', '').strip()\n",
    "\n",
    "        # Extract key-value pairs using the regular expression\n",
    "        matches = pattern.findall(cleaned_string)\n",
    "\n",
    "        # Create a dictionary from the matches\n",
    "        log_dict = {}\n",
    "        for key, _, value in matches:\n",
    "            # print(key)\n",
    "            value = value.strip().replace(',', '')  # Remove trailing newlines and whitespace\n",
    "            # print(value)\n",
    "            if value.isdigit():\n",
    "                log_dict[key] = int(value)\n",
    "            elif key == 'Class':\n",
    "                log_dict[key] = 0 if value in ['Normal'] else 1\n",
    "            else:\n",
    "                print(\"here\")\n",
    "                log_dict[key] = 0  # Replace any unexpected strings with 0\n",
    "\n",
    "        # Ensure that all keys are present, with missing ones having a value of 0 or an appropriate default\n",
    "        row_data = {key: log_dict.get(key, 0 if key != 'Class' else 'Normal') for key in fieldnames[1:]}\n",
    "        row_data['Index'] = index\n",
    "        writer.writerow(row_data)\n",
    "\n",
    "# Save all variables to a file\n",
    "print(f\"Variables are saved to: {variables_output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth and processed data\n",
    "ground_truth_df = pd.read_csv(ground_truth_file_path)\n",
    "processed_df = pd.read_csv(variables_output_file_path)\n",
    "\n",
    "# Ensure the dataframes are of the same length for comparison\n",
    "min_length = min(len(ground_truth_df), len(processed_df))\n",
    "ground_truth_df = ground_truth_df[:min_length]\n",
    "processed_df = processed_df[:min_length]\n",
    "\n",
    "# 1. Per Log Message Evaluation (Overall Accuracy per Log)\n",
    "log_message_accuracies = []\n",
    "for idx in range(min_length):\n",
    "    ground_truth_row = ground_truth_df.iloc[idx, 2:]  # Skip 'Index' column\n",
    "    processed_row = processed_df.iloc[idx, 1:-1]        # Skip 'Index' column\n",
    "    accuracy = (ground_truth_row == processed_row).mean()\n",
    "    log_message_accuracies.append(accuracy)\n",
    "\n",
    "overall_log_accuracy = sum(log_message_accuracies) / len(log_message_accuracies)\n",
    "print(f\"Overall Log Message Accuracy: {overall_log_accuracy * 100:.2f}%\")\n",
    "\n",
    "# 2. Evaluation Per Variable Type (Precision, Recall, F1 Score, and Accuracy for each variable type)\n",
    "variable_types = ['OID', 'LOI', 'OBN', 'TID', 'SID', 'TDA', 'CRS', 'OBA', 'STC', 'OTP']\n",
    "precision_scores = {}\n",
    "recall_scores = {}\n",
    "f1_scores = {}\n",
    "# accuracy_scores = {}\n",
    "\n",
    "for var in variable_types:\n",
    "    y_true = ground_truth_df[var]\n",
    "    y_pred = processed_df[var]\n",
    "    \n",
    "    precision_scores[var] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_scores[var] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1_scores[var] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    # accuracy_scores[var] = accuracy_score(y_true, y_pred)\n",
    "\n",
    "# Print Per Variable Type Metrics\n",
    "print(\"\\nEvaluation Per Variable Type:\")\n",
    "for var in variable_types:\n",
    "    print(f\"{var}:\")\n",
    "    print(f\"  Precision: {precision_scores[var] * 100:.2f}%  Recall: {recall_scores[var] * 100:.2f}%  F1 Score: {f1_scores[var] * 100:.2f}%\")\n",
    "    # print(f\"  Accuracy: {accuracy_scores[var] * 100:.2f}%\")\n",
    "\n",
    "# 3. Overall Evaluation for All Variables\n",
    "ground_truth_flat = ground_truth_df[variable_types].values.flatten()\n",
    "processed_flat = processed_df[variable_types].values.flatten()\n",
    "\n",
    "overall_precision = precision_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "overall_recall = recall_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "overall_f1 = f1_score(ground_truth_flat, processed_flat, average='weighted', zero_division=0)\n",
    "# overall_accuracy = accuracy_score(ground_truth_flat, processed_flat)\n",
    "\n",
    "# Print Overall Metrics\n",
    "print(\"\\nOverall Evaluation for All Variables:\")\n",
    "print(f\"Overall Precision: {overall_precision * 100:.2f}%\")\n",
    "print(f\"Overall Recall: {overall_recall * 100:.2f}%\")\n",
    "print(f\"Overall F1 Score: {overall_f1 * 100:.2f}%\")\n",
    "# print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Distribution of Log Message Accuracies\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(log_message_accuracies, bins=5, kde=True)\n",
    "plt.title('Distribution of Log Message Accuracies', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Save the plot\n",
    "output_file_path = os.path.join(output_directory, 'distribution_of_log_message_accuracies.png')\n",
    "plt.savefig(output_file_path, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Plotting System-wise Average Accuracies\n",
    "\n",
    "# Add the calculated accuracies to the ground truth DataFrame\n",
    "ground_truth_df['Accuracy'] = log_message_accuracies\n",
    "\n",
    "# Group by 'System' and calculate average accuracy per system\n",
    "system_wise_accuracy = ground_truth_df.groupby('System')['Accuracy'].mean()\n",
    "\n",
    "# Normalize accuracy values for color mapping\n",
    "norm = mcolors.Normalize(vmin=system_wise_accuracy.min(), vmax=system_wise_accuracy.max())\n",
    "colors = [cm.viridis(norm(value)) for value in system_wise_accuracy.values]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.barplot(x=system_wise_accuracy.index, y=system_wise_accuracy.values)\n",
    "\n",
    "# Assign color to each bar manually to follow the computed color based on accuracy values\n",
    "for bar, color in zip(ax.patches, colors):\n",
    "    bar.set_facecolor(color)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('System-wise Average Accuracy', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('System')\n",
    "plt.ylabel('Average Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add data labels to each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height() * 100:.2f}%', \n",
    "                (p.get_x() + p.get_width() / 2, p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(os.path.join(output_directory, 'system_wise_average_accuracy.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Per Variable Type Accuracy Comparison with Data Labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Set color manually using a color map rather than using the deprecated palette without hue\n",
    "colors = sns.color_palette('Blues', len(recall_scores))\n",
    "ax = sns.barplot(x=list(recall_scores.keys()), y=list(recall_scores.values()), palette=colors)\n",
    "\n",
    "# Add data labels on each bar\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{p.get_height() * 100:.2f}%', \n",
    "                (p.get_x() + p.get_width() / 2, p.get_height()), \n",
    "                ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "plt.title('Per Variable Type Recall Comparison', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Variable Type')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "# Save the plot before showing it\n",
    "plt.savefig(os.path.join(output_directory, 'per_variable_type_recall_comparison.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.Box Plot of Metrics Per Variable Type\n",
    "metrics_all = pd.DataFrame({'Precision': list(precision_scores.values()),\n",
    "                            'Recall': list(recall_scores.values()),\n",
    "                            'F1 Score': list(f1_scores.values())}, index=variable_types)\n",
    "plt.figure(figsize=(14, 8))\n",
    "ax = sns.boxplot(data=metrics_all)\n",
    "\n",
    "# Add median value labels on each box\n",
    "medians = metrics_all.median()\n",
    "\n",
    "for i, median in enumerate(medians):\n",
    "    ax.text(i, median, f'{median:.2f}', horizontalalignment='center', color='black', size=10)\n",
    "\n",
    "plt.title('Box Plot of Metrics per Variable Type', fontsize=16, fontweight='normal', pad=15)\n",
    "plt.xlabel('Metric')\n",
    "plt.ylabel('Score')\n",
    "plt.ylim(0, 1)\n",
    "# Save the plot before showing it\n",
    "plt.savefig(os.path.join(output_directory, 'box_plot_of_metrics_per_tariable_type.png'), bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Cumulative Distribution of Log Accuracy\n",
    "cumulative_accuracies = np.cumsum(log_message_accuracies) / np.arange(1, len(log_message_accuracies) + 1)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(cumulative_accuracies) + 1), cumulative_accuracies, marker='o')\n",
    "plt.title('Cumulative Distribution of Log Accuracies')\n",
    "plt.xlabel('Log Index')\n",
    "plt.ylabel('Cumulative Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
